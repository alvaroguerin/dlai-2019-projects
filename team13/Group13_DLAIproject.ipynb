{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Group13_DLAIproject.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAN97QD3ME9S",
        "colab_type": "text"
      },
      "source": [
        "# **DLAI PROJECT**\n",
        "\n",
        "Project created by Carles Garcia Cabrera (Group 13), student of DLAI during autumm 2019, under the supervison of Noé Casas and Xavier Giró.\n",
        "\n",
        "The project is broken down in 4 different parts:\n",
        "  - Exercise 1: Convolutional Autoencoder\n",
        "  - Exercise 2: Transfer Learning\n",
        "  - Optional exercise A: Variational Autoencoder\n",
        "  - Optional exercise B: Conditional GAN\n",
        "\n",
        "During the whole notebook I will try to explain what I want to do before coding it and along the code I will comment the commands that I feel are most important."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRzUbdcrNsSP",
        "colab_type": "text"
      },
      "source": [
        "# **Let's start!**\n",
        "First of all the imports for the packages I am going to use:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jdpYc_nvL_2w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Core libraries\n",
        "import numpy as np\n",
        "np.random.seed(1)\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "torch.manual_seed(1)\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.manual_seed_all(1)\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, utils\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from tabulate import tabulate\n",
        "from statistics import mean\n",
        "\n",
        "import copy\n",
        "\n",
        "#GPU Enabling\n",
        "if not torch.cuda.is_available():\n",
        "    raise Exception(\"You should enable GPU in the runtime menu.\")\n",
        "device = torch.device(\"cuda:0\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OX01DPIPcP7",
        "colab_type": "text"
      },
      "source": [
        "# **Exercise 1: Convolutional Autoencoder**\n",
        "First we will load the **MNIST dataset** and then we will split the trainig data into 95% train and 5% validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHhquCBmPr4_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7bc84ecc-00b0-4d2a-d155-a5d6efd33bef"
      },
      "source": [
        "#Train import\n",
        "mnist_trainset = datasets.MNIST('data', train=True, download=True,\n",
        "                                transform=transforms.Compose([\n",
        "                                    transforms.ToTensor(),\n",
        "                                    transforms.Normalize((0.5,), (0.5,))\n",
        "                                    ]))\n",
        "##Split train into train and val\n",
        "mnist_trainset, mnist_valset = torch.utils.data.random_split(mnist_trainset, [int(len(mnist_trainset)*0.95),int(len(mnist_trainset)*0.05)])\n",
        "\n",
        "#Test\n",
        "mnist_testset = datasets.MNIST('data', train=False, \n",
        "                               transform=transforms.Compose([\n",
        "                                   transforms.ToTensor(),\n",
        "                                   transforms.Normalize((0.5,), (0.5,))\n",
        "                               ]))\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:03, 3115230.39it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 48647.08it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:02, 801403.77it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 18076.58it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFI3u8Jka0ne",
        "colab_type": "text"
      },
      "source": [
        "And now some hyperparametes that we will use during the exercise:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm2IQ0Xea7TA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = {\n",
        "    'batch_size':128,\n",
        "    'num_epochs':30,\n",
        "    'num_epochs2':1000,\n",
        "    'num_epochs3':500,\n",
        "    'num_epochs4':2000,\n",
        "    'test_batch_size':64,\n",
        "    'hidden1':64,\n",
        "    'hidden2':128,\n",
        "    'bottleneck1':10,\n",
        "    'bottleneck2':50,\n",
        "    'bottleneck3':100,\n",
        "    'learning_rate':1e-3,\n",
        "    'log_interval': 100\n",
        "}\n",
        "hparams['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpgkGO5gaXUM",
        "colab_type": "text"
      },
      "source": [
        "And now the dataloaders:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dPKaE1jzYyh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    mnist_trainset,\n",
        "    batch_size=hparams['batch_size'], \n",
        "    shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    mnist_valset,\n",
        "    batch_size=hparams['batch_size'], \n",
        "    shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    mnist_testset,\n",
        "    batch_size=hparams['test_batch_size'], \n",
        "    shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "26KQmiPzbH7X",
        "colab_type": "text"
      },
      "source": [
        "Now we will implement three different **autoencoders**, the difference is the bottleneck size."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4eFi2awMo9zd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class autoencoder1(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "    super(autoencoder1,self).__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(1, hparams['hidden1'], 4, stride = 2, padding = 1), \n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(hparams['hidden1'], hparams['hidden2'], 4, stride = 2, padding = 1),\n",
        "        nn.ReLU(True),\n",
        "    )\n",
        "\n",
        "    self.linear1 = nn.Linear(hparams['hidden2']*7*7, hparams['bottleneck1'])\n",
        "    self.linear2 = nn.Linear(hparams['bottleneck1'], hparams['hidden2']*7*7)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.softmax = nn.Softmax()\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(hparams['hidden2'], hparams['hidden1'], 4, stride = 2, padding = 1),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(hparams['hidden1'], 1, 4, stride = 2, padding = 1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    #bottleneck\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.linear1(x)\n",
        "    x = self.linear2(x)\n",
        "    x = x.view(x.size(0), hparams['hidden2'], 7, 7)\n",
        "    ##\n",
        "    y = self.decoder(x)\n",
        "    return y\n",
        "\n",
        "class autoencoder2(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "    super(autoencoder2,self).__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(1, hparams['hidden1'], 4, stride = 2, padding = 1), \n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(hparams['hidden1'], hparams['hidden2'], 4, stride = 2, padding = 1),\n",
        "        nn.ReLU(True),\n",
        "    )\n",
        "\n",
        "    self.linear1 = nn.Linear(hparams['hidden2']*7*7, hparams['bottleneck2'])\n",
        "    self.linear2 = nn.Linear(hparams['bottleneck2'], hparams['hidden2']*7*7)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(hparams['hidden2'], hparams['hidden1'], 4, stride = 2, padding = 1),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(hparams['hidden1'], 1, 4, stride = 2, padding = 1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    #bottleneck\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.linear1(x)\n",
        "    x = self.linear2(x)\n",
        "    x = x.view(x.size(0), hparams['hidden2'], 7, 7)\n",
        "    ##\n",
        "    y = self.decoder(x)\n",
        "    return y\n",
        "\n",
        "class autoencoder3(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "    super(autoencoder3,self).__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(1, hparams['hidden1'], 4, stride = 2, padding = 1), \n",
        "        nn.ReLU(True),\n",
        "        nn.Conv2d(hparams['hidden1'], hparams['hidden2'], 4, stride = 2, padding = 1),\n",
        "        nn.ReLU(True),\n",
        "    )\n",
        "\n",
        "    self.linear1 = nn.Linear(hparams['hidden2']*7*7, hparams['bottleneck3'])\n",
        "    self.linear2 = nn.Linear(hparams['bottleneck3'], hparams['hidden2']*7*7)\n",
        "    self.relu = nn.ReLU()\n",
        "\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(hparams['hidden2'], hparams['hidden1'], 4, stride = 2, padding = 1),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(hparams['hidden1'], 1, 4, stride = 2, padding = 1),\n",
        "        nn.Tanh()\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.encoder(x)\n",
        "    #bottleneck\n",
        "    x = x.view(x.size(0), -1)\n",
        "    x = self.linear1(x)\n",
        "    x = self.linear2(x)\n",
        "    x = x.view(x.size(0), hparams['hidden2'], 7, 7)\n",
        "    ##\n",
        "    y = self.decoder(x)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mdi8-DZhf3T1",
        "colab_type": "text"
      },
      "source": [
        "# **FAIL!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "btuJWbjwQ7gc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "    super(autoencoder,self).__init__()\n",
        "    self.encoder = nn.Sequential(\n",
        "        nn.Conv2d(1, hparams['hidden1'], 3, padding = 1), \n",
        "        nn.ReLU(True),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(hparams['hidden1'], hparams['hidden2'], 3, padding = 1),\n",
        "        nn.ReLU(True),\n",
        "        nn.MaxPool2d(2,2),\n",
        "        nn.Conv2d(hparams['hidden2'], hparams['hidden3'], 3, padding = 1),\n",
        "        nn.ReLU(True),\n",
        "    )\n",
        "    self.decoder = nn.Sequential(\n",
        "        nn.ConvTranspose2d(hparams['hidden3'], hparams['hidden2'], 2, stride = 2),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(hparams['hidden2'], hparams['hidden1'], 2, stride = 2),\n",
        "        nn.ReLU(True),\n",
        "        nn.ConvTranspose2d(hparams['hidden1'], 1, 1, stride = 1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "  \n",
        "  def forward(self, x):\n",
        "    #print(x.shape)\n",
        "    x = self.encoder(x)\n",
        "    #print(x.shape)\n",
        "    y = self.decoder(x)\n",
        "    #print(y.shape)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH4GlWY8f6PY",
        "colab_type": "text"
      },
      "source": [
        "# **FAIL!**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTd9N-epbU0c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class autoencoder(nn.Module):\n",
        "  def __init__(self, hparams):\n",
        "    super().__init__()\n",
        "    #Convs\n",
        "    self.conv1 = nn.Conv2d(1, hparams['hidden1'], 3, padding = 1)\n",
        "    self.conv11 = nn.Conv2d(hparams['hidden1'], hparams['hidden1'], 3, padding = 1)\n",
        "    self.conv2 = nn.Conv2d(hparams['hidden1'], hparams['hidden2'], 3, padding = 1)\n",
        "    self.conv22 = nn.Conv2d(hparams['hidden2'], hparams['hidden2'], 3, padding = 1)\n",
        "    self.conv3 = nn.Conv2d(hparams['hidden2'], hparams['hidden3'], 3, padding = 1)\n",
        "    self.conv33 = nn.Conv2d(hparams['hidden3'], hparams['hidden3'], 3, padding = 1)\n",
        "    #Maxpool\n",
        "    self.maxpool = nn.MaxPool2d(2)\n",
        "    #UpConvs\n",
        "    self.upconv1 = nn.ConvTranspose2d(hparams['hidden3'],hparams['hidden2'],2, stride = 2)\n",
        "    self.upconv2 = nn.ConvTranspose2d(hparams['hidden2'],hparams['hidden1'],2, stride = 2)\n",
        "    #To 1 channel\n",
        "    self.upconv3 = nn.ConvTranspose2d(hparams['hidden1'],1,1, stride = 1,)\n",
        "    #Conv1x1\n",
        "    self.conv1x1 = nn.Conv2d(1,1,1) \n",
        "    #Activations\n",
        "    self.relu = nn.ReLU()\n",
        "    self.sigm = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    #Encoder x =(28x28,1)\n",
        "    x = self.conv1(x) #(28x28,32)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv11(x) \n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x) #(14,14,32) \n",
        "    x = self.conv2(x) #First down arrow (14,14,64)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv22(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.maxpool(x) #(7,7,64)\n",
        "    ## Second down arrow \n",
        "    ## -> Bottle neck\n",
        "    x = self.conv3(x) #(7,7,128)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv33(x)\n",
        "    x = self.relu(x)\n",
        "    #Decoder\n",
        "    x = self.upconv1(x) #First up arrow (14,14,64)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv22(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.upconv2(x) #Second up arrow (28,28,32)\n",
        "    x = self.relu(x)\n",
        "    x = self.conv11(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.upconv3(x) # (28,28,1)\n",
        "    y = self.sigm(x)\n",
        "    return y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2k1texjtf68",
        "colab_type": "text"
      },
      "source": [
        "Now, let's define the **training**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05ErO6I9edmz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(train_loader, network, optimizer, criterion, hparams, epoch):\n",
        "  network.train()\n",
        "  device = hparams['device']\n",
        "  losses = []\n",
        "  for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
        "      data = data.to(device)\n",
        "      optimizer.zero_grad()\n",
        "      output = network(data)\n",
        "      loss = criterion(output, data)\n",
        "      loss.backward()\n",
        "      losses.append(loss.item())\n",
        "      optimizer.step()\n",
        "      if batch_idx % hparams['log_interval'] == 0 or batch_idx >= len(train_loader):\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item()\n",
        "              ))\n",
        "  return np.mean(losses)\n",
        "\n",
        "def eval_epoch(val_loader, network, criterion, hparams):\n",
        "  network.eval()\n",
        "  device = hparams['device']\n",
        "  eval_loss = 0\n",
        "  with torch.no_grad():\n",
        "      for data, target in val_loader:\n",
        "          data = data.to(device)\n",
        "          output = network(data)\n",
        "          eval_loss += criterion(output, data).item() \n",
        "  eval_loss /= len(val_loader.dataset)\n",
        "  print('Eval set: Average loss: {:.4f}'.format(\n",
        "      eval_loss*100, len(val_loader.dataset),\n",
        "      ))\n",
        "  return eval_loss*100\n",
        "\n",
        "def train_net(network, train_loader, eval_loader, optimizer, num_epochs, plot=True):\n",
        "  tr_losses = []\n",
        "  te_losses = []\n",
        "\n",
        "  network.to(hparams['device'])\n",
        "  criterion = nn.MSELoss()\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "    tr_loss = train_epoch(train_loader, network, optimizer, criterion, hparams, epoch)\n",
        "    te_loss = eval_epoch(eval_loader, network, criterion, hparams)\n",
        "    te_losses.append(te_loss)\n",
        "    tr_losses.append(tr_loss)\n",
        "  rets = {'tr_losses':tr_losses, 'te_losses':te_losses}\n",
        "  if plot:\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.subplot(2,1,1)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('NLLLoss')\n",
        "    plt.plot(tr_losses, label='Train')\n",
        "    plt.plot(te_losses, label='Eval')\n",
        "    plt.legend()\n",
        "  return rets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBZOiAATsa3a",
        "colab_type": "text"
      },
      "source": [
        "Model, criterion and optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crYpFtqqsd1N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "model1 = autoencoder1(hparams).cuda()\n",
        "optimizer1 = torch.optim.Adam(model1.parameters(), lr = hparams['learning_rate'],\n",
        "                             weight_decay = 1e-5)\n",
        "num_params1 = sum(p.numel() for p in model1.parameters() if p.requires_grad)\n",
        "\n",
        "model2 = autoencoder2(hparams).cuda()\n",
        "optimizer2 = torch.optim.Adam(model2.parameters(), lr = hparams['learning_rate'],\n",
        "                             weight_decay = 1e-5)\n",
        "num_params2 = sum(p.numel() for p in model2.parameters() if p.requires_grad)\n",
        "\n",
        "model3 = autoencoder3(hparams).cuda()\n",
        "optimizer3 = torch.optim.Adam(model3.parameters(), lr = hparams['learning_rate'],\n",
        "                             weight_decay = 1e-5)\n",
        "num_params3 = sum(p.numel() for p in model3.parameters() if p.requires_grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_5Wg9wTZ_AWv",
        "colab_type": "text"
      },
      "source": [
        "Let's **TRAIN**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzzN7wiPXOML",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c762343-6509-48ae-c42b-5fdb93f29be1"
      },
      "source": [
        "autoen1 = train_net(model1, train_loader, val_loader, optimizer1, hparams['num_epochs'])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [12800/57000 (22%)]\tLoss: 0.111580\n",
            "Train Epoch: 1 [25600/57000 (45%)]\tLoss: 0.096076\n",
            "Train Epoch: 1 [38400/57000 (67%)]\tLoss: 0.082491\n",
            "Train Epoch: 1 [51200/57000 (90%)]\tLoss: 0.074157\n",
            "Train Epoch: 1 [17840/57000 (100%)]\tLoss: 0.067941\n",
            "Eval set: Average loss: 0.0598\n",
            "Train Epoch: 2 [12800/57000 (22%)]\tLoss: 0.068979\n",
            "Train Epoch: 2 [25600/57000 (45%)]\tLoss: 0.069852\n",
            "Train Epoch: 2 [38400/57000 (67%)]\tLoss: 0.070459\n",
            "Train Epoch: 2 [51200/57000 (90%)]\tLoss: 0.061533\n",
            "Train Epoch: 2 [17840/57000 (100%)]\tLoss: 0.067567\n",
            "Eval set: Average loss: 0.0511\n",
            "Train Epoch: 3 [12800/57000 (22%)]\tLoss: 0.062713\n",
            "Train Epoch: 3 [25600/57000 (45%)]\tLoss: 0.061126\n",
            "Train Epoch: 3 [38400/57000 (67%)]\tLoss: 0.058674\n",
            "Train Epoch: 3 [51200/57000 (90%)]\tLoss: 0.062396\n",
            "Train Epoch: 3 [17840/57000 (100%)]\tLoss: 0.066953\n",
            "Eval set: Average loss: 0.0482\n",
            "Train Epoch: 4 [12800/57000 (22%)]\tLoss: 0.059744\n",
            "Train Epoch: 4 [25600/57000 (45%)]\tLoss: 0.064880\n",
            "Train Epoch: 4 [38400/57000 (67%)]\tLoss: 0.053380\n",
            "Train Epoch: 4 [51200/57000 (90%)]\tLoss: 0.055835\n",
            "Train Epoch: 4 [17840/57000 (100%)]\tLoss: 0.051882\n",
            "Eval set: Average loss: 0.0459\n",
            "Train Epoch: 5 [12800/57000 (22%)]\tLoss: 0.047767\n",
            "Train Epoch: 5 [25600/57000 (45%)]\tLoss: 0.058377\n",
            "Train Epoch: 5 [38400/57000 (67%)]\tLoss: 0.059455\n",
            "Train Epoch: 5 [51200/57000 (90%)]\tLoss: 0.060920\n",
            "Train Epoch: 5 [17840/57000 (100%)]\tLoss: 0.053982\n",
            "Eval set: Average loss: 0.0454\n",
            "Train Epoch: 6 [12800/57000 (22%)]\tLoss: 0.056381\n",
            "Train Epoch: 6 [25600/57000 (45%)]\tLoss: 0.056412\n",
            "Train Epoch: 6 [38400/57000 (67%)]\tLoss: 0.050475\n",
            "Train Epoch: 6 [51200/57000 (90%)]\tLoss: 0.053276\n",
            "Train Epoch: 6 [17840/57000 (100%)]\tLoss: 0.052292\n",
            "Eval set: Average loss: 0.0437\n",
            "Train Epoch: 7 [12800/57000 (22%)]\tLoss: 0.050999\n",
            "Train Epoch: 7 [25600/57000 (45%)]\tLoss: 0.055848\n",
            "Train Epoch: 7 [38400/57000 (67%)]\tLoss: 0.049618\n",
            "Train Epoch: 7 [51200/57000 (90%)]\tLoss: 0.057917\n",
            "Train Epoch: 7 [17840/57000 (100%)]\tLoss: 0.053787\n",
            "Eval set: Average loss: 0.0436\n",
            "Train Epoch: 8 [12800/57000 (22%)]\tLoss: 0.052513\n",
            "Train Epoch: 8 [25600/57000 (45%)]\tLoss: 0.050789\n",
            "Train Epoch: 8 [38400/57000 (67%)]\tLoss: 0.053818\n",
            "Train Epoch: 8 [51200/57000 (90%)]\tLoss: 0.055079\n",
            "Train Epoch: 8 [17840/57000 (100%)]\tLoss: 0.047620\n",
            "Eval set: Average loss: 0.0422\n",
            "Train Epoch: 9 [12800/57000 (22%)]\tLoss: 0.049211\n",
            "Train Epoch: 9 [25600/57000 (45%)]\tLoss: 0.053571\n",
            "Train Epoch: 9 [38400/57000 (67%)]\tLoss: 0.050001\n",
            "Train Epoch: 9 [51200/57000 (90%)]\tLoss: 0.048385\n",
            "Train Epoch: 9 [17840/57000 (100%)]\tLoss: 0.054073\n",
            "Eval set: Average loss: 0.0423\n",
            "Train Epoch: 10 [12800/57000 (22%)]\tLoss: 0.058684\n",
            "Train Epoch: 10 [25600/57000 (45%)]\tLoss: 0.048890\n",
            "Train Epoch: 10 [38400/57000 (67%)]\tLoss: 0.048927\n",
            "Train Epoch: 10 [51200/57000 (90%)]\tLoss: 0.050699\n",
            "Train Epoch: 10 [17840/57000 (100%)]\tLoss: 0.059223\n",
            "Eval set: Average loss: 0.0418\n",
            "Train Epoch: 11 [12800/57000 (22%)]\tLoss: 0.047308\n",
            "Train Epoch: 11 [25600/57000 (45%)]\tLoss: 0.046874\n",
            "Train Epoch: 11 [38400/57000 (67%)]\tLoss: 0.056035\n",
            "Train Epoch: 11 [51200/57000 (90%)]\tLoss: 0.054483\n",
            "Train Epoch: 11 [17840/57000 (100%)]\tLoss: 0.062506\n",
            "Eval set: Average loss: 0.0411\n",
            "Train Epoch: 12 [12800/57000 (22%)]\tLoss: 0.051155\n",
            "Train Epoch: 12 [25600/57000 (45%)]\tLoss: 0.046970\n",
            "Train Epoch: 12 [38400/57000 (67%)]\tLoss: 0.052638\n",
            "Train Epoch: 12 [51200/57000 (90%)]\tLoss: 0.047809\n",
            "Train Epoch: 12 [17840/57000 (100%)]\tLoss: 0.050661\n",
            "Eval set: Average loss: 0.0407\n",
            "Train Epoch: 13 [12800/57000 (22%)]\tLoss: 0.053305\n",
            "Train Epoch: 13 [25600/57000 (45%)]\tLoss: 0.049226\n",
            "Train Epoch: 13 [38400/57000 (67%)]\tLoss: 0.050393\n",
            "Train Epoch: 13 [51200/57000 (90%)]\tLoss: 0.050148\n",
            "Train Epoch: 13 [17840/57000 (100%)]\tLoss: 0.052461\n",
            "Eval set: Average loss: 0.0403\n",
            "Train Epoch: 14 [12800/57000 (22%)]\tLoss: 0.052305\n",
            "Train Epoch: 14 [25600/57000 (45%)]\tLoss: 0.048090\n",
            "Train Epoch: 14 [38400/57000 (67%)]\tLoss: 0.047521\n",
            "Train Epoch: 14 [51200/57000 (90%)]\tLoss: 0.049298\n",
            "Train Epoch: 14 [17840/57000 (100%)]\tLoss: 0.042229\n",
            "Eval set: Average loss: 0.0399\n",
            "Train Epoch: 15 [12800/57000 (22%)]\tLoss: 0.046725\n",
            "Train Epoch: 15 [25600/57000 (45%)]\tLoss: 0.050713\n",
            "Train Epoch: 15 [38400/57000 (67%)]\tLoss: 0.049988\n",
            "Train Epoch: 15 [51200/57000 (90%)]\tLoss: 0.049898\n",
            "Train Epoch: 15 [17840/57000 (100%)]\tLoss: 0.050254\n",
            "Eval set: Average loss: 0.0398\n",
            "Train Epoch: 16 [12800/57000 (22%)]\tLoss: 0.046208\n",
            "Train Epoch: 16 [25600/57000 (45%)]\tLoss: 0.048518\n",
            "Train Epoch: 16 [38400/57000 (67%)]\tLoss: 0.049856\n",
            "Train Epoch: 16 [51200/57000 (90%)]\tLoss: 0.049501\n",
            "Train Epoch: 16 [17840/57000 (100%)]\tLoss: 0.050325\n",
            "Eval set: Average loss: 0.0401\n",
            "Train Epoch: 17 [12800/57000 (22%)]\tLoss: 0.047309\n",
            "Train Epoch: 17 [25600/57000 (45%)]\tLoss: 0.050626\n",
            "Train Epoch: 17 [38400/57000 (67%)]\tLoss: 0.049043\n",
            "Train Epoch: 17 [51200/57000 (90%)]\tLoss: 0.049340\n",
            "Train Epoch: 17 [17840/57000 (100%)]\tLoss: 0.044255\n",
            "Eval set: Average loss: 0.0399\n",
            "Train Epoch: 18 [12800/57000 (22%)]\tLoss: 0.047373\n",
            "Train Epoch: 18 [25600/57000 (45%)]\tLoss: 0.051891\n",
            "Train Epoch: 18 [38400/57000 (67%)]\tLoss: 0.049906\n",
            "Train Epoch: 18 [51200/57000 (90%)]\tLoss: 0.047675\n",
            "Train Epoch: 18 [17840/57000 (100%)]\tLoss: 0.045484\n",
            "Eval set: Average loss: 0.0393\n",
            "Train Epoch: 19 [12800/57000 (22%)]\tLoss: 0.042434\n",
            "Train Epoch: 19 [25600/57000 (45%)]\tLoss: 0.050191\n",
            "Train Epoch: 19 [38400/57000 (67%)]\tLoss: 0.048685\n",
            "Train Epoch: 19 [51200/57000 (90%)]\tLoss: 0.052781\n",
            "Train Epoch: 19 [17840/57000 (100%)]\tLoss: 0.045161\n",
            "Eval set: Average loss: 0.0392\n",
            "Train Epoch: 20 [12800/57000 (22%)]\tLoss: 0.046359\n",
            "Train Epoch: 20 [25600/57000 (45%)]\tLoss: 0.046638\n",
            "Train Epoch: 20 [38400/57000 (67%)]\tLoss: 0.054712\n",
            "Train Epoch: 20 [51200/57000 (90%)]\tLoss: 0.050199\n",
            "Train Epoch: 20 [17840/57000 (100%)]\tLoss: 0.051840\n",
            "Eval set: Average loss: 0.0392\n",
            "Train Epoch: 21 [12800/57000 (22%)]\tLoss: 0.045156\n",
            "Train Epoch: 21 [25600/57000 (45%)]\tLoss: 0.051307\n",
            "Train Epoch: 21 [38400/57000 (67%)]\tLoss: 0.047953\n",
            "Train Epoch: 21 [51200/57000 (90%)]\tLoss: 0.052815\n",
            "Train Epoch: 21 [17840/57000 (100%)]\tLoss: 0.052903\n",
            "Eval set: Average loss: 0.0391\n",
            "Train Epoch: 22 [12800/57000 (22%)]\tLoss: 0.044174\n",
            "Train Epoch: 22 [25600/57000 (45%)]\tLoss: 0.049553\n",
            "Train Epoch: 22 [38400/57000 (67%)]\tLoss: 0.049576\n",
            "Train Epoch: 22 [51200/57000 (90%)]\tLoss: 0.048023\n",
            "Train Epoch: 22 [17840/57000 (100%)]\tLoss: 0.051756\n",
            "Eval set: Average loss: 0.0390\n",
            "Train Epoch: 23 [12800/57000 (22%)]\tLoss: 0.043252\n",
            "Train Epoch: 23 [25600/57000 (45%)]\tLoss: 0.048194\n",
            "Train Epoch: 23 [38400/57000 (67%)]\tLoss: 0.049214\n",
            "Train Epoch: 23 [51200/57000 (90%)]\tLoss: 0.046512\n",
            "Train Epoch: 23 [17840/57000 (100%)]\tLoss: 0.043847\n",
            "Eval set: Average loss: 0.0387\n",
            "Train Epoch: 24 [12800/57000 (22%)]\tLoss: 0.045730\n",
            "Train Epoch: 24 [25600/57000 (45%)]\tLoss: 0.051122\n",
            "Train Epoch: 24 [38400/57000 (67%)]\tLoss: 0.048621\n",
            "Train Epoch: 24 [51200/57000 (90%)]\tLoss: 0.047490\n",
            "Train Epoch: 24 [17840/57000 (100%)]\tLoss: 0.044632\n",
            "Eval set: Average loss: 0.0385\n",
            "Train Epoch: 25 [12800/57000 (22%)]\tLoss: 0.046248\n",
            "Train Epoch: 25 [25600/57000 (45%)]\tLoss: 0.049755\n",
            "Train Epoch: 25 [38400/57000 (67%)]\tLoss: 0.049288\n",
            "Train Epoch: 25 [51200/57000 (90%)]\tLoss: 0.046192\n",
            "Train Epoch: 25 [17840/57000 (100%)]\tLoss: 0.053066\n",
            "Eval set: Average loss: 0.0388\n",
            "Train Epoch: 26 [12800/57000 (22%)]\tLoss: 0.050565\n",
            "Train Epoch: 26 [25600/57000 (45%)]\tLoss: 0.046163\n",
            "Train Epoch: 26 [38400/57000 (67%)]\tLoss: 0.048749\n",
            "Train Epoch: 26 [51200/57000 (90%)]\tLoss: 0.044447\n",
            "Train Epoch: 26 [17840/57000 (100%)]\tLoss: 0.043060\n",
            "Eval set: Average loss: 0.0386\n",
            "Train Epoch: 27 [12800/57000 (22%)]\tLoss: 0.044830\n",
            "Train Epoch: 27 [25600/57000 (45%)]\tLoss: 0.051760\n",
            "Train Epoch: 27 [38400/57000 (67%)]\tLoss: 0.049036\n",
            "Train Epoch: 27 [51200/57000 (90%)]\tLoss: 0.046445\n",
            "Train Epoch: 27 [17840/57000 (100%)]\tLoss: 0.052321\n",
            "Eval set: Average loss: 0.0385\n",
            "Train Epoch: 28 [12800/57000 (22%)]\tLoss: 0.042564\n",
            "Train Epoch: 28 [25600/57000 (45%)]\tLoss: 0.041995\n",
            "Train Epoch: 28 [38400/57000 (67%)]\tLoss: 0.048322\n",
            "Train Epoch: 28 [51200/57000 (90%)]\tLoss: 0.047694\n",
            "Train Epoch: 28 [17840/57000 (100%)]\tLoss: 0.047370\n",
            "Eval set: Average loss: 0.0384\n",
            "Train Epoch: 29 [12800/57000 (22%)]\tLoss: 0.050157\n",
            "Train Epoch: 29 [25600/57000 (45%)]\tLoss: 0.044908\n",
            "Train Epoch: 29 [38400/57000 (67%)]\tLoss: 0.041821\n",
            "Train Epoch: 29 [51200/57000 (90%)]\tLoss: 0.043765\n",
            "Train Epoch: 29 [17840/57000 (100%)]\tLoss: 0.049110\n",
            "Eval set: Average loss: 0.0383\n",
            "Train Epoch: 30 [12800/57000 (22%)]\tLoss: 0.046921\n",
            "Train Epoch: 30 [25600/57000 (45%)]\tLoss: 0.045633\n",
            "Train Epoch: 30 [38400/57000 (67%)]\tLoss: 0.045033\n",
            "Train Epoch: 30 [51200/57000 (90%)]\tLoss: 0.048677\n",
            "Train Epoch: 30 [17840/57000 (100%)]\tLoss: 0.041350\n",
            "Eval set: Average loss: 0.0383\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAADzCAYAAADD2rFtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJiIiIlJE/j+SiyujZtDkuQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npU2bqbknNqZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "da454050-bc2c-4c51-e7c8-f3ba2f686038"
      },
      "source": [
        "autoen2 = train_net(model2, train_loader, val_loader, optimizer2, hparams['num_epochs'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [12800/57000 (22%)]\tLoss: 0.078223\n",
            "Train Epoch: 1 [25600/57000 (45%)]\tLoss: 0.034420\n",
            "Train Epoch: 1 [38400/57000 (67%)]\tLoss: 0.024731\n",
            "Train Epoch: 1 [51200/57000 (90%)]\tLoss: 0.021118\n",
            "Train Epoch: 1 [17840/57000 (100%)]\tLoss: 0.019150\n",
            "Eval set: Average loss: 0.0158\n",
            "Train Epoch: 2 [12800/57000 (22%)]\tLoss: 0.018246\n",
            "Train Epoch: 2 [25600/57000 (45%)]\tLoss: 0.015438\n",
            "Train Epoch: 2 [38400/57000 (67%)]\tLoss: 0.016325\n",
            "Train Epoch: 2 [51200/57000 (90%)]\tLoss: 0.014314\n",
            "Train Epoch: 2 [17840/57000 (100%)]\tLoss: 0.013451\n",
            "Eval set: Average loss: 0.0110\n",
            "Train Epoch: 3 [12800/57000 (22%)]\tLoss: 0.012889\n",
            "Train Epoch: 3 [25600/57000 (45%)]\tLoss: 0.014397\n",
            "Train Epoch: 3 [38400/57000 (67%)]\tLoss: 0.011280\n",
            "Train Epoch: 3 [51200/57000 (90%)]\tLoss: 0.011940\n",
            "Train Epoch: 3 [17840/57000 (100%)]\tLoss: 0.012325\n",
            "Eval set: Average loss: 0.0099\n",
            "Train Epoch: 4 [12800/57000 (22%)]\tLoss: 0.012124\n",
            "Train Epoch: 4 [25600/57000 (45%)]\tLoss: 0.010997\n",
            "Train Epoch: 4 [38400/57000 (67%)]\tLoss: 0.011281\n",
            "Train Epoch: 4 [51200/57000 (90%)]\tLoss: 0.011152\n",
            "Train Epoch: 4 [17840/57000 (100%)]\tLoss: 0.009951\n",
            "Eval set: Average loss: 0.0090\n",
            "Train Epoch: 5 [12800/57000 (22%)]\tLoss: 0.010449\n",
            "Train Epoch: 5 [25600/57000 (45%)]\tLoss: 0.011294\n",
            "Train Epoch: 5 [38400/57000 (67%)]\tLoss: 0.010851\n",
            "Train Epoch: 5 [51200/57000 (90%)]\tLoss: 0.011454\n",
            "Train Epoch: 5 [17840/57000 (100%)]\tLoss: 0.010077\n",
            "Eval set: Average loss: 0.0086\n",
            "Train Epoch: 6 [12800/57000 (22%)]\tLoss: 0.010353\n",
            "Train Epoch: 6 [25600/57000 (45%)]\tLoss: 0.009770\n",
            "Train Epoch: 6 [38400/57000 (67%)]\tLoss: 0.010886\n",
            "Train Epoch: 6 [51200/57000 (90%)]\tLoss: 0.009974\n",
            "Train Epoch: 6 [17840/57000 (100%)]\tLoss: 0.011872\n",
            "Eval set: Average loss: 0.0084\n",
            "Train Epoch: 7 [12800/57000 (22%)]\tLoss: 0.010088\n",
            "Train Epoch: 7 [25600/57000 (45%)]\tLoss: 0.010113\n",
            "Train Epoch: 7 [38400/57000 (67%)]\tLoss: 0.009433\n",
            "Train Epoch: 7 [51200/57000 (90%)]\tLoss: 0.009740\n",
            "Train Epoch: 7 [17840/57000 (100%)]\tLoss: 0.009500\n",
            "Eval set: Average loss: 0.0081\n",
            "Train Epoch: 8 [12800/57000 (22%)]\tLoss: 0.010094\n",
            "Train Epoch: 8 [25600/57000 (45%)]\tLoss: 0.010521\n",
            "Train Epoch: 8 [38400/57000 (67%)]\tLoss: 0.009334\n",
            "Train Epoch: 8 [51200/57000 (90%)]\tLoss: 0.008632\n",
            "Train Epoch: 8 [17840/57000 (100%)]\tLoss: 0.010127\n",
            "Eval set: Average loss: 0.0079\n",
            "Train Epoch: 9 [12800/57000 (22%)]\tLoss: 0.009303\n",
            "Train Epoch: 9 [25600/57000 (45%)]\tLoss: 0.009758\n",
            "Train Epoch: 9 [38400/57000 (67%)]\tLoss: 0.009933\n",
            "Train Epoch: 9 [51200/57000 (90%)]\tLoss: 0.010129\n",
            "Train Epoch: 9 [17840/57000 (100%)]\tLoss: 0.009133\n",
            "Eval set: Average loss: 0.0076\n",
            "Train Epoch: 10 [12800/57000 (22%)]\tLoss: 0.009536\n",
            "Train Epoch: 10 [25600/57000 (45%)]\tLoss: 0.009285\n",
            "Train Epoch: 10 [38400/57000 (67%)]\tLoss: 0.008374\n",
            "Train Epoch: 10 [51200/57000 (90%)]\tLoss: 0.009403\n",
            "Train Epoch: 10 [17840/57000 (100%)]\tLoss: 0.009941\n",
            "Eval set: Average loss: 0.0076\n",
            "Train Epoch: 11 [12800/57000 (22%)]\tLoss: 0.008432\n",
            "Train Epoch: 11 [25600/57000 (45%)]\tLoss: 0.009533\n",
            "Train Epoch: 11 [38400/57000 (67%)]\tLoss: 0.008878\n",
            "Train Epoch: 11 [51200/57000 (90%)]\tLoss: 0.009330\n",
            "Train Epoch: 11 [17840/57000 (100%)]\tLoss: 0.008355\n",
            "Eval set: Average loss: 0.0075\n",
            "Train Epoch: 12 [12800/57000 (22%)]\tLoss: 0.008855\n",
            "Train Epoch: 12 [25600/57000 (45%)]\tLoss: 0.008714\n",
            "Train Epoch: 12 [38400/57000 (67%)]\tLoss: 0.009754\n",
            "Train Epoch: 12 [51200/57000 (90%)]\tLoss: 0.009141\n",
            "Train Epoch: 12 [17840/57000 (100%)]\tLoss: 0.008146\n",
            "Eval set: Average loss: 0.0077\n",
            "Train Epoch: 13 [12800/57000 (22%)]\tLoss: 0.009079\n",
            "Train Epoch: 13 [25600/57000 (45%)]\tLoss: 0.009093\n",
            "Train Epoch: 13 [38400/57000 (67%)]\tLoss: 0.008653\n",
            "Train Epoch: 13 [51200/57000 (90%)]\tLoss: 0.007769\n",
            "Train Epoch: 13 [17840/57000 (100%)]\tLoss: 0.011295\n",
            "Eval set: Average loss: 0.0073\n",
            "Train Epoch: 14 [12800/57000 (22%)]\tLoss: 0.009182\n",
            "Train Epoch: 14 [25600/57000 (45%)]\tLoss: 0.009563\n",
            "Train Epoch: 14 [38400/57000 (67%)]\tLoss: 0.008940\n",
            "Train Epoch: 14 [51200/57000 (90%)]\tLoss: 0.009143\n",
            "Train Epoch: 14 [17840/57000 (100%)]\tLoss: 0.008387\n",
            "Eval set: Average loss: 0.0075\n",
            "Train Epoch: 15 [12800/57000 (22%)]\tLoss: 0.009380\n",
            "Train Epoch: 15 [25600/57000 (45%)]\tLoss: 0.008740\n",
            "Train Epoch: 15 [38400/57000 (67%)]\tLoss: 0.008449\n",
            "Train Epoch: 15 [51200/57000 (90%)]\tLoss: 0.008407\n",
            "Train Epoch: 15 [17840/57000 (100%)]\tLoss: 0.008509\n",
            "Eval set: Average loss: 0.0071\n",
            "Train Epoch: 16 [12800/57000 (22%)]\tLoss: 0.007493\n",
            "Train Epoch: 16 [25600/57000 (45%)]\tLoss: 0.009003\n",
            "Train Epoch: 16 [38400/57000 (67%)]\tLoss: 0.008853\n",
            "Train Epoch: 16 [51200/57000 (90%)]\tLoss: 0.008671\n",
            "Train Epoch: 16 [17840/57000 (100%)]\tLoss: 0.008760\n",
            "Eval set: Average loss: 0.0071\n",
            "Train Epoch: 17 [12800/57000 (22%)]\tLoss: 0.009451\n",
            "Train Epoch: 17 [25600/57000 (45%)]\tLoss: 0.008148\n",
            "Train Epoch: 17 [38400/57000 (67%)]\tLoss: 0.008666\n",
            "Train Epoch: 17 [51200/57000 (90%)]\tLoss: 0.008713\n",
            "Train Epoch: 17 [17840/57000 (100%)]\tLoss: 0.008358\n",
            "Eval set: Average loss: 0.0070\n",
            "Train Epoch: 18 [12800/57000 (22%)]\tLoss: 0.008755\n",
            "Train Epoch: 18 [25600/57000 (45%)]\tLoss: 0.008625\n",
            "Train Epoch: 18 [38400/57000 (67%)]\tLoss: 0.008332\n",
            "Train Epoch: 18 [51200/57000 (90%)]\tLoss: 0.008675\n",
            "Train Epoch: 18 [17840/57000 (100%)]\tLoss: 0.008757\n",
            "Eval set: Average loss: 0.0070\n",
            "Train Epoch: 19 [12800/57000 (22%)]\tLoss: 0.007928\n",
            "Train Epoch: 19 [25600/57000 (45%)]\tLoss: 0.008031\n",
            "Train Epoch: 19 [38400/57000 (67%)]\tLoss: 0.008478\n",
            "Train Epoch: 19 [51200/57000 (90%)]\tLoss: 0.008416\n",
            "Train Epoch: 19 [17840/57000 (100%)]\tLoss: 0.008371\n",
            "Eval set: Average loss: 0.0069\n",
            "Train Epoch: 20 [12800/57000 (22%)]\tLoss: 0.008734\n",
            "Train Epoch: 20 [25600/57000 (45%)]\tLoss: 0.008479\n",
            "Train Epoch: 20 [38400/57000 (67%)]\tLoss: 0.009527\n",
            "Train Epoch: 20 [51200/57000 (90%)]\tLoss: 0.008321\n",
            "Train Epoch: 20 [17840/57000 (100%)]\tLoss: 0.008902\n",
            "Eval set: Average loss: 0.0069\n",
            "Train Epoch: 21 [12800/57000 (22%)]\tLoss: 0.008167\n",
            "Train Epoch: 21 [25600/57000 (45%)]\tLoss: 0.007862\n",
            "Train Epoch: 21 [38400/57000 (67%)]\tLoss: 0.008260\n",
            "Train Epoch: 21 [51200/57000 (90%)]\tLoss: 0.008655\n",
            "Train Epoch: 21 [17840/57000 (100%)]\tLoss: 0.008356\n",
            "Eval set: Average loss: 0.0068\n",
            "Train Epoch: 22 [12800/57000 (22%)]\tLoss: 0.008892\n",
            "Train Epoch: 22 [25600/57000 (45%)]\tLoss: 0.007408\n",
            "Train Epoch: 22 [38400/57000 (67%)]\tLoss: 0.008797\n",
            "Train Epoch: 22 [51200/57000 (90%)]\tLoss: 0.008074\n",
            "Train Epoch: 22 [17840/57000 (100%)]\tLoss: 0.007944\n",
            "Eval set: Average loss: 0.0069\n",
            "Train Epoch: 23 [12800/57000 (22%)]\tLoss: 0.007928\n",
            "Train Epoch: 23 [25600/57000 (45%)]\tLoss: 0.008728\n",
            "Train Epoch: 23 [38400/57000 (67%)]\tLoss: 0.007625\n",
            "Train Epoch: 23 [51200/57000 (90%)]\tLoss: 0.008666\n",
            "Train Epoch: 23 [17840/57000 (100%)]\tLoss: 0.008232\n",
            "Eval set: Average loss: 0.0067\n",
            "Train Epoch: 24 [12800/57000 (22%)]\tLoss: 0.008050\n",
            "Train Epoch: 24 [25600/57000 (45%)]\tLoss: 0.008115\n",
            "Train Epoch: 24 [38400/57000 (67%)]\tLoss: 0.008826\n",
            "Train Epoch: 24 [51200/57000 (90%)]\tLoss: 0.008707\n",
            "Train Epoch: 24 [17840/57000 (100%)]\tLoss: 0.007772\n",
            "Eval set: Average loss: 0.0068\n",
            "Train Epoch: 25 [12800/57000 (22%)]\tLoss: 0.008949\n",
            "Train Epoch: 25 [25600/57000 (45%)]\tLoss: 0.008269\n",
            "Train Epoch: 25 [38400/57000 (67%)]\tLoss: 0.008129\n",
            "Train Epoch: 25 [51200/57000 (90%)]\tLoss: 0.008008\n",
            "Train Epoch: 25 [17840/57000 (100%)]\tLoss: 0.008516\n",
            "Eval set: Average loss: 0.0067\n",
            "Train Epoch: 26 [12800/57000 (22%)]\tLoss: 0.008186\n",
            "Train Epoch: 26 [25600/57000 (45%)]\tLoss: 0.008806\n",
            "Train Epoch: 26 [38400/57000 (67%)]\tLoss: 0.008238\n",
            "Train Epoch: 26 [51200/57000 (90%)]\tLoss: 0.007787\n",
            "Train Epoch: 26 [17840/57000 (100%)]\tLoss: 0.007420\n",
            "Eval set: Average loss: 0.0066\n",
            "Train Epoch: 27 [12800/57000 (22%)]\tLoss: 0.008178\n",
            "Train Epoch: 27 [25600/57000 (45%)]\tLoss: 0.008274\n",
            "Train Epoch: 27 [38400/57000 (67%)]\tLoss: 0.007810\n",
            "Train Epoch: 27 [51200/57000 (90%)]\tLoss: 0.008043\n",
            "Train Epoch: 27 [17840/57000 (100%)]\tLoss: 0.009802\n",
            "Eval set: Average loss: 0.0066\n",
            "Train Epoch: 28 [12800/57000 (22%)]\tLoss: 0.008497\n",
            "Train Epoch: 28 [25600/57000 (45%)]\tLoss: 0.008401\n",
            "Train Epoch: 28 [38400/57000 (67%)]\tLoss: 0.008316\n",
            "Train Epoch: 28 [51200/57000 (90%)]\tLoss: 0.007940\n",
            "Train Epoch: 28 [17840/57000 (100%)]\tLoss: 0.007930\n",
            "Eval set: Average loss: 0.0067\n",
            "Train Epoch: 29 [12800/57000 (22%)]\tLoss: 0.007549\n",
            "Train Epoch: 29 [25600/57000 (45%)]\tLoss: 0.007909\n",
            "Train Epoch: 29 [38400/57000 (67%)]\tLoss: 0.007602\n",
            "Train Epoch: 29 [51200/57000 (90%)]\tLoss: 0.008614\n",
            "Train Epoch: 29 [17840/57000 (100%)]\tLoss: 0.009818\n",
            "Eval set: Average loss: 0.0067\n",
            "Train Epoch: 30 [12800/57000 (22%)]\tLoss: 0.008066\n",
            "Train Epoch: 30 [25600/57000 (45%)]\tLoss: 0.007611\n",
            "Train Epoch: 30 [38400/57000 (67%)]\tLoss: 0.007978\n",
            "Train Epoch: 30 [51200/57000 (90%)]\tLoss: 0.008550\n",
            "Train Epoch: 30 [17840/57000 (100%)]\tLoss: 0.007896\n",
            "Eval set: Average loss: 0.0065\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAADzCAYAAADD2rFtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJvL/AWr0ajqx3BQAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nl_iO9i-idNf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7e504a57-ae0e-494e-9ccd-ccf809dfc2f6"
      },
      "source": [
        "autoen3 = train_net(model3, train_loader, val_loader, optimizer3, hparams['num_epochs'])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [12800/57000 (22%)]\tLoss: 0.077523\n",
            "Train Epoch: 1 [25600/57000 (45%)]\tLoss: 0.031343\n",
            "Train Epoch: 1 [38400/57000 (67%)]\tLoss: 0.020099\n",
            "Train Epoch: 1 [51200/57000 (90%)]\tLoss: 0.017135\n",
            "Train Epoch: 1 [17840/57000 (100%)]\tLoss: 0.018381\n",
            "Eval set: Average loss: 0.0131\n",
            "Train Epoch: 2 [12800/57000 (22%)]\tLoss: 0.014815\n",
            "Train Epoch: 2 [25600/57000 (45%)]\tLoss: 0.013195\n",
            "Train Epoch: 2 [38400/57000 (67%)]\tLoss: 0.011474\n",
            "Train Epoch: 2 [51200/57000 (90%)]\tLoss: 0.011424\n",
            "Train Epoch: 2 [17840/57000 (100%)]\tLoss: 0.010789\n",
            "Eval set: Average loss: 0.0084\n",
            "Train Epoch: 3 [12800/57000 (22%)]\tLoss: 0.009599\n",
            "Train Epoch: 3 [25600/57000 (45%)]\tLoss: 0.009326\n",
            "Train Epoch: 3 [38400/57000 (67%)]\tLoss: 0.008820\n",
            "Train Epoch: 3 [51200/57000 (90%)]\tLoss: 0.009161\n",
            "Train Epoch: 3 [17840/57000 (100%)]\tLoss: 0.009186\n",
            "Eval set: Average loss: 0.0069\n",
            "Train Epoch: 4 [12800/57000 (22%)]\tLoss: 0.007831\n",
            "Train Epoch: 4 [25600/57000 (45%)]\tLoss: 0.008438\n",
            "Train Epoch: 4 [38400/57000 (67%)]\tLoss: 0.007225\n",
            "Train Epoch: 4 [51200/57000 (90%)]\tLoss: 0.007744\n",
            "Train Epoch: 4 [17840/57000 (100%)]\tLoss: 0.007458\n",
            "Eval set: Average loss: 0.0062\n",
            "Train Epoch: 5 [12800/57000 (22%)]\tLoss: 0.006748\n",
            "Train Epoch: 5 [25600/57000 (45%)]\tLoss: 0.007345\n",
            "Train Epoch: 5 [38400/57000 (67%)]\tLoss: 0.007133\n",
            "Train Epoch: 5 [51200/57000 (90%)]\tLoss: 0.007279\n",
            "Train Epoch: 5 [17840/57000 (100%)]\tLoss: 0.008231\n",
            "Eval set: Average loss: 0.0056\n",
            "Train Epoch: 6 [12800/57000 (22%)]\tLoss: 0.006317\n",
            "Train Epoch: 6 [25600/57000 (45%)]\tLoss: 0.006881\n",
            "Train Epoch: 6 [38400/57000 (67%)]\tLoss: 0.006954\n",
            "Train Epoch: 6 [51200/57000 (90%)]\tLoss: 0.006676\n",
            "Train Epoch: 6 [17840/57000 (100%)]\tLoss: 0.006537\n",
            "Eval set: Average loss: 0.0053\n",
            "Train Epoch: 7 [12800/57000 (22%)]\tLoss: 0.006048\n",
            "Train Epoch: 7 [25600/57000 (45%)]\tLoss: 0.005762\n",
            "Train Epoch: 7 [38400/57000 (67%)]\tLoss: 0.006088\n",
            "Train Epoch: 7 [51200/57000 (90%)]\tLoss: 0.006326\n",
            "Train Epoch: 7 [17840/57000 (100%)]\tLoss: 0.005866\n",
            "Eval set: Average loss: 0.0051\n",
            "Train Epoch: 8 [12800/57000 (22%)]\tLoss: 0.005830\n",
            "Train Epoch: 8 [25600/57000 (45%)]\tLoss: 0.006260\n",
            "Train Epoch: 8 [38400/57000 (67%)]\tLoss: 0.006261\n",
            "Train Epoch: 8 [51200/57000 (90%)]\tLoss: 0.006346\n",
            "Train Epoch: 8 [17840/57000 (100%)]\tLoss: 0.006954\n",
            "Eval set: Average loss: 0.0049\n",
            "Train Epoch: 9 [12800/57000 (22%)]\tLoss: 0.005421\n",
            "Train Epoch: 9 [25600/57000 (45%)]\tLoss: 0.005576\n",
            "Train Epoch: 9 [38400/57000 (67%)]\tLoss: 0.005607\n",
            "Train Epoch: 9 [51200/57000 (90%)]\tLoss: 0.005665\n",
            "Train Epoch: 9 [17840/57000 (100%)]\tLoss: 0.005578\n",
            "Eval set: Average loss: 0.0046\n",
            "Train Epoch: 10 [12800/57000 (22%)]\tLoss: 0.005260\n",
            "Train Epoch: 10 [25600/57000 (45%)]\tLoss: 0.005609\n",
            "Train Epoch: 10 [38400/57000 (67%)]\tLoss: 0.005781\n",
            "Train Epoch: 10 [51200/57000 (90%)]\tLoss: 0.005751\n",
            "Train Epoch: 10 [17840/57000 (100%)]\tLoss: 0.005932\n",
            "Eval set: Average loss: 0.0045\n",
            "Train Epoch: 11 [12800/57000 (22%)]\tLoss: 0.005508\n",
            "Train Epoch: 11 [25600/57000 (45%)]\tLoss: 0.005377\n",
            "Train Epoch: 11 [38400/57000 (67%)]\tLoss: 0.005558\n",
            "Train Epoch: 11 [51200/57000 (90%)]\tLoss: 0.005678\n",
            "Train Epoch: 11 [17840/57000 (100%)]\tLoss: 0.005456\n",
            "Eval set: Average loss: 0.0044\n",
            "Train Epoch: 12 [12800/57000 (22%)]\tLoss: 0.005282\n",
            "Train Epoch: 12 [25600/57000 (45%)]\tLoss: 0.005426\n",
            "Train Epoch: 12 [38400/57000 (67%)]\tLoss: 0.005265\n",
            "Train Epoch: 12 [51200/57000 (90%)]\tLoss: 0.005204\n",
            "Train Epoch: 12 [17840/57000 (100%)]\tLoss: 0.005574\n",
            "Eval set: Average loss: 0.0043\n",
            "Train Epoch: 13 [12800/57000 (22%)]\tLoss: 0.005100\n",
            "Train Epoch: 13 [25600/57000 (45%)]\tLoss: 0.005308\n",
            "Train Epoch: 13 [38400/57000 (67%)]\tLoss: 0.005481\n",
            "Train Epoch: 13 [51200/57000 (90%)]\tLoss: 0.004742\n",
            "Train Epoch: 13 [17840/57000 (100%)]\tLoss: 0.004801\n",
            "Eval set: Average loss: 0.0043\n",
            "Train Epoch: 14 [12800/57000 (22%)]\tLoss: 0.005002\n",
            "Train Epoch: 14 [25600/57000 (45%)]\tLoss: 0.005057\n",
            "Train Epoch: 14 [38400/57000 (67%)]\tLoss: 0.005500\n",
            "Train Epoch: 14 [51200/57000 (90%)]\tLoss: 0.004949\n",
            "Train Epoch: 14 [17840/57000 (100%)]\tLoss: 0.004930\n",
            "Eval set: Average loss: 0.0041\n",
            "Train Epoch: 15 [12800/57000 (22%)]\tLoss: 0.004786\n",
            "Train Epoch: 15 [25600/57000 (45%)]\tLoss: 0.005301\n",
            "Train Epoch: 15 [38400/57000 (67%)]\tLoss: 0.005095\n",
            "Train Epoch: 15 [51200/57000 (90%)]\tLoss: 0.004805\n",
            "Train Epoch: 15 [17840/57000 (100%)]\tLoss: 0.004466\n",
            "Eval set: Average loss: 0.0040\n",
            "Train Epoch: 16 [12800/57000 (22%)]\tLoss: 0.004903\n",
            "Train Epoch: 16 [25600/57000 (45%)]\tLoss: 0.004772\n",
            "Train Epoch: 16 [38400/57000 (67%)]\tLoss: 0.004890\n",
            "Train Epoch: 16 [51200/57000 (90%)]\tLoss: 0.005265\n",
            "Train Epoch: 16 [17840/57000 (100%)]\tLoss: 0.004521\n",
            "Eval set: Average loss: 0.0040\n",
            "Train Epoch: 17 [12800/57000 (22%)]\tLoss: 0.004486\n",
            "Train Epoch: 17 [25600/57000 (45%)]\tLoss: 0.004471\n",
            "Train Epoch: 17 [38400/57000 (67%)]\tLoss: 0.004529\n",
            "Train Epoch: 17 [51200/57000 (90%)]\tLoss: 0.004732\n",
            "Train Epoch: 17 [17840/57000 (100%)]\tLoss: 0.004757\n",
            "Eval set: Average loss: 0.0039\n",
            "Train Epoch: 18 [12800/57000 (22%)]\tLoss: 0.004826\n",
            "Train Epoch: 18 [25600/57000 (45%)]\tLoss: 0.004798\n",
            "Train Epoch: 18 [38400/57000 (67%)]\tLoss: 0.004841\n",
            "Train Epoch: 18 [51200/57000 (90%)]\tLoss: 0.004751\n",
            "Train Epoch: 18 [17840/57000 (100%)]\tLoss: 0.004955\n",
            "Eval set: Average loss: 0.0039\n",
            "Train Epoch: 19 [12800/57000 (22%)]\tLoss: 0.004574\n",
            "Train Epoch: 19 [25600/57000 (45%)]\tLoss: 0.004741\n",
            "Train Epoch: 19 [38400/57000 (67%)]\tLoss: 0.004196\n",
            "Train Epoch: 19 [51200/57000 (90%)]\tLoss: 0.004547\n",
            "Train Epoch: 19 [17840/57000 (100%)]\tLoss: 0.004644\n",
            "Eval set: Average loss: 0.0038\n",
            "Train Epoch: 20 [12800/57000 (22%)]\tLoss: 0.004508\n",
            "Train Epoch: 20 [25600/57000 (45%)]\tLoss: 0.004613\n",
            "Train Epoch: 20 [38400/57000 (67%)]\tLoss: 0.004468\n",
            "Train Epoch: 20 [51200/57000 (90%)]\tLoss: 0.004395\n",
            "Train Epoch: 20 [17840/57000 (100%)]\tLoss: 0.004081\n",
            "Eval set: Average loss: 0.0037\n",
            "Train Epoch: 21 [12800/57000 (22%)]\tLoss: 0.004570\n",
            "Train Epoch: 21 [25600/57000 (45%)]\tLoss: 0.004639\n",
            "Train Epoch: 21 [38400/57000 (67%)]\tLoss: 0.004222\n",
            "Train Epoch: 21 [51200/57000 (90%)]\tLoss: 0.004143\n",
            "Train Epoch: 21 [17840/57000 (100%)]\tLoss: 0.004772\n",
            "Eval set: Average loss: 0.0037\n",
            "Train Epoch: 22 [12800/57000 (22%)]\tLoss: 0.004566\n",
            "Train Epoch: 22 [25600/57000 (45%)]\tLoss: 0.004813\n",
            "Train Epoch: 22 [38400/57000 (67%)]\tLoss: 0.004496\n",
            "Train Epoch: 22 [51200/57000 (90%)]\tLoss: 0.004405\n",
            "Train Epoch: 22 [17840/57000 (100%)]\tLoss: 0.005047\n",
            "Eval set: Average loss: 0.0037\n",
            "Train Epoch: 23 [12800/57000 (22%)]\tLoss: 0.004639\n",
            "Train Epoch: 23 [25600/57000 (45%)]\tLoss: 0.004527\n",
            "Train Epoch: 23 [38400/57000 (67%)]\tLoss: 0.004395\n",
            "Train Epoch: 23 [51200/57000 (90%)]\tLoss: 0.004710\n",
            "Train Epoch: 23 [17840/57000 (100%)]\tLoss: 0.004269\n",
            "Eval set: Average loss: 0.0036\n",
            "Train Epoch: 24 [12800/57000 (22%)]\tLoss: 0.004406\n",
            "Train Epoch: 24 [25600/57000 (45%)]\tLoss: 0.004074\n",
            "Train Epoch: 24 [38400/57000 (67%)]\tLoss: 0.004176\n",
            "Train Epoch: 24 [51200/57000 (90%)]\tLoss: 0.004270\n",
            "Train Epoch: 24 [17840/57000 (100%)]\tLoss: 0.004014\n",
            "Eval set: Average loss: 0.0036\n",
            "Train Epoch: 25 [12800/57000 (22%)]\tLoss: 0.004289\n",
            "Train Epoch: 25 [25600/57000 (45%)]\tLoss: 0.004211\n",
            "Train Epoch: 25 [38400/57000 (67%)]\tLoss: 0.004126\n",
            "Train Epoch: 25 [51200/57000 (90%)]\tLoss: 0.004957\n",
            "Train Epoch: 25 [17840/57000 (100%)]\tLoss: 0.004342\n",
            "Eval set: Average loss: 0.0035\n",
            "Train Epoch: 26 [12800/57000 (22%)]\tLoss: 0.004511\n",
            "Train Epoch: 26 [25600/57000 (45%)]\tLoss: 0.004597\n",
            "Train Epoch: 26 [38400/57000 (67%)]\tLoss: 0.004295\n",
            "Train Epoch: 26 [51200/57000 (90%)]\tLoss: 0.004541\n",
            "Train Epoch: 26 [17840/57000 (100%)]\tLoss: 0.004379\n",
            "Eval set: Average loss: 0.0035\n",
            "Train Epoch: 27 [12800/57000 (22%)]\tLoss: 0.004406\n",
            "Train Epoch: 27 [25600/57000 (45%)]\tLoss: 0.004171\n",
            "Train Epoch: 27 [38400/57000 (67%)]\tLoss: 0.004074\n",
            "Train Epoch: 27 [51200/57000 (90%)]\tLoss: 0.004251\n",
            "Train Epoch: 27 [17840/57000 (100%)]\tLoss: 0.003993\n",
            "Eval set: Average loss: 0.0035\n",
            "Train Epoch: 28 [12800/57000 (22%)]\tLoss: 0.004058\n",
            "Train Epoch: 28 [25600/57000 (45%)]\tLoss: 0.004450\n",
            "Train Epoch: 28 [38400/57000 (67%)]\tLoss: 0.004500\n",
            "Train Epoch: 28 [51200/57000 (90%)]\tLoss: 0.004120\n",
            "Train Epoch: 28 [17840/57000 (100%)]\tLoss: 0.003960\n",
            "Eval set: Average loss: 0.0035\n",
            "Train Epoch: 29 [12800/57000 (22%)]\tLoss: 0.004102\n",
            "Train Epoch: 29 [25600/57000 (45%)]\tLoss: 0.004408\n",
            "Train Epoch: 29 [38400/57000 (67%)]\tLoss: 0.004115\n",
            "Train Epoch: 29 [51200/57000 (90%)]\tLoss: 0.004381\n",
            "Train Epoch: 29 [17840/57000 (100%)]\tLoss: 0.004205\n",
            "Eval set: Average loss: 0.0035\n",
            "Train Epoch: 30 [12800/57000 (22%)]\tLoss: 0.004212\n",
            "Train Epoch: 30 [25600/57000 (45%)]\tLoss: 0.004248\n",
            "Train Epoch: 30 [38400/57000 (67%)]\tLoss: 0.004426\n",
            "Train Epoch: 30 [51200/57000 (90%)]\tLoss: 0.004068\n",
            "Train Epoch: 30 [17840/57000 (100%)]\tLoss: 0.004156\n",
            "Eval set: Average loss: 0.0035\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAADzCAYAAADD2rFtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p4hD1vzHhkEv",
        "colab_type": "text"
      },
      "source": [
        "We can see how, the bigger the bottleneck and hence the smaller the compression, the smaller the loss.\n",
        "\n",
        "Let's plot all the curves in the same chart:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke_46veTh83x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 514
        },
        "outputId": "cad11f15-cc6c-4dff-b504-7558087018a7"
      },
      "source": [
        "train_loss1 = autoen1[\"tr_losses\"]\n",
        "val_loss1 = autoen1[\"te_losses\"]\n",
        "\n",
        "train_loss2 = autoen2[\"tr_losses\"]\n",
        "val_loss2 = autoen2[\"te_losses\"]\n",
        "\n",
        "train_loss3 = autoen3[\"tr_losses\"]\n",
        "val_loss3 = autoen3[\"te_losses\"]\n",
        "\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('NLLLoss')\n",
        "plt.plot(train_loss1, label='Autoencoder1_Train')\n",
        "plt.plot(val_loss1, label='Autoencoder1_Eval')\n",
        "plt.plot(train_loss2, label='Autoencoder2_Train')\n",
        "plt.plot(val_loss2, label='Autoencoder2_Eval')\n",
        "plt.plot(train_loss3, label='Autoencoder3_Train3')\n",
        "plt.plot(val_loss3, label='Autoencoder3_Eval3')\n",
        "plt.legend()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f1e02a54a90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAHgCAYAAADg78rsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n9RCGMyIiIqIe8v8AGcQHuLRwRNQAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x576 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wwqnxRRvTp_",
        "colab_type": "text"
      },
      "source": [
        "Now we need to define the **testing**:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCIfFkqovUP8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_model(network, test_loader):\n",
        "  network.eval()\n",
        "  test_loss_avg = 0\n",
        "  num_batches = 0\n",
        "  for data, _ in test_loader:\n",
        "      with torch.no_grad():\n",
        "          data = data.to(device)\n",
        "          data_recon = network(data)\n",
        "          loss = F.mse_loss(data_recon, data)\n",
        "          test_loss_avg += loss.item()\n",
        "          num_batches += 1  \n",
        "  test_loss_avg /= num_batches\n",
        "  print('Average Reconstruction Error: %f' % (test_loss_avg))\n",
        "  return test_loss_avg"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbhL3e6J2zDQ",
        "colab_type": "text"
      },
      "source": [
        "And compute the testing set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjIwi0Vu22o-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "24f0d895-a953-4bc3-e277-1a0c97b5da7b"
      },
      "source": [
        "test_autoen1 = test_model(model1, test_loader)\n",
        "test_autoen2 = test_model(model2, test_loader)\n",
        "test_autoen3 = test_model(model3, test_loader)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average Reconstruction Error: 0.048085\n",
            "Average Reconstruction Error: 0.008026\n",
            "Average Reconstruction Error: 0.004259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3uZSjWY23z2",
        "colab_type": "text"
      },
      "source": [
        "Let's wrapp up our results so far:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xovN4jvB296U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "4438e3c5-5491-4009-b8e7-280e93eaa371"
      },
      "source": [
        "print(tabulate([\n",
        "                ['Autoencoder1(10)', mean(val_loss1), test_autoen1, num_params1], \n",
        "                ['Autoencoder2(50)', mean(val_loss2), test_autoen2, num_params2], \n",
        "                ['Autoencoder3(100)', mean(val_loss3), test_autoen3, num_params3]\n",
        "                ], \n",
        "               headers=['Model(Bottleneck size)', 'MSE_val', 'MSE_test', 'Num_params']\n",
        "               )\n",
        ")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model(Bottleneck size)       MSE_val    MSE_test    Num_params\n",
            "------------------------  ----------  ----------  ------------\n",
            "Autoencoder1(10)          0.0416566   0.0480854         396171\n",
            "Autoencoder2(50)          0.00775763  0.00802607        897971\n",
            "Autoencoder3(100)         0.0046924   0.00425861       1525221\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kfi_tCY3Doe",
        "colab_type": "text"
      },
      "source": [
        "Good, but we better check how they look like and compare with our input data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TuCOe-LO3Lsy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "a4ab836e-a8e5-4bb3-c7f9-2cdd9d77dd49"
      },
      "source": [
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    return x\n",
        "\n",
        "def show_image(img):\n",
        "    img = to_img(img)\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "def visualise_output(images, model):\n",
        "    with torch.no_grad():\n",
        "        images = images.to(device)\n",
        "        images = model(images)\n",
        "        images = images.cpu()\n",
        "        images = to_img(images)\n",
        "        np_imagegrid = utils.make_grid(images[0:5],5, 5).numpy()\n",
        "        plt.imshow(np.transpose(np_imagegrid, (1, 2, 0)))\n",
        "        plt.show()\n",
        "\n",
        "from random import sample, randint\n",
        "images, _ = iter(test_loader).next()\n",
        "model1.eval()\n",
        "print('Using model1(Bottleneck = 10)')\n",
        "print('---------------------------------')\n",
        "# First visualise the original images\n",
        "print('Original images')\n",
        "show_image(utils.make_grid(images[0:5],5,5))\n",
        "plt.show()\n",
        "# Reconstruct and visualise the images using the autoencoder\n",
        "print('Autoencoder reconstruction:')\n",
        "visualise_output(images, model1)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using model1(Bottleneck = 10)\n",
            "---------------------------------\n",
            "Original images\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABtCAYAAAC4JjrOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Autoencoder reconstruction:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABtCAYAAAC4JjrOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngoCjYbqyxAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GIMpLy5f3NJa",
        "colab_type": "text"
      },
      "source": [
        "What if we **inject noise** in the decoder's input?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XiGkb4txdjC9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        },
        "outputId": "a5454091-ae9e-44ed-ca6b-6107bef49ab7"
      },
      "source": [
        "model1.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "  images, labels = iter(test_loader).next()\n",
        "  images = images.to(device)\n",
        "  latent = model1.encoder(images)\n",
        "  latent = latent.view(latent.size(0), -1)\n",
        "  latent = model1.linear1(latent)\n",
        "  latent = latent.cpu()\n",
        "\n",
        "  mean = latent.mean(dim=0)\n",
        "  std = (latent - mean).pow(2).mean(dim=0).sqrt()\n",
        "\n",
        "  latent = torch.randn(128, 10)*std + mean\n",
        "\n",
        "  # reconstruct images from the latent vectors\n",
        "  latent = latent.to(device)\n",
        "  img_recon = model1.linear2(latent)\n",
        "  img_recon = img_recon.view(img_recon.size(0), hparams['hidden2'], 7, 7)\n",
        "  img_recon = model1.relu(img_recon)\n",
        "  img_recon = model1.decoder(img_recon)\n",
        "  img_recon = img_recon.cpu()\n",
        "\n",
        "  show_image(utils.make_grid(img_recon[:5],5,5))\n",
        "  plt.show()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAABtCAYAAAC4JjrOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n7eoOAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6MwNZVp3hM41",
        "colab_type": "text"
      },
      "source": [
        "# **Exercise 2: Transfer Learning**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4-dErfzhUQR",
        "colab_type": "text"
      },
      "source": [
        "In this exercise we will reuse the encoding part of Model1 of the exercise 1 to classify the images into their corresponding number. To achieve that we will build several approaches and afterwards we will compare them.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXZj-hkI2FBe",
        "colab_type": "text"
      },
      "source": [
        "First of all we will get a subset of 100 images from our training set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YpmPhboy2KIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "mnist_trainset2, _ = torch.utils.data.random_split(mnist_trainset, [int(len(mnist_trainset)/570),int(len(mnist_trainset)*569/570)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qy3GeVqq28QM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader2 = torch.utils.data.DataLoader(\n",
        "    mnist_trainset2,\n",
        "    batch_size=hparams['batch_size'], \n",
        "    shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziTsOOSi3GM5",
        "colab_type": "text"
      },
      "source": [
        "And we will use, as said, the model1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E3buRnA73MRK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex2model1 = copy.deepcopy(model1)\n",
        "ex2model2 = copy.deepcopy(model1)\n",
        "ex2model3 = autoencoder1(hparams).cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_1n0sEPW3bZL",
        "colab_type": "text"
      },
      "source": [
        "And now we will create a digit (0-9) classification model **reusing the encoder of the autoencoder** and adding what is needed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUxO0GwX76gJ",
        "colab_type": "text"
      },
      "source": [
        "First we define the training for this particular problem"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Pwi_T5W6DKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch2(train_loader, network, optimizer, criterion, hparams, epoch):\n",
        "  network.train()\n",
        "  device = hparams['device']\n",
        "  losses = []\n",
        "  for batch_idx, (data, target) in enumerate(train_loader, 1):\n",
        "      data = data.to(device)\n",
        "      target = target.to(device)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      #Using the encoder + linear + softmax\n",
        "      output = network.encoder(data)\n",
        "      output = output.view(output.size(0), -1)\n",
        "      output = network.linear1(output)\n",
        "      output = network.softmax(output)\n",
        "\n",
        "      loss = criterion(output, target)\n",
        "      loss.backward()\n",
        "      losses.append(loss.item())\n",
        "      optimizer.step()\n",
        "      if batch_idx % hparams['log_interval'] == 0 or batch_idx >= len(train_loader):\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "              epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "              100. * batch_idx / len(train_loader), loss.item()\n",
        "              ))\n",
        "  return np.mean(losses)\n",
        "\n",
        "def train_net2(network, train_loader, optimizer, num_epochs, criterion):\n",
        "  tr_losses = []\n",
        "\n",
        "  network.to(hparams['device'])\n",
        "\n",
        "  for epoch in range(1, num_epochs + 1):\n",
        "    tr_loss = train_epoch2(train_loader, network, optimizer, criterion, hparams, epoch)\n",
        "    tr_losses.append(tr_loss)\n",
        "  rets = {'tr_losses':tr_losses}\n",
        "  return rets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UEXpBjZM5nTP",
        "colab_type": "text"
      },
      "source": [
        "The first approach will be to use **pre-training**, which is using the weights of the autoencoder as initial values for the network weights and train a classification model on the subset of 100 samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "my-JmNuB8kFR",
        "colab_type": "text"
      },
      "source": [
        "Criterion and optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "609dGGTr3kDs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex2criterion = nn.CrossEntropyLoss()\n",
        "ex2optimizer1 = torch.optim.Adam(ex2model1.parameters(), lr = hparams['learning_rate'],\n",
        "                             weight_decay = 1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVbD3JhV8qgA",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2H55GCX68rcx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0e98b059-a849-4114-c4f9-547ee494a450"
      },
      "source": [
        "classifier1 = train_net2(ex2model1, train_loader2, ex2optimizer1, hparams['num_epochs2'], ex2criterion)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [100/100 (100%)]\tLoss: 2.346251\n",
            "Train Epoch: 2 [100/100 (100%)]\tLoss: 2.325722\n",
            "Train Epoch: 3 [100/100 (100%)]\tLoss: 2.304563\n",
            "Train Epoch: 4 [100/100 (100%)]\tLoss: 2.282768\n",
            "Train Epoch: 5 [100/100 (100%)]\tLoss: 2.259653\n",
            "Train Epoch: 6 [100/100 (100%)]\tLoss: 2.234435\n",
            "Train Epoch: 7 [100/100 (100%)]\tLoss: 2.206271\n",
            "Train Epoch: 8 [100/100 (100%)]\tLoss: 2.175637\n",
            "Train Epoch: 9 [100/100 (100%)]\tLoss: 2.144656\n",
            "Train Epoch: 10 [100/100 (100%)]\tLoss: 2.116192\n",
            "Train Epoch: 11 [100/100 (100%)]\tLoss: 2.088499\n",
            "Train Epoch: 12 [100/100 (100%)]\tLoss: 2.056522\n",
            "Train Epoch: 13 [100/100 (100%)]\tLoss: 2.018424\n",
            "Train Epoch: 14 [100/100 (100%)]\tLoss: 1.978335\n",
            "Train Epoch: 15 [100/100 (100%)]\tLoss: 1.940467\n",
            "Train Epoch: 16 [100/100 (100%)]\tLoss: 1.903726\n",
            "Train Epoch: 17 [100/100 (100%)]\tLoss: 1.872588\n",
            "Train Epoch: 18 [100/100 (100%)]\tLoss: 1.847575\n",
            "Train Epoch: 19 [100/100 (100%)]\tLoss: 1.824475\n",
            "Train Epoch: 20 [100/100 (100%)]\tLoss: 1.801901\n",
            "Train Epoch: 21 [100/100 (100%)]\tLoss: 1.781543\n",
            "Train Epoch: 22 [100/100 (100%)]\tLoss: 1.764661\n",
            "Train Epoch: 23 [100/100 (100%)]\tLoss: 1.751812\n",
            "Train Epoch: 24 [100/100 (100%)]\tLoss: 1.742551\n",
            "Train Epoch: 25 [100/100 (100%)]\tLoss: 1.735040\n",
            "Train Epoch: 26 [100/100 (100%)]\tLoss: 1.729733\n",
            "Train Epoch: 27 [100/100 (100%)]\tLoss: 1.726559\n",
            "Train Epoch: 28 [100/100 (100%)]\tLoss: 1.722927\n",
            "Train Epoch: 29 [100/100 (100%)]\tLoss: 1.716243\n",
            "Train Epoch: 30 [100/100 (100%)]\tLoss: 1.705288\n",
            "Train Epoch: 31 [100/100 (100%)]\tLoss: 1.692617\n",
            "Train Epoch: 32 [100/100 (100%)]\tLoss: 1.679824\n",
            "Train Epoch: 33 [100/100 (100%)]\tLoss: 1.665335\n",
            "Train Epoch: 34 [100/100 (100%)]\tLoss: 1.662151\n",
            "Train Epoch: 35 [100/100 (100%)]\tLoss: 1.657168\n",
            "Train Epoch: 36 [100/100 (100%)]\tLoss: 1.648367\n",
            "Train Epoch: 37 [100/100 (100%)]\tLoss: 1.643415\n",
            "Train Epoch: 38 [100/100 (100%)]\tLoss: 1.641057\n",
            "Train Epoch: 39 [100/100 (100%)]\tLoss: 1.639919\n",
            "Train Epoch: 40 [100/100 (100%)]\tLoss: 1.639122\n",
            "Train Epoch: 41 [100/100 (100%)]\tLoss: 1.638345\n",
            "Train Epoch: 42 [100/100 (100%)]\tLoss: 1.637685\n",
            "Train Epoch: 43 [100/100 (100%)]\tLoss: 1.637248\n",
            "Train Epoch: 44 [100/100 (100%)]\tLoss: 1.636936\n",
            "Train Epoch: 45 [100/100 (100%)]\tLoss: 1.636608\n",
            "Train Epoch: 46 [100/100 (100%)]\tLoss: 1.636341\n",
            "Train Epoch: 47 [100/100 (100%)]\tLoss: 1.636247\n",
            "Train Epoch: 48 [100/100 (100%)]\tLoss: 1.636258\n",
            "Train Epoch: 49 [100/100 (100%)]\tLoss: 1.636233\n",
            "Train Epoch: 50 [100/100 (100%)]\tLoss: 1.636102\n",
            "Train Epoch: 51 [100/100 (100%)]\tLoss: 1.635841\n",
            "Train Epoch: 52 [100/100 (100%)]\tLoss: 1.635529\n",
            "Train Epoch: 53 [100/100 (100%)]\tLoss: 1.635328\n",
            "Train Epoch: 54 [100/100 (100%)]\tLoss: 1.635219\n",
            "Train Epoch: 55 [100/100 (100%)]\tLoss: 1.634945\n",
            "Train Epoch: 56 [100/100 (100%)]\tLoss: 1.634041\n",
            "Train Epoch: 57 [100/100 (100%)]\tLoss: 1.631322\n",
            "Train Epoch: 58 [100/100 (100%)]\tLoss: 1.627558\n",
            "Train Epoch: 59 [100/100 (100%)]\tLoss: 1.626779\n",
            "Train Epoch: 60 [100/100 (100%)]\tLoss: 1.626871\n",
            "Train Epoch: 61 [100/100 (100%)]\tLoss: 1.626889\n",
            "Train Epoch: 62 [100/100 (100%)]\tLoss: 1.626565\n",
            "Train Epoch: 63 [100/100 (100%)]\tLoss: 1.625870\n",
            "Train Epoch: 64 [100/100 (100%)]\tLoss: 1.625180\n",
            "Train Epoch: 65 [100/100 (100%)]\tLoss: 1.624961\n",
            "Train Epoch: 66 [100/100 (100%)]\tLoss: 1.625031\n",
            "Train Epoch: 67 [100/100 (100%)]\tLoss: 1.625094\n",
            "Train Epoch: 68 [100/100 (100%)]\tLoss: 1.625094\n",
            "Train Epoch: 69 [100/100 (100%)]\tLoss: 1.625027\n",
            "Train Epoch: 70 [100/100 (100%)]\tLoss: 1.624884\n",
            "Train Epoch: 71 [100/100 (100%)]\tLoss: 1.624668\n",
            "Train Epoch: 72 [100/100 (100%)]\tLoss: 1.624412\n",
            "Train Epoch: 73 [100/100 (100%)]\tLoss: 1.624180\n",
            "Train Epoch: 74 [100/100 (100%)]\tLoss: 1.624066\n",
            "Train Epoch: 75 [100/100 (100%)]\tLoss: 1.624069\n",
            "Train Epoch: 76 [100/100 (100%)]\tLoss: 1.624107\n",
            "Train Epoch: 77 [100/100 (100%)]\tLoss: 1.624137\n",
            "Train Epoch: 78 [100/100 (100%)]\tLoss: 1.624146\n",
            "Train Epoch: 79 [100/100 (100%)]\tLoss: 1.624127\n",
            "Train Epoch: 80 [100/100 (100%)]\tLoss: 1.624078\n",
            "Train Epoch: 81 [100/100 (100%)]\tLoss: 1.624007\n",
            "Train Epoch: 82 [100/100 (100%)]\tLoss: 1.623922\n",
            "Train Epoch: 83 [100/100 (100%)]\tLoss: 1.623837\n",
            "Train Epoch: 84 [100/100 (100%)]\tLoss: 1.623769\n",
            "Train Epoch: 85 [100/100 (100%)]\tLoss: 1.623729\n",
            "Train Epoch: 86 [100/100 (100%)]\tLoss: 1.623712\n",
            "Train Epoch: 87 [100/100 (100%)]\tLoss: 1.623697\n",
            "Train Epoch: 88 [100/100 (100%)]\tLoss: 1.623673\n",
            "Train Epoch: 89 [100/100 (100%)]\tLoss: 1.623641\n",
            "Train Epoch: 90 [100/100 (100%)]\tLoss: 1.623609\n",
            "Train Epoch: 91 [100/100 (100%)]\tLoss: 1.623584\n",
            "Train Epoch: 92 [100/100 (100%)]\tLoss: 1.623566\n",
            "Train Epoch: 93 [100/100 (100%)]\tLoss: 1.623548\n",
            "Train Epoch: 94 [100/100 (100%)]\tLoss: 1.623524\n",
            "Train Epoch: 95 [100/100 (100%)]\tLoss: 1.623493\n",
            "Train Epoch: 96 [100/100 (100%)]\tLoss: 1.623458\n",
            "Train Epoch: 97 [100/100 (100%)]\tLoss: 1.623430\n",
            "Train Epoch: 98 [100/100 (100%)]\tLoss: 1.623411\n",
            "Train Epoch: 99 [100/100 (100%)]\tLoss: 1.623400\n",
            "Train Epoch: 100 [100/100 (100%)]\tLoss: 1.623391\n",
            "Train Epoch: 101 [100/100 (100%)]\tLoss: 1.623378\n",
            "Train Epoch: 102 [100/100 (100%)]\tLoss: 1.623362\n",
            "Train Epoch: 103 [100/100 (100%)]\tLoss: 1.623345\n",
            "Train Epoch: 104 [100/100 (100%)]\tLoss: 1.623331\n",
            "Train Epoch: 105 [100/100 (100%)]\tLoss: 1.623320\n",
            "Train Epoch: 106 [100/100 (100%)]\tLoss: 1.623312\n",
            "Train Epoch: 107 [100/100 (100%)]\tLoss: 1.623302\n",
            "Train Epoch: 108 [100/100 (100%)]\tLoss: 1.623290\n",
            "Train Epoch: 109 [100/100 (100%)]\tLoss: 1.623276\n",
            "Train Epoch: 110 [100/100 (100%)]\tLoss: 1.623263\n",
            "Train Epoch: 111 [100/100 (100%)]\tLoss: 1.623253\n",
            "Train Epoch: 112 [100/100 (100%)]\tLoss: 1.623244\n",
            "Train Epoch: 113 [100/100 (100%)]\tLoss: 1.623235\n",
            "Train Epoch: 114 [100/100 (100%)]\tLoss: 1.623225\n",
            "Train Epoch: 115 [100/100 (100%)]\tLoss: 1.623214\n",
            "Train Epoch: 116 [100/100 (100%)]\tLoss: 1.623201\n",
            "Train Epoch: 117 [100/100 (100%)]\tLoss: 1.623190\n",
            "Train Epoch: 118 [100/100 (100%)]\tLoss: 1.623179\n",
            "Train Epoch: 119 [100/100 (100%)]\tLoss: 1.623168\n",
            "Train Epoch: 120 [100/100 (100%)]\tLoss: 1.623156\n",
            "Train Epoch: 121 [100/100 (100%)]\tLoss: 1.623144\n",
            "Train Epoch: 122 [100/100 (100%)]\tLoss: 1.623132\n",
            "Train Epoch: 123 [100/100 (100%)]\tLoss: 1.623121\n",
            "Train Epoch: 124 [100/100 (100%)]\tLoss: 1.623110\n",
            "Train Epoch: 125 [100/100 (100%)]\tLoss: 1.623097\n",
            "Train Epoch: 126 [100/100 (100%)]\tLoss: 1.623084\n",
            "Train Epoch: 127 [100/100 (100%)]\tLoss: 1.623069\n",
            "Train Epoch: 128 [100/100 (100%)]\tLoss: 1.623052\n",
            "Train Epoch: 129 [100/100 (100%)]\tLoss: 1.623034\n",
            "Train Epoch: 130 [100/100 (100%)]\tLoss: 1.623012\n",
            "Train Epoch: 131 [100/100 (100%)]\tLoss: 1.622984\n",
            "Train Epoch: 132 [100/100 (100%)]\tLoss: 1.622950\n",
            "Train Epoch: 133 [100/100 (100%)]\tLoss: 1.622910\n",
            "Train Epoch: 134 [100/100 (100%)]\tLoss: 1.622866\n",
            "Train Epoch: 135 [100/100 (100%)]\tLoss: 1.622826\n",
            "Train Epoch: 136 [100/100 (100%)]\tLoss: 1.622798\n",
            "Train Epoch: 137 [100/100 (100%)]\tLoss: 1.622784\n",
            "Train Epoch: 138 [100/100 (100%)]\tLoss: 1.622774\n",
            "Train Epoch: 139 [100/100 (100%)]\tLoss: 1.622765\n",
            "Train Epoch: 140 [100/100 (100%)]\tLoss: 1.622758\n",
            "Train Epoch: 141 [100/100 (100%)]\tLoss: 1.622755\n",
            "Train Epoch: 142 [100/100 (100%)]\tLoss: 1.622755\n",
            "Train Epoch: 143 [100/100 (100%)]\tLoss: 1.622754\n",
            "Train Epoch: 144 [100/100 (100%)]\tLoss: 1.622749\n",
            "Train Epoch: 145 [100/100 (100%)]\tLoss: 1.622741\n",
            "Train Epoch: 146 [100/100 (100%)]\tLoss: 1.622733\n",
            "Train Epoch: 147 [100/100 (100%)]\tLoss: 1.622725\n",
            "Train Epoch: 148 [100/100 (100%)]\tLoss: 1.622718\n",
            "Train Epoch: 149 [100/100 (100%)]\tLoss: 1.622711\n",
            "Train Epoch: 150 [100/100 (100%)]\tLoss: 1.622705\n",
            "Train Epoch: 151 [100/100 (100%)]\tLoss: 1.622701\n",
            "Train Epoch: 152 [100/100 (100%)]\tLoss: 1.622699\n",
            "Train Epoch: 153 [100/100 (100%)]\tLoss: 1.622696\n",
            "Train Epoch: 154 [100/100 (100%)]\tLoss: 1.622692\n",
            "Train Epoch: 155 [100/100 (100%)]\tLoss: 1.622687\n",
            "Train Epoch: 156 [100/100 (100%)]\tLoss: 1.622682\n",
            "Train Epoch: 157 [100/100 (100%)]\tLoss: 1.622678\n",
            "Train Epoch: 158 [100/100 (100%)]\tLoss: 1.622675\n",
            "Train Epoch: 159 [100/100 (100%)]\tLoss: 1.622671\n",
            "Train Epoch: 160 [100/100 (100%)]\tLoss: 1.622667\n",
            "Train Epoch: 161 [100/100 (100%)]\tLoss: 1.622662\n",
            "Train Epoch: 162 [100/100 (100%)]\tLoss: 1.622659\n",
            "Train Epoch: 163 [100/100 (100%)]\tLoss: 1.622656\n",
            "Train Epoch: 164 [100/100 (100%)]\tLoss: 1.622653\n",
            "Train Epoch: 165 [100/100 (100%)]\tLoss: 1.622649\n",
            "Train Epoch: 166 [100/100 (100%)]\tLoss: 1.622645\n",
            "Train Epoch: 167 [100/100 (100%)]\tLoss: 1.622641\n",
            "Train Epoch: 168 [100/100 (100%)]\tLoss: 1.622636\n",
            "Train Epoch: 169 [100/100 (100%)]\tLoss: 1.622633\n",
            "Train Epoch: 170 [100/100 (100%)]\tLoss: 1.622629\n",
            "Train Epoch: 171 [100/100 (100%)]\tLoss: 1.622624\n",
            "Train Epoch: 172 [100/100 (100%)]\tLoss: 1.622620\n",
            "Train Epoch: 173 [100/100 (100%)]\tLoss: 1.622615\n",
            "Train Epoch: 174 [100/100 (100%)]\tLoss: 1.622611\n",
            "Train Epoch: 175 [100/100 (100%)]\tLoss: 1.622606\n",
            "Train Epoch: 176 [100/100 (100%)]\tLoss: 1.622601\n",
            "Train Epoch: 177 [100/100 (100%)]\tLoss: 1.622595\n",
            "Train Epoch: 178 [100/100 (100%)]\tLoss: 1.622589\n",
            "Train Epoch: 179 [100/100 (100%)]\tLoss: 1.622583\n",
            "Train Epoch: 180 [100/100 (100%)]\tLoss: 1.622576\n",
            "Train Epoch: 181 [100/100 (100%)]\tLoss: 1.622569\n",
            "Train Epoch: 182 [100/100 (100%)]\tLoss: 1.622563\n",
            "Train Epoch: 183 [100/100 (100%)]\tLoss: 1.622555\n",
            "Train Epoch: 184 [100/100 (100%)]\tLoss: 1.622548\n",
            "Train Epoch: 185 [100/100 (100%)]\tLoss: 1.622541\n",
            "Train Epoch: 186 [100/100 (100%)]\tLoss: 1.622533\n",
            "Train Epoch: 187 [100/100 (100%)]\tLoss: 1.622526\n",
            "Train Epoch: 188 [100/100 (100%)]\tLoss: 1.622520\n",
            "Train Epoch: 189 [100/100 (100%)]\tLoss: 1.622514\n",
            "Train Epoch: 190 [100/100 (100%)]\tLoss: 1.622508\n",
            "Train Epoch: 191 [100/100 (100%)]\tLoss: 1.622503\n",
            "Train Epoch: 192 [100/100 (100%)]\tLoss: 1.622499\n",
            "Train Epoch: 193 [100/100 (100%)]\tLoss: 1.622495\n",
            "Train Epoch: 194 [100/100 (100%)]\tLoss: 1.622491\n",
            "Train Epoch: 195 [100/100 (100%)]\tLoss: 1.622489\n",
            "Train Epoch: 196 [100/100 (100%)]\tLoss: 1.622486\n",
            "Train Epoch: 197 [100/100 (100%)]\tLoss: 1.622483\n",
            "Train Epoch: 198 [100/100 (100%)]\tLoss: 1.622481\n",
            "Train Epoch: 199 [100/100 (100%)]\tLoss: 1.622479\n",
            "Train Epoch: 200 [100/100 (100%)]\tLoss: 1.622476\n",
            "Train Epoch: 201 [100/100 (100%)]\tLoss: 1.622474\n",
            "Train Epoch: 202 [100/100 (100%)]\tLoss: 1.622471\n",
            "Train Epoch: 203 [100/100 (100%)]\tLoss: 1.622469\n",
            "Train Epoch: 204 [100/100 (100%)]\tLoss: 1.622468\n",
            "Train Epoch: 205 [100/100 (100%)]\tLoss: 1.622465\n",
            "Train Epoch: 206 [100/100 (100%)]\tLoss: 1.622463\n",
            "Train Epoch: 207 [100/100 (100%)]\tLoss: 1.622461\n",
            "Train Epoch: 208 [100/100 (100%)]\tLoss: 1.622459\n",
            "Train Epoch: 209 [100/100 (100%)]\tLoss: 1.622457\n",
            "Train Epoch: 210 [100/100 (100%)]\tLoss: 1.622455\n",
            "Train Epoch: 211 [100/100 (100%)]\tLoss: 1.622453\n",
            "Train Epoch: 212 [100/100 (100%)]\tLoss: 1.622451\n",
            "Train Epoch: 213 [100/100 (100%)]\tLoss: 1.622450\n",
            "Train Epoch: 214 [100/100 (100%)]\tLoss: 1.622447\n",
            "Train Epoch: 215 [100/100 (100%)]\tLoss: 1.622446\n",
            "Train Epoch: 216 [100/100 (100%)]\tLoss: 1.622444\n",
            "Train Epoch: 217 [100/100 (100%)]\tLoss: 1.622441\n",
            "Train Epoch: 218 [100/100 (100%)]\tLoss: 1.622439\n",
            "Train Epoch: 219 [100/100 (100%)]\tLoss: 1.622437\n",
            "Train Epoch: 220 [100/100 (100%)]\tLoss: 1.622435\n",
            "Train Epoch: 221 [100/100 (100%)]\tLoss: 1.622433\n",
            "Train Epoch: 222 [100/100 (100%)]\tLoss: 1.622430\n",
            "Train Epoch: 223 [100/100 (100%)]\tLoss: 1.622428\n",
            "Train Epoch: 224 [100/100 (100%)]\tLoss: 1.622425\n",
            "Train Epoch: 225 [100/100 (100%)]\tLoss: 1.622422\n",
            "Train Epoch: 226 [100/100 (100%)]\tLoss: 1.622419\n",
            "Train Epoch: 227 [100/100 (100%)]\tLoss: 1.622415\n",
            "Train Epoch: 228 [100/100 (100%)]\tLoss: 1.622412\n",
            "Train Epoch: 229 [100/100 (100%)]\tLoss: 1.622408\n",
            "Train Epoch: 230 [100/100 (100%)]\tLoss: 1.622404\n",
            "Train Epoch: 231 [100/100 (100%)]\tLoss: 1.622400\n",
            "Train Epoch: 232 [100/100 (100%)]\tLoss: 1.622395\n",
            "Train Epoch: 233 [100/100 (100%)]\tLoss: 1.622389\n",
            "Train Epoch: 234 [100/100 (100%)]\tLoss: 1.622384\n",
            "Train Epoch: 235 [100/100 (100%)]\tLoss: 1.622378\n",
            "Train Epoch: 236 [100/100 (100%)]\tLoss: 1.622373\n",
            "Train Epoch: 237 [100/100 (100%)]\tLoss: 1.622367\n",
            "Train Epoch: 238 [100/100 (100%)]\tLoss: 1.622361\n",
            "Train Epoch: 239 [100/100 (100%)]\tLoss: 1.622356\n",
            "Train Epoch: 240 [100/100 (100%)]\tLoss: 1.622351\n",
            "Train Epoch: 241 [100/100 (100%)]\tLoss: 1.622348\n",
            "Train Epoch: 242 [100/100 (100%)]\tLoss: 1.622345\n",
            "Train Epoch: 243 [100/100 (100%)]\tLoss: 1.622343\n",
            "Train Epoch: 244 [100/100 (100%)]\tLoss: 1.622341\n",
            "Train Epoch: 245 [100/100 (100%)]\tLoss: 1.622340\n",
            "Train Epoch: 246 [100/100 (100%)]\tLoss: 1.622339\n",
            "Train Epoch: 247 [100/100 (100%)]\tLoss: 1.622338\n",
            "Train Epoch: 248 [100/100 (100%)]\tLoss: 1.622337\n",
            "Train Epoch: 249 [100/100 (100%)]\tLoss: 1.622336\n",
            "Train Epoch: 250 [100/100 (100%)]\tLoss: 1.622334\n",
            "Train Epoch: 251 [100/100 (100%)]\tLoss: 1.622332\n",
            "Train Epoch: 252 [100/100 (100%)]\tLoss: 1.622329\n",
            "Train Epoch: 253 [100/100 (100%)]\tLoss: 1.622327\n",
            "Train Epoch: 254 [100/100 (100%)]\tLoss: 1.622325\n",
            "Train Epoch: 255 [100/100 (100%)]\tLoss: 1.622322\n",
            "Train Epoch: 256 [100/100 (100%)]\tLoss: 1.622319\n",
            "Train Epoch: 257 [100/100 (100%)]\tLoss: 1.622316\n",
            "Train Epoch: 258 [100/100 (100%)]\tLoss: 1.622312\n",
            "Train Epoch: 259 [100/100 (100%)]\tLoss: 1.622308\n",
            "Train Epoch: 260 [100/100 (100%)]\tLoss: 1.622303\n",
            "Train Epoch: 261 [100/100 (100%)]\tLoss: 1.622298\n",
            "Train Epoch: 262 [100/100 (100%)]\tLoss: 1.622293\n",
            "Train Epoch: 263 [100/100 (100%)]\tLoss: 1.622287\n",
            "Train Epoch: 264 [100/100 (100%)]\tLoss: 1.622281\n",
            "Train Epoch: 265 [100/100 (100%)]\tLoss: 1.622273\n",
            "Train Epoch: 266 [100/100 (100%)]\tLoss: 1.622267\n",
            "Train Epoch: 267 [100/100 (100%)]\tLoss: 1.622260\n",
            "Train Epoch: 268 [100/100 (100%)]\tLoss: 1.622255\n",
            "Train Epoch: 269 [100/100 (100%)]\tLoss: 1.622252\n",
            "Train Epoch: 270 [100/100 (100%)]\tLoss: 1.622249\n",
            "Train Epoch: 271 [100/100 (100%)]\tLoss: 1.622246\n",
            "Train Epoch: 272 [100/100 (100%)]\tLoss: 1.622241\n",
            "Train Epoch: 273 [100/100 (100%)]\tLoss: 1.622238\n",
            "Train Epoch: 274 [100/100 (100%)]\tLoss: 1.622234\n",
            "Train Epoch: 275 [100/100 (100%)]\tLoss: 1.622233\n",
            "Train Epoch: 276 [100/100 (100%)]\tLoss: 1.622230\n",
            "Train Epoch: 277 [100/100 (100%)]\tLoss: 1.622227\n",
            "Train Epoch: 278 [100/100 (100%)]\tLoss: 1.622222\n",
            "Train Epoch: 279 [100/100 (100%)]\tLoss: 1.622216\n",
            "Train Epoch: 280 [100/100 (100%)]\tLoss: 1.622209\n",
            "Train Epoch: 281 [100/100 (100%)]\tLoss: 1.622199\n",
            "Train Epoch: 282 [100/100 (100%)]\tLoss: 1.622191\n",
            "Train Epoch: 283 [100/100 (100%)]\tLoss: 1.622196\n",
            "Train Epoch: 284 [100/100 (100%)]\tLoss: 1.622188\n",
            "Train Epoch: 285 [100/100 (100%)]\tLoss: 1.622178\n",
            "Train Epoch: 286 [100/100 (100%)]\tLoss: 1.622175\n",
            "Train Epoch: 287 [100/100 (100%)]\tLoss: 1.622173\n",
            "Train Epoch: 288 [100/100 (100%)]\tLoss: 1.622169\n",
            "Train Epoch: 289 [100/100 (100%)]\tLoss: 1.622163\n",
            "Train Epoch: 290 [100/100 (100%)]\tLoss: 1.622156\n",
            "Train Epoch: 291 [100/100 (100%)]\tLoss: 1.622149\n",
            "Train Epoch: 292 [100/100 (100%)]\tLoss: 1.622145\n",
            "Train Epoch: 293 [100/100 (100%)]\tLoss: 1.622141\n",
            "Train Epoch: 294 [100/100 (100%)]\tLoss: 1.622132\n",
            "Train Epoch: 295 [100/100 (100%)]\tLoss: 1.622125\n",
            "Train Epoch: 296 [100/100 (100%)]\tLoss: 1.622118\n",
            "Train Epoch: 297 [100/100 (100%)]\tLoss: 1.622111\n",
            "Train Epoch: 298 [100/100 (100%)]\tLoss: 1.622101\n",
            "Train Epoch: 299 [100/100 (100%)]\tLoss: 1.622091\n",
            "Train Epoch: 300 [100/100 (100%)]\tLoss: 1.622082\n",
            "Train Epoch: 301 [100/100 (100%)]\tLoss: 1.622073\n",
            "Train Epoch: 302 [100/100 (100%)]\tLoss: 1.622059\n",
            "Train Epoch: 303 [100/100 (100%)]\tLoss: 1.622044\n",
            "Train Epoch: 304 [100/100 (100%)]\tLoss: 1.622030\n",
            "Train Epoch: 305 [100/100 (100%)]\tLoss: 1.622021\n",
            "Train Epoch: 306 [100/100 (100%)]\tLoss: 1.622019\n",
            "Train Epoch: 307 [100/100 (100%)]\tLoss: 1.622018\n",
            "Train Epoch: 308 [100/100 (100%)]\tLoss: 1.622016\n",
            "Train Epoch: 309 [100/100 (100%)]\tLoss: 1.622014\n",
            "Train Epoch: 310 [100/100 (100%)]\tLoss: 1.622010\n",
            "Train Epoch: 311 [100/100 (100%)]\tLoss: 1.622006\n",
            "Train Epoch: 312 [100/100 (100%)]\tLoss: 1.622001\n",
            "Train Epoch: 313 [100/100 (100%)]\tLoss: 1.621996\n",
            "Train Epoch: 314 [100/100 (100%)]\tLoss: 1.621989\n",
            "Train Epoch: 315 [100/100 (100%)]\tLoss: 1.621984\n",
            "Train Epoch: 316 [100/100 (100%)]\tLoss: 1.621978\n",
            "Train Epoch: 317 [100/100 (100%)]\tLoss: 1.621972\n",
            "Train Epoch: 318 [100/100 (100%)]\tLoss: 1.621968\n",
            "Train Epoch: 319 [100/100 (100%)]\tLoss: 1.621965\n",
            "Train Epoch: 320 [100/100 (100%)]\tLoss: 1.621962\n",
            "Train Epoch: 321 [100/100 (100%)]\tLoss: 1.621961\n",
            "Train Epoch: 322 [100/100 (100%)]\tLoss: 1.621959\n",
            "Train Epoch: 323 [100/100 (100%)]\tLoss: 1.621958\n",
            "Train Epoch: 324 [100/100 (100%)]\tLoss: 1.621957\n",
            "Train Epoch: 325 [100/100 (100%)]\tLoss: 1.621955\n",
            "Train Epoch: 326 [100/100 (100%)]\tLoss: 1.621954\n",
            "Train Epoch: 327 [100/100 (100%)]\tLoss: 1.621952\n",
            "Train Epoch: 328 [100/100 (100%)]\tLoss: 1.621950\n",
            "Train Epoch: 329 [100/100 (100%)]\tLoss: 1.621948\n",
            "Train Epoch: 330 [100/100 (100%)]\tLoss: 1.621946\n",
            "Train Epoch: 331 [100/100 (100%)]\tLoss: 1.621944\n",
            "Train Epoch: 332 [100/100 (100%)]\tLoss: 1.621942\n",
            "Train Epoch: 333 [100/100 (100%)]\tLoss: 1.621939\n",
            "Train Epoch: 334 [100/100 (100%)]\tLoss: 1.621937\n",
            "Train Epoch: 335 [100/100 (100%)]\tLoss: 1.621935\n",
            "Train Epoch: 336 [100/100 (100%)]\tLoss: 1.621933\n",
            "Train Epoch: 337 [100/100 (100%)]\tLoss: 1.621930\n",
            "Train Epoch: 338 [100/100 (100%)]\tLoss: 1.621928\n",
            "Train Epoch: 339 [100/100 (100%)]\tLoss: 1.621925\n",
            "Train Epoch: 340 [100/100 (100%)]\tLoss: 1.621923\n",
            "Train Epoch: 341 [100/100 (100%)]\tLoss: 1.621920\n",
            "Train Epoch: 342 [100/100 (100%)]\tLoss: 1.621918\n",
            "Train Epoch: 343 [100/100 (100%)]\tLoss: 1.621916\n",
            "Train Epoch: 344 [100/100 (100%)]\tLoss: 1.621914\n",
            "Train Epoch: 345 [100/100 (100%)]\tLoss: 1.621913\n",
            "Train Epoch: 346 [100/100 (100%)]\tLoss: 1.621913\n",
            "Train Epoch: 347 [100/100 (100%)]\tLoss: 1.621912\n",
            "Train Epoch: 348 [100/100 (100%)]\tLoss: 1.621911\n",
            "Train Epoch: 349 [100/100 (100%)]\tLoss: 1.621910\n",
            "Train Epoch: 350 [100/100 (100%)]\tLoss: 1.621909\n",
            "Train Epoch: 351 [100/100 (100%)]\tLoss: 1.621909\n",
            "Train Epoch: 352 [100/100 (100%)]\tLoss: 1.621907\n",
            "Train Epoch: 353 [100/100 (100%)]\tLoss: 1.621907\n",
            "Train Epoch: 354 [100/100 (100%)]\tLoss: 1.621906\n",
            "Train Epoch: 355 [100/100 (100%)]\tLoss: 1.621905\n",
            "Train Epoch: 356 [100/100 (100%)]\tLoss: 1.621904\n",
            "Train Epoch: 357 [100/100 (100%)]\tLoss: 1.621903\n",
            "Train Epoch: 358 [100/100 (100%)]\tLoss: 1.621902\n",
            "Train Epoch: 359 [100/100 (100%)]\tLoss: 1.621902\n",
            "Train Epoch: 360 [100/100 (100%)]\tLoss: 1.621901\n",
            "Train Epoch: 361 [100/100 (100%)]\tLoss: 1.621901\n",
            "Train Epoch: 362 [100/100 (100%)]\tLoss: 1.621900\n",
            "Train Epoch: 363 [100/100 (100%)]\tLoss: 1.621899\n",
            "Train Epoch: 364 [100/100 (100%)]\tLoss: 1.621899\n",
            "Train Epoch: 365 [100/100 (100%)]\tLoss: 1.621898\n",
            "Train Epoch: 366 [100/100 (100%)]\tLoss: 1.621898\n",
            "Train Epoch: 367 [100/100 (100%)]\tLoss: 1.621898\n",
            "Train Epoch: 368 [100/100 (100%)]\tLoss: 1.621897\n",
            "Train Epoch: 369 [100/100 (100%)]\tLoss: 1.621897\n",
            "Train Epoch: 370 [100/100 (100%)]\tLoss: 1.621897\n",
            "Train Epoch: 371 [100/100 (100%)]\tLoss: 1.621897\n",
            "Train Epoch: 372 [100/100 (100%)]\tLoss: 1.621897\n",
            "Train Epoch: 373 [100/100 (100%)]\tLoss: 1.621897\n",
            "Train Epoch: 374 [100/100 (100%)]\tLoss: 1.621896\n",
            "Train Epoch: 375 [100/100 (100%)]\tLoss: 1.621896\n",
            "Train Epoch: 376 [100/100 (100%)]\tLoss: 1.621896\n",
            "Train Epoch: 377 [100/100 (100%)]\tLoss: 1.621895\n",
            "Train Epoch: 378 [100/100 (100%)]\tLoss: 1.621895\n",
            "Train Epoch: 379 [100/100 (100%)]\tLoss: 1.621895\n",
            "Train Epoch: 380 [100/100 (100%)]\tLoss: 1.621894\n",
            "Train Epoch: 381 [100/100 (100%)]\tLoss: 1.621894\n",
            "Train Epoch: 382 [100/100 (100%)]\tLoss: 1.621894\n",
            "Train Epoch: 383 [100/100 (100%)]\tLoss: 1.621894\n",
            "Train Epoch: 384 [100/100 (100%)]\tLoss: 1.621893\n",
            "Train Epoch: 385 [100/100 (100%)]\tLoss: 1.621893\n",
            "Train Epoch: 386 [100/100 (100%)]\tLoss: 1.621892\n",
            "Train Epoch: 387 [100/100 (100%)]\tLoss: 1.621892\n",
            "Train Epoch: 388 [100/100 (100%)]\tLoss: 1.621891\n",
            "Train Epoch: 389 [100/100 (100%)]\tLoss: 1.621891\n",
            "Train Epoch: 390 [100/100 (100%)]\tLoss: 1.621889\n",
            "Train Epoch: 391 [100/100 (100%)]\tLoss: 1.621889\n",
            "Train Epoch: 392 [100/100 (100%)]\tLoss: 1.621888\n",
            "Train Epoch: 393 [100/100 (100%)]\tLoss: 1.621886\n",
            "Train Epoch: 394 [100/100 (100%)]\tLoss: 1.621884\n",
            "Train Epoch: 395 [100/100 (100%)]\tLoss: 1.621882\n",
            "Train Epoch: 396 [100/100 (100%)]\tLoss: 1.621880\n",
            "Train Epoch: 397 [100/100 (100%)]\tLoss: 1.621877\n",
            "Train Epoch: 398 [100/100 (100%)]\tLoss: 1.621874\n",
            "Train Epoch: 399 [100/100 (100%)]\tLoss: 1.621872\n",
            "Train Epoch: 400 [100/100 (100%)]\tLoss: 1.621870\n",
            "Train Epoch: 401 [100/100 (100%)]\tLoss: 1.621866\n",
            "Train Epoch: 402 [100/100 (100%)]\tLoss: 1.621862\n",
            "Train Epoch: 403 [100/100 (100%)]\tLoss: 1.621858\n",
            "Train Epoch: 404 [100/100 (100%)]\tLoss: 1.621858\n",
            "Train Epoch: 405 [100/100 (100%)]\tLoss: 1.621858\n",
            "Train Epoch: 406 [100/100 (100%)]\tLoss: 1.621857\n",
            "Train Epoch: 407 [100/100 (100%)]\tLoss: 1.621856\n",
            "Train Epoch: 408 [100/100 (100%)]\tLoss: 1.621855\n",
            "Train Epoch: 409 [100/100 (100%)]\tLoss: 1.621855\n",
            "Train Epoch: 410 [100/100 (100%)]\tLoss: 1.621854\n",
            "Train Epoch: 411 [100/100 (100%)]\tLoss: 1.621854\n",
            "Train Epoch: 412 [100/100 (100%)]\tLoss: 1.621854\n",
            "Train Epoch: 413 [100/100 (100%)]\tLoss: 1.621854\n",
            "Train Epoch: 414 [100/100 (100%)]\tLoss: 1.621853\n",
            "Train Epoch: 415 [100/100 (100%)]\tLoss: 1.621852\n",
            "Train Epoch: 416 [100/100 (100%)]\tLoss: 1.621852\n",
            "Train Epoch: 417 [100/100 (100%)]\tLoss: 1.621851\n",
            "Train Epoch: 418 [100/100 (100%)]\tLoss: 1.621850\n",
            "Train Epoch: 419 [100/100 (100%)]\tLoss: 1.621850\n",
            "Train Epoch: 420 [100/100 (100%)]\tLoss: 1.621849\n",
            "Train Epoch: 421 [100/100 (100%)]\tLoss: 1.621848\n",
            "Train Epoch: 422 [100/100 (100%)]\tLoss: 1.621846\n",
            "Train Epoch: 423 [100/100 (100%)]\tLoss: 1.621845\n",
            "Train Epoch: 424 [100/100 (100%)]\tLoss: 1.621843\n",
            "Train Epoch: 425 [100/100 (100%)]\tLoss: 1.621841\n",
            "Train Epoch: 426 [100/100 (100%)]\tLoss: 1.621841\n",
            "Train Epoch: 427 [100/100 (100%)]\tLoss: 1.621842\n",
            "Train Epoch: 428 [100/100 (100%)]\tLoss: 1.621842\n",
            "Train Epoch: 429 [100/100 (100%)]\tLoss: 1.621841\n",
            "Train Epoch: 430 [100/100 (100%)]\tLoss: 1.621841\n",
            "Train Epoch: 431 [100/100 (100%)]\tLoss: 1.621841\n",
            "Train Epoch: 432 [100/100 (100%)]\tLoss: 1.621841\n",
            "Train Epoch: 433 [100/100 (100%)]\tLoss: 1.621841\n",
            "Train Epoch: 434 [100/100 (100%)]\tLoss: 1.621841\n",
            "Train Epoch: 435 [100/100 (100%)]\tLoss: 1.621840\n",
            "Train Epoch: 436 [100/100 (100%)]\tLoss: 1.621840\n",
            "Train Epoch: 437 [100/100 (100%)]\tLoss: 1.621840\n",
            "Train Epoch: 438 [100/100 (100%)]\tLoss: 1.621840\n",
            "Train Epoch: 439 [100/100 (100%)]\tLoss: 1.621839\n",
            "Train Epoch: 440 [100/100 (100%)]\tLoss: 1.621840\n",
            "Train Epoch: 441 [100/100 (100%)]\tLoss: 1.621840\n",
            "Train Epoch: 442 [100/100 (100%)]\tLoss: 1.621839\n",
            "Train Epoch: 443 [100/100 (100%)]\tLoss: 1.621839\n",
            "Train Epoch: 444 [100/100 (100%)]\tLoss: 1.621839\n",
            "Train Epoch: 445 [100/100 (100%)]\tLoss: 1.621839\n",
            "Train Epoch: 446 [100/100 (100%)]\tLoss: 1.621839\n",
            "Train Epoch: 447 [100/100 (100%)]\tLoss: 1.621839\n",
            "Train Epoch: 448 [100/100 (100%)]\tLoss: 1.621838\n",
            "Train Epoch: 449 [100/100 (100%)]\tLoss: 1.621838\n",
            "Train Epoch: 450 [100/100 (100%)]\tLoss: 1.621838\n",
            "Train Epoch: 451 [100/100 (100%)]\tLoss: 1.621838\n",
            "Train Epoch: 452 [100/100 (100%)]\tLoss: 1.621838\n",
            "Train Epoch: 453 [100/100 (100%)]\tLoss: 1.621838\n",
            "Train Epoch: 454 [100/100 (100%)]\tLoss: 1.621838\n",
            "Train Epoch: 455 [100/100 (100%)]\tLoss: 1.621838\n",
            "Train Epoch: 456 [100/100 (100%)]\tLoss: 1.621837\n",
            "Train Epoch: 457 [100/100 (100%)]\tLoss: 1.621837\n",
            "Train Epoch: 458 [100/100 (100%)]\tLoss: 1.621837\n",
            "Train Epoch: 459 [100/100 (100%)]\tLoss: 1.621837\n",
            "Train Epoch: 460 [100/100 (100%)]\tLoss: 1.621836\n",
            "Train Epoch: 461 [100/100 (100%)]\tLoss: 1.621836\n",
            "Train Epoch: 462 [100/100 (100%)]\tLoss: 1.621836\n",
            "Train Epoch: 463 [100/100 (100%)]\tLoss: 1.621835\n",
            "Train Epoch: 464 [100/100 (100%)]\tLoss: 1.621834\n",
            "Train Epoch: 465 [100/100 (100%)]\tLoss: 1.621833\n",
            "Train Epoch: 466 [100/100 (100%)]\tLoss: 1.621831\n",
            "Train Epoch: 467 [100/100 (100%)]\tLoss: 1.621826\n",
            "Train Epoch: 468 [100/100 (100%)]\tLoss: 1.621811\n",
            "Train Epoch: 469 [100/100 (100%)]\tLoss: 1.621745\n",
            "Train Epoch: 470 [100/100 (100%)]\tLoss: 1.621126\n",
            "Train Epoch: 471 [100/100 (100%)]\tLoss: 1.607356\n",
            "Train Epoch: 472 [100/100 (100%)]\tLoss: 1.556656\n",
            "Train Epoch: 473 [100/100 (100%)]\tLoss: 1.782180\n",
            "Train Epoch: 474 [100/100 (100%)]\tLoss: 1.557966\n",
            "Train Epoch: 475 [100/100 (100%)]\tLoss: 1.610302\n",
            "Train Epoch: 476 [100/100 (100%)]\tLoss: 1.622501\n",
            "Train Epoch: 477 [100/100 (100%)]\tLoss: 1.623483\n",
            "Train Epoch: 478 [100/100 (100%)]\tLoss: 1.623873\n",
            "Train Epoch: 479 [100/100 (100%)]\tLoss: 1.624022\n",
            "Train Epoch: 480 [100/100 (100%)]\tLoss: 1.624117\n",
            "Train Epoch: 481 [100/100 (100%)]\tLoss: 1.624306\n",
            "Train Epoch: 482 [100/100 (100%)]\tLoss: 1.624394\n",
            "Train Epoch: 483 [100/100 (100%)]\tLoss: 1.624128\n",
            "Train Epoch: 484 [100/100 (100%)]\tLoss: 1.623912\n",
            "Train Epoch: 485 [100/100 (100%)]\tLoss: 1.623611\n",
            "Train Epoch: 486 [100/100 (100%)]\tLoss: 1.623495\n",
            "Train Epoch: 487 [100/100 (100%)]\tLoss: 1.623628\n",
            "Train Epoch: 488 [100/100 (100%)]\tLoss: 1.623512\n",
            "Train Epoch: 489 [100/100 (100%)]\tLoss: 1.623309\n",
            "Train Epoch: 490 [100/100 (100%)]\tLoss: 1.623356\n",
            "Train Epoch: 491 [100/100 (100%)]\tLoss: 1.623323\n",
            "Train Epoch: 492 [100/100 (100%)]\tLoss: 1.623199\n",
            "Train Epoch: 493 [100/100 (100%)]\tLoss: 1.623113\n",
            "Train Epoch: 494 [100/100 (100%)]\tLoss: 1.622996\n",
            "Train Epoch: 495 [100/100 (100%)]\tLoss: 1.622903\n",
            "Train Epoch: 496 [100/100 (100%)]\tLoss: 1.622837\n",
            "Train Epoch: 497 [100/100 (100%)]\tLoss: 1.622785\n",
            "Train Epoch: 498 [100/100 (100%)]\tLoss: 1.622842\n",
            "Train Epoch: 499 [100/100 (100%)]\tLoss: 1.622822\n",
            "Train Epoch: 500 [100/100 (100%)]\tLoss: 1.622705\n",
            "Train Epoch: 501 [100/100 (100%)]\tLoss: 1.622660\n",
            "Train Epoch: 502 [100/100 (100%)]\tLoss: 1.622675\n",
            "Train Epoch: 503 [100/100 (100%)]\tLoss: 1.622657\n",
            "Train Epoch: 504 [100/100 (100%)]\tLoss: 1.622620\n",
            "Train Epoch: 505 [100/100 (100%)]\tLoss: 1.622578\n",
            "Train Epoch: 506 [100/100 (100%)]\tLoss: 1.622537\n",
            "Train Epoch: 507 [100/100 (100%)]\tLoss: 1.622528\n",
            "Train Epoch: 508 [100/100 (100%)]\tLoss: 1.622511\n",
            "Train Epoch: 509 [100/100 (100%)]\tLoss: 1.622461\n",
            "Train Epoch: 510 [100/100 (100%)]\tLoss: 1.622418\n",
            "Train Epoch: 511 [100/100 (100%)]\tLoss: 1.622394\n",
            "Train Epoch: 512 [100/100 (100%)]\tLoss: 1.622385\n",
            "Train Epoch: 513 [100/100 (100%)]\tLoss: 1.622359\n",
            "Train Epoch: 514 [100/100 (100%)]\tLoss: 1.622321\n",
            "Train Epoch: 515 [100/100 (100%)]\tLoss: 1.622299\n",
            "Train Epoch: 516 [100/100 (100%)]\tLoss: 1.622293\n",
            "Train Epoch: 517 [100/100 (100%)]\tLoss: 1.622281\n",
            "Train Epoch: 518 [100/100 (100%)]\tLoss: 1.622256\n",
            "Train Epoch: 519 [100/100 (100%)]\tLoss: 1.622235\n",
            "Train Epoch: 520 [100/100 (100%)]\tLoss: 1.622221\n",
            "Train Epoch: 521 [100/100 (100%)]\tLoss: 1.622205\n",
            "Train Epoch: 522 [100/100 (100%)]\tLoss: 1.622188\n",
            "Train Epoch: 523 [100/100 (100%)]\tLoss: 1.622176\n",
            "Train Epoch: 524 [100/100 (100%)]\tLoss: 1.622167\n",
            "Train Epoch: 525 [100/100 (100%)]\tLoss: 1.622154\n",
            "Train Epoch: 526 [100/100 (100%)]\tLoss: 1.622140\n",
            "Train Epoch: 527 [100/100 (100%)]\tLoss: 1.622129\n",
            "Train Epoch: 528 [100/100 (100%)]\tLoss: 1.622121\n",
            "Train Epoch: 529 [100/100 (100%)]\tLoss: 1.622114\n",
            "Train Epoch: 530 [100/100 (100%)]\tLoss: 1.622106\n",
            "Train Epoch: 531 [100/100 (100%)]\tLoss: 1.622100\n",
            "Train Epoch: 532 [100/100 (100%)]\tLoss: 1.622097\n",
            "Train Epoch: 533 [100/100 (100%)]\tLoss: 1.622092\n",
            "Train Epoch: 534 [100/100 (100%)]\tLoss: 1.622087\n",
            "Train Epoch: 535 [100/100 (100%)]\tLoss: 1.622081\n",
            "Train Epoch: 536 [100/100 (100%)]\tLoss: 1.622075\n",
            "Train Epoch: 537 [100/100 (100%)]\tLoss: 1.622070\n",
            "Train Epoch: 538 [100/100 (100%)]\tLoss: 1.622063\n",
            "Train Epoch: 539 [100/100 (100%)]\tLoss: 1.622057\n",
            "Train Epoch: 540 [100/100 (100%)]\tLoss: 1.622050\n",
            "Train Epoch: 541 [100/100 (100%)]\tLoss: 1.622043\n",
            "Train Epoch: 542 [100/100 (100%)]\tLoss: 1.622037\n",
            "Train Epoch: 543 [100/100 (100%)]\tLoss: 1.622032\n",
            "Train Epoch: 544 [100/100 (100%)]\tLoss: 1.622027\n",
            "Train Epoch: 545 [100/100 (100%)]\tLoss: 1.622022\n",
            "Train Epoch: 546 [100/100 (100%)]\tLoss: 1.622017\n",
            "Train Epoch: 547 [100/100 (100%)]\tLoss: 1.622014\n",
            "Train Epoch: 548 [100/100 (100%)]\tLoss: 1.622010\n",
            "Train Epoch: 549 [100/100 (100%)]\tLoss: 1.622007\n",
            "Train Epoch: 550 [100/100 (100%)]\tLoss: 1.622003\n",
            "Train Epoch: 551 [100/100 (100%)]\tLoss: 1.621999\n",
            "Train Epoch: 552 [100/100 (100%)]\tLoss: 1.621996\n",
            "Train Epoch: 553 [100/100 (100%)]\tLoss: 1.621992\n",
            "Train Epoch: 554 [100/100 (100%)]\tLoss: 1.621988\n",
            "Train Epoch: 555 [100/100 (100%)]\tLoss: 1.621985\n",
            "Train Epoch: 556 [100/100 (100%)]\tLoss: 1.621981\n",
            "Train Epoch: 557 [100/100 (100%)]\tLoss: 1.621977\n",
            "Train Epoch: 558 [100/100 (100%)]\tLoss: 1.621973\n",
            "Train Epoch: 559 [100/100 (100%)]\tLoss: 1.621969\n",
            "Train Epoch: 560 [100/100 (100%)]\tLoss: 1.621965\n",
            "Train Epoch: 561 [100/100 (100%)]\tLoss: 1.621960\n",
            "Train Epoch: 562 [100/100 (100%)]\tLoss: 1.621955\n",
            "Train Epoch: 563 [100/100 (100%)]\tLoss: 1.621950\n",
            "Train Epoch: 564 [100/100 (100%)]\tLoss: 1.621945\n",
            "Train Epoch: 565 [100/100 (100%)]\tLoss: 1.621941\n",
            "Train Epoch: 566 [100/100 (100%)]\tLoss: 1.621936\n",
            "Train Epoch: 567 [100/100 (100%)]\tLoss: 1.621932\n",
            "Train Epoch: 568 [100/100 (100%)]\tLoss: 1.621927\n",
            "Train Epoch: 569 [100/100 (100%)]\tLoss: 1.621923\n",
            "Train Epoch: 570 [100/100 (100%)]\tLoss: 1.621919\n",
            "Train Epoch: 571 [100/100 (100%)]\tLoss: 1.621914\n",
            "Train Epoch: 572 [100/100 (100%)]\tLoss: 1.621910\n",
            "Train Epoch: 573 [100/100 (100%)]\tLoss: 1.621907\n",
            "Train Epoch: 574 [100/100 (100%)]\tLoss: 1.621904\n",
            "Train Epoch: 575 [100/100 (100%)]\tLoss: 1.621901\n",
            "Train Epoch: 576 [100/100 (100%)]\tLoss: 1.621899\n",
            "Train Epoch: 577 [100/100 (100%)]\tLoss: 1.621897\n",
            "Train Epoch: 578 [100/100 (100%)]\tLoss: 1.621896\n",
            "Train Epoch: 579 [100/100 (100%)]\tLoss: 1.621894\n",
            "Train Epoch: 580 [100/100 (100%)]\tLoss: 1.621893\n",
            "Train Epoch: 581 [100/100 (100%)]\tLoss: 1.621891\n",
            "Train Epoch: 582 [100/100 (100%)]\tLoss: 1.621890\n",
            "Train Epoch: 583 [100/100 (100%)]\tLoss: 1.621888\n",
            "Train Epoch: 584 [100/100 (100%)]\tLoss: 1.621886\n",
            "Train Epoch: 585 [100/100 (100%)]\tLoss: 1.621885\n",
            "Train Epoch: 586 [100/100 (100%)]\tLoss: 1.621883\n",
            "Train Epoch: 587 [100/100 (100%)]\tLoss: 1.621881\n",
            "Train Epoch: 588 [100/100 (100%)]\tLoss: 1.621879\n",
            "Train Epoch: 589 [100/100 (100%)]\tLoss: 1.621877\n",
            "Train Epoch: 590 [100/100 (100%)]\tLoss: 1.621875\n",
            "Train Epoch: 591 [100/100 (100%)]\tLoss: 1.621873\n",
            "Train Epoch: 592 [100/100 (100%)]\tLoss: 1.621871\n",
            "Train Epoch: 593 [100/100 (100%)]\tLoss: 1.621869\n",
            "Train Epoch: 594 [100/100 (100%)]\tLoss: 1.621867\n",
            "Train Epoch: 595 [100/100 (100%)]\tLoss: 1.621865\n",
            "Train Epoch: 596 [100/100 (100%)]\tLoss: 1.621863\n",
            "Train Epoch: 597 [100/100 (100%)]\tLoss: 1.621860\n",
            "Train Epoch: 598 [100/100 (100%)]\tLoss: 1.621858\n",
            "Train Epoch: 599 [100/100 (100%)]\tLoss: 1.621856\n",
            "Train Epoch: 600 [100/100 (100%)]\tLoss: 1.621854\n",
            "Train Epoch: 601 [100/100 (100%)]\tLoss: 1.621852\n",
            "Train Epoch: 602 [100/100 (100%)]\tLoss: 1.621849\n",
            "Train Epoch: 603 [100/100 (100%)]\tLoss: 1.621848\n",
            "Train Epoch: 604 [100/100 (100%)]\tLoss: 1.621846\n",
            "Train Epoch: 605 [100/100 (100%)]\tLoss: 1.621844\n",
            "Train Epoch: 606 [100/100 (100%)]\tLoss: 1.621843\n",
            "Train Epoch: 607 [100/100 (100%)]\tLoss: 1.621841\n",
            "Train Epoch: 608 [100/100 (100%)]\tLoss: 1.621840\n",
            "Train Epoch: 609 [100/100 (100%)]\tLoss: 1.621839\n",
            "Train Epoch: 610 [100/100 (100%)]\tLoss: 1.621839\n",
            "Train Epoch: 611 [100/100 (100%)]\tLoss: 1.621838\n",
            "Train Epoch: 612 [100/100 (100%)]\tLoss: 1.621837\n",
            "Train Epoch: 613 [100/100 (100%)]\tLoss: 1.621837\n",
            "Train Epoch: 614 [100/100 (100%)]\tLoss: 1.621836\n",
            "Train Epoch: 615 [100/100 (100%)]\tLoss: 1.621835\n",
            "Train Epoch: 616 [100/100 (100%)]\tLoss: 1.621834\n",
            "Train Epoch: 617 [100/100 (100%)]\tLoss: 1.621834\n",
            "Train Epoch: 618 [100/100 (100%)]\tLoss: 1.621833\n",
            "Train Epoch: 619 [100/100 (100%)]\tLoss: 1.621832\n",
            "Train Epoch: 620 [100/100 (100%)]\tLoss: 1.621832\n",
            "Train Epoch: 621 [100/100 (100%)]\tLoss: 1.621832\n",
            "Train Epoch: 622 [100/100 (100%)]\tLoss: 1.621831\n",
            "Train Epoch: 623 [100/100 (100%)]\tLoss: 1.621830\n",
            "Train Epoch: 624 [100/100 (100%)]\tLoss: 1.621830\n",
            "Train Epoch: 625 [100/100 (100%)]\tLoss: 1.621829\n",
            "Train Epoch: 626 [100/100 (100%)]\tLoss: 1.621828\n",
            "Train Epoch: 627 [100/100 (100%)]\tLoss: 1.621828\n",
            "Train Epoch: 628 [100/100 (100%)]\tLoss: 1.621827\n",
            "Train Epoch: 629 [100/100 (100%)]\tLoss: 1.621827\n",
            "Train Epoch: 630 [100/100 (100%)]\tLoss: 1.621826\n",
            "Train Epoch: 631 [100/100 (100%)]\tLoss: 1.621825\n",
            "Train Epoch: 632 [100/100 (100%)]\tLoss: 1.621825\n",
            "Train Epoch: 633 [100/100 (100%)]\tLoss: 1.621824\n",
            "Train Epoch: 634 [100/100 (100%)]\tLoss: 1.621824\n",
            "Train Epoch: 635 [100/100 (100%)]\tLoss: 1.621823\n",
            "Train Epoch: 636 [100/100 (100%)]\tLoss: 1.621823\n",
            "Train Epoch: 637 [100/100 (100%)]\tLoss: 1.621822\n",
            "Train Epoch: 638 [100/100 (100%)]\tLoss: 1.621822\n",
            "Train Epoch: 639 [100/100 (100%)]\tLoss: 1.621821\n",
            "Train Epoch: 640 [100/100 (100%)]\tLoss: 1.621821\n",
            "Train Epoch: 641 [100/100 (100%)]\tLoss: 1.621820\n",
            "Train Epoch: 642 [100/100 (100%)]\tLoss: 1.621820\n",
            "Train Epoch: 643 [100/100 (100%)]\tLoss: 1.621819\n",
            "Train Epoch: 644 [100/100 (100%)]\tLoss: 1.621819\n",
            "Train Epoch: 645 [100/100 (100%)]\tLoss: 1.621819\n",
            "Train Epoch: 646 [100/100 (100%)]\tLoss: 1.621818\n",
            "Train Epoch: 647 [100/100 (100%)]\tLoss: 1.621818\n",
            "Train Epoch: 648 [100/100 (100%)]\tLoss: 1.621817\n",
            "Train Epoch: 649 [100/100 (100%)]\tLoss: 1.621817\n",
            "Train Epoch: 650 [100/100 (100%)]\tLoss: 1.621817\n",
            "Train Epoch: 651 [100/100 (100%)]\tLoss: 1.621817\n",
            "Train Epoch: 652 [100/100 (100%)]\tLoss: 1.621816\n",
            "Train Epoch: 653 [100/100 (100%)]\tLoss: 1.621816\n",
            "Train Epoch: 654 [100/100 (100%)]\tLoss: 1.621816\n",
            "Train Epoch: 655 [100/100 (100%)]\tLoss: 1.621815\n",
            "Train Epoch: 656 [100/100 (100%)]\tLoss: 1.621815\n",
            "Train Epoch: 657 [100/100 (100%)]\tLoss: 1.621815\n",
            "Train Epoch: 658 [100/100 (100%)]\tLoss: 1.621814\n",
            "Train Epoch: 659 [100/100 (100%)]\tLoss: 1.621814\n",
            "Train Epoch: 660 [100/100 (100%)]\tLoss: 1.621814\n",
            "Train Epoch: 661 [100/100 (100%)]\tLoss: 1.621814\n",
            "Train Epoch: 662 [100/100 (100%)]\tLoss: 1.621813\n",
            "Train Epoch: 663 [100/100 (100%)]\tLoss: 1.621813\n",
            "Train Epoch: 664 [100/100 (100%)]\tLoss: 1.621813\n",
            "Train Epoch: 665 [100/100 (100%)]\tLoss: 1.621812\n",
            "Train Epoch: 666 [100/100 (100%)]\tLoss: 1.621812\n",
            "Train Epoch: 667 [100/100 (100%)]\tLoss: 1.621811\n",
            "Train Epoch: 668 [100/100 (100%)]\tLoss: 1.621811\n",
            "Train Epoch: 669 [100/100 (100%)]\tLoss: 1.621811\n",
            "Train Epoch: 670 [100/100 (100%)]\tLoss: 1.621811\n",
            "Train Epoch: 671 [100/100 (100%)]\tLoss: 1.621810\n",
            "Train Epoch: 672 [100/100 (100%)]\tLoss: 1.621810\n",
            "Train Epoch: 673 [100/100 (100%)]\tLoss: 1.621809\n",
            "Train Epoch: 674 [100/100 (100%)]\tLoss: 1.621809\n",
            "Train Epoch: 675 [100/100 (100%)]\tLoss: 1.621808\n",
            "Train Epoch: 676 [100/100 (100%)]\tLoss: 1.621808\n",
            "Train Epoch: 677 [100/100 (100%)]\tLoss: 1.621808\n",
            "Train Epoch: 678 [100/100 (100%)]\tLoss: 1.621807\n",
            "Train Epoch: 679 [100/100 (100%)]\tLoss: 1.621807\n",
            "Train Epoch: 680 [100/100 (100%)]\tLoss: 1.621806\n",
            "Train Epoch: 681 [100/100 (100%)]\tLoss: 1.621805\n",
            "Train Epoch: 682 [100/100 (100%)]\tLoss: 1.621805\n",
            "Train Epoch: 683 [100/100 (100%)]\tLoss: 1.621804\n",
            "Train Epoch: 684 [100/100 (100%)]\tLoss: 1.621804\n",
            "Train Epoch: 685 [100/100 (100%)]\tLoss: 1.621803\n",
            "Train Epoch: 686 [100/100 (100%)]\tLoss: 1.621803\n",
            "Train Epoch: 687 [100/100 (100%)]\tLoss: 1.621802\n",
            "Train Epoch: 688 [100/100 (100%)]\tLoss: 1.621802\n",
            "Train Epoch: 689 [100/100 (100%)]\tLoss: 1.621801\n",
            "Train Epoch: 690 [100/100 (100%)]\tLoss: 1.621801\n",
            "Train Epoch: 691 [100/100 (100%)]\tLoss: 1.621800\n",
            "Train Epoch: 692 [100/100 (100%)]\tLoss: 1.621799\n",
            "Train Epoch: 693 [100/100 (100%)]\tLoss: 1.621799\n",
            "Train Epoch: 694 [100/100 (100%)]\tLoss: 1.621799\n",
            "Train Epoch: 695 [100/100 (100%)]\tLoss: 1.621798\n",
            "Train Epoch: 696 [100/100 (100%)]\tLoss: 1.621798\n",
            "Train Epoch: 697 [100/100 (100%)]\tLoss: 1.621797\n",
            "Train Epoch: 698 [100/100 (100%)]\tLoss: 1.621796\n",
            "Train Epoch: 699 [100/100 (100%)]\tLoss: 1.621796\n",
            "Train Epoch: 700 [100/100 (100%)]\tLoss: 1.621796\n",
            "Train Epoch: 701 [100/100 (100%)]\tLoss: 1.621795\n",
            "Train Epoch: 702 [100/100 (100%)]\tLoss: 1.621795\n",
            "Train Epoch: 703 [100/100 (100%)]\tLoss: 1.621794\n",
            "Train Epoch: 704 [100/100 (100%)]\tLoss: 1.621794\n",
            "Train Epoch: 705 [100/100 (100%)]\tLoss: 1.621793\n",
            "Train Epoch: 706 [100/100 (100%)]\tLoss: 1.621792\n",
            "Train Epoch: 707 [100/100 (100%)]\tLoss: 1.621792\n",
            "Train Epoch: 708 [100/100 (100%)]\tLoss: 1.621792\n",
            "Train Epoch: 709 [100/100 (100%)]\tLoss: 1.621791\n",
            "Train Epoch: 710 [100/100 (100%)]\tLoss: 1.621791\n",
            "Train Epoch: 711 [100/100 (100%)]\tLoss: 1.621791\n",
            "Train Epoch: 712 [100/100 (100%)]\tLoss: 1.621790\n",
            "Train Epoch: 713 [100/100 (100%)]\tLoss: 1.621790\n",
            "Train Epoch: 714 [100/100 (100%)]\tLoss: 1.621790\n",
            "Train Epoch: 715 [100/100 (100%)]\tLoss: 1.621789\n",
            "Train Epoch: 716 [100/100 (100%)]\tLoss: 1.621789\n",
            "Train Epoch: 717 [100/100 (100%)]\tLoss: 1.621789\n",
            "Train Epoch: 718 [100/100 (100%)]\tLoss: 1.621788\n",
            "Train Epoch: 719 [100/100 (100%)]\tLoss: 1.621788\n",
            "Train Epoch: 720 [100/100 (100%)]\tLoss: 1.621788\n",
            "Train Epoch: 721 [100/100 (100%)]\tLoss: 1.621787\n",
            "Train Epoch: 722 [100/100 (100%)]\tLoss: 1.621787\n",
            "Train Epoch: 723 [100/100 (100%)]\tLoss: 1.621787\n",
            "Train Epoch: 724 [100/100 (100%)]\tLoss: 1.621787\n",
            "Train Epoch: 725 [100/100 (100%)]\tLoss: 1.621787\n",
            "Train Epoch: 726 [100/100 (100%)]\tLoss: 1.621786\n",
            "Train Epoch: 727 [100/100 (100%)]\tLoss: 1.621786\n",
            "Train Epoch: 728 [100/100 (100%)]\tLoss: 1.621786\n",
            "Train Epoch: 729 [100/100 (100%)]\tLoss: 1.621786\n",
            "Train Epoch: 730 [100/100 (100%)]\tLoss: 1.621785\n",
            "Train Epoch: 731 [100/100 (100%)]\tLoss: 1.621785\n",
            "Train Epoch: 732 [100/100 (100%)]\tLoss: 1.621785\n",
            "Train Epoch: 733 [100/100 (100%)]\tLoss: 1.621785\n",
            "Train Epoch: 734 [100/100 (100%)]\tLoss: 1.621784\n",
            "Train Epoch: 735 [100/100 (100%)]\tLoss: 1.621784\n",
            "Train Epoch: 736 [100/100 (100%)]\tLoss: 1.621784\n",
            "Train Epoch: 737 [100/100 (100%)]\tLoss: 1.621784\n",
            "Train Epoch: 738 [100/100 (100%)]\tLoss: 1.621783\n",
            "Train Epoch: 739 [100/100 (100%)]\tLoss: 1.621783\n",
            "Train Epoch: 740 [100/100 (100%)]\tLoss: 1.621783\n",
            "Train Epoch: 741 [100/100 (100%)]\tLoss: 1.621782\n",
            "Train Epoch: 742 [100/100 (100%)]\tLoss: 1.621782\n",
            "Train Epoch: 743 [100/100 (100%)]\tLoss: 1.621782\n",
            "Train Epoch: 744 [100/100 (100%)]\tLoss: 1.621781\n",
            "Train Epoch: 745 [100/100 (100%)]\tLoss: 1.621781\n",
            "Train Epoch: 746 [100/100 (100%)]\tLoss: 1.621781\n",
            "Train Epoch: 747 [100/100 (100%)]\tLoss: 1.621780\n",
            "Train Epoch: 748 [100/100 (100%)]\tLoss: 1.621780\n",
            "Train Epoch: 749 [100/100 (100%)]\tLoss: 1.621780\n",
            "Train Epoch: 750 [100/100 (100%)]\tLoss: 1.621780\n",
            "Train Epoch: 751 [100/100 (100%)]\tLoss: 1.621779\n",
            "Train Epoch: 752 [100/100 (100%)]\tLoss: 1.621779\n",
            "Train Epoch: 753 [100/100 (100%)]\tLoss: 1.621778\n",
            "Train Epoch: 754 [100/100 (100%)]\tLoss: 1.621778\n",
            "Train Epoch: 755 [100/100 (100%)]\tLoss: 1.621778\n",
            "Train Epoch: 756 [100/100 (100%)]\tLoss: 1.621777\n",
            "Train Epoch: 757 [100/100 (100%)]\tLoss: 1.621777\n",
            "Train Epoch: 758 [100/100 (100%)]\tLoss: 1.621776\n",
            "Train Epoch: 759 [100/100 (100%)]\tLoss: 1.621776\n",
            "Train Epoch: 760 [100/100 (100%)]\tLoss: 1.621776\n",
            "Train Epoch: 761 [100/100 (100%)]\tLoss: 1.621775\n",
            "Train Epoch: 762 [100/100 (100%)]\tLoss: 1.621775\n",
            "Train Epoch: 763 [100/100 (100%)]\tLoss: 1.621774\n",
            "Train Epoch: 764 [100/100 (100%)]\tLoss: 1.621774\n",
            "Train Epoch: 765 [100/100 (100%)]\tLoss: 1.621773\n",
            "Train Epoch: 766 [100/100 (100%)]\tLoss: 1.621773\n",
            "Train Epoch: 767 [100/100 (100%)]\tLoss: 1.621772\n",
            "Train Epoch: 768 [100/100 (100%)]\tLoss: 1.621772\n",
            "Train Epoch: 769 [100/100 (100%)]\tLoss: 1.621771\n",
            "Train Epoch: 770 [100/100 (100%)]\tLoss: 1.621771\n",
            "Train Epoch: 771 [100/100 (100%)]\tLoss: 1.621771\n",
            "Train Epoch: 772 [100/100 (100%)]\tLoss: 1.621770\n",
            "Train Epoch: 773 [100/100 (100%)]\tLoss: 1.621770\n",
            "Train Epoch: 774 [100/100 (100%)]\tLoss: 1.621770\n",
            "Train Epoch: 775 [100/100 (100%)]\tLoss: 1.621769\n",
            "Train Epoch: 776 [100/100 (100%)]\tLoss: 1.621768\n",
            "Train Epoch: 777 [100/100 (100%)]\tLoss: 1.621768\n",
            "Train Epoch: 778 [100/100 (100%)]\tLoss: 1.621768\n",
            "Train Epoch: 779 [100/100 (100%)]\tLoss: 1.621767\n",
            "Train Epoch: 780 [100/100 (100%)]\tLoss: 1.621767\n",
            "Train Epoch: 781 [100/100 (100%)]\tLoss: 1.621766\n",
            "Train Epoch: 782 [100/100 (100%)]\tLoss: 1.621766\n",
            "Train Epoch: 783 [100/100 (100%)]\tLoss: 1.621765\n",
            "Train Epoch: 784 [100/100 (100%)]\tLoss: 1.621765\n",
            "Train Epoch: 785 [100/100 (100%)]\tLoss: 1.621764\n",
            "Train Epoch: 786 [100/100 (100%)]\tLoss: 1.621764\n",
            "Train Epoch: 787 [100/100 (100%)]\tLoss: 1.621763\n",
            "Train Epoch: 788 [100/100 (100%)]\tLoss: 1.621763\n",
            "Train Epoch: 789 [100/100 (100%)]\tLoss: 1.621762\n",
            "Train Epoch: 790 [100/100 (100%)]\tLoss: 1.621762\n",
            "Train Epoch: 791 [100/100 (100%)]\tLoss: 1.621761\n",
            "Train Epoch: 792 [100/100 (100%)]\tLoss: 1.621760\n",
            "Train Epoch: 793 [100/100 (100%)]\tLoss: 1.621760\n",
            "Train Epoch: 794 [100/100 (100%)]\tLoss: 1.621759\n",
            "Train Epoch: 795 [100/100 (100%)]\tLoss: 1.621759\n",
            "Train Epoch: 796 [100/100 (100%)]\tLoss: 1.621758\n",
            "Train Epoch: 797 [100/100 (100%)]\tLoss: 1.621757\n",
            "Train Epoch: 798 [100/100 (100%)]\tLoss: 1.621757\n",
            "Train Epoch: 799 [100/100 (100%)]\tLoss: 1.621756\n",
            "Train Epoch: 800 [100/100 (100%)]\tLoss: 1.621755\n",
            "Train Epoch: 801 [100/100 (100%)]\tLoss: 1.621754\n",
            "Train Epoch: 802 [100/100 (100%)]\tLoss: 1.621753\n",
            "Train Epoch: 803 [100/100 (100%)]\tLoss: 1.621752\n",
            "Train Epoch: 804 [100/100 (100%)]\tLoss: 1.621751\n",
            "Train Epoch: 805 [100/100 (100%)]\tLoss: 1.621750\n",
            "Train Epoch: 806 [100/100 (100%)]\tLoss: 1.621750\n",
            "Train Epoch: 807 [100/100 (100%)]\tLoss: 1.621748\n",
            "Train Epoch: 808 [100/100 (100%)]\tLoss: 1.621747\n",
            "Train Epoch: 809 [100/100 (100%)]\tLoss: 1.621746\n",
            "Train Epoch: 810 [100/100 (100%)]\tLoss: 1.621745\n",
            "Train Epoch: 811 [100/100 (100%)]\tLoss: 1.621744\n",
            "Train Epoch: 812 [100/100 (100%)]\tLoss: 1.621742\n",
            "Train Epoch: 813 [100/100 (100%)]\tLoss: 1.621741\n",
            "Train Epoch: 814 [100/100 (100%)]\tLoss: 1.621740\n",
            "Train Epoch: 815 [100/100 (100%)]\tLoss: 1.621738\n",
            "Train Epoch: 816 [100/100 (100%)]\tLoss: 1.621736\n",
            "Train Epoch: 817 [100/100 (100%)]\tLoss: 1.621735\n",
            "Train Epoch: 818 [100/100 (100%)]\tLoss: 1.621733\n",
            "Train Epoch: 819 [100/100 (100%)]\tLoss: 1.621731\n",
            "Train Epoch: 820 [100/100 (100%)]\tLoss: 1.621729\n",
            "Train Epoch: 821 [100/100 (100%)]\tLoss: 1.621728\n",
            "Train Epoch: 822 [100/100 (100%)]\tLoss: 1.621727\n",
            "Train Epoch: 823 [100/100 (100%)]\tLoss: 1.621725\n",
            "Train Epoch: 824 [100/100 (100%)]\tLoss: 1.621724\n",
            "Train Epoch: 825 [100/100 (100%)]\tLoss: 1.621723\n",
            "Train Epoch: 826 [100/100 (100%)]\tLoss: 1.621722\n",
            "Train Epoch: 827 [100/100 (100%)]\tLoss: 1.621722\n",
            "Train Epoch: 828 [100/100 (100%)]\tLoss: 1.621721\n",
            "Train Epoch: 829 [100/100 (100%)]\tLoss: 1.621721\n",
            "Train Epoch: 830 [100/100 (100%)]\tLoss: 1.621720\n",
            "Train Epoch: 831 [100/100 (100%)]\tLoss: 1.621720\n",
            "Train Epoch: 832 [100/100 (100%)]\tLoss: 1.621720\n",
            "Train Epoch: 833 [100/100 (100%)]\tLoss: 1.621720\n",
            "Train Epoch: 834 [100/100 (100%)]\tLoss: 1.621719\n",
            "Train Epoch: 835 [100/100 (100%)]\tLoss: 1.621719\n",
            "Train Epoch: 836 [100/100 (100%)]\tLoss: 1.621719\n",
            "Train Epoch: 837 [100/100 (100%)]\tLoss: 1.621719\n",
            "Train Epoch: 838 [100/100 (100%)]\tLoss: 1.621718\n",
            "Train Epoch: 839 [100/100 (100%)]\tLoss: 1.621718\n",
            "Train Epoch: 840 [100/100 (100%)]\tLoss: 1.621718\n",
            "Train Epoch: 841 [100/100 (100%)]\tLoss: 1.621718\n",
            "Train Epoch: 842 [100/100 (100%)]\tLoss: 1.621718\n",
            "Train Epoch: 843 [100/100 (100%)]\tLoss: 1.621717\n",
            "Train Epoch: 844 [100/100 (100%)]\tLoss: 1.621717\n",
            "Train Epoch: 845 [100/100 (100%)]\tLoss: 1.621716\n",
            "Train Epoch: 846 [100/100 (100%)]\tLoss: 1.621716\n",
            "Train Epoch: 847 [100/100 (100%)]\tLoss: 1.621716\n",
            "Train Epoch: 848 [100/100 (100%)]\tLoss: 1.621716\n",
            "Train Epoch: 849 [100/100 (100%)]\tLoss: 1.621715\n",
            "Train Epoch: 850 [100/100 (100%)]\tLoss: 1.621715\n",
            "Train Epoch: 851 [100/100 (100%)]\tLoss: 1.621714\n",
            "Train Epoch: 852 [100/100 (100%)]\tLoss: 1.621714\n",
            "Train Epoch: 853 [100/100 (100%)]\tLoss: 1.621713\n",
            "Train Epoch: 854 [100/100 (100%)]\tLoss: 1.621713\n",
            "Train Epoch: 855 [100/100 (100%)]\tLoss: 1.621712\n",
            "Train Epoch: 856 [100/100 (100%)]\tLoss: 1.621711\n",
            "Train Epoch: 857 [100/100 (100%)]\tLoss: 1.621710\n",
            "Train Epoch: 858 [100/100 (100%)]\tLoss: 1.621709\n",
            "Train Epoch: 859 [100/100 (100%)]\tLoss: 1.621707\n",
            "Train Epoch: 860 [100/100 (100%)]\tLoss: 1.621705\n",
            "Train Epoch: 861 [100/100 (100%)]\tLoss: 1.621703\n",
            "Train Epoch: 862 [100/100 (100%)]\tLoss: 1.621700\n",
            "Train Epoch: 863 [100/100 (100%)]\tLoss: 1.621697\n",
            "Train Epoch: 864 [100/100 (100%)]\tLoss: 1.621695\n",
            "Train Epoch: 865 [100/100 (100%)]\tLoss: 1.621694\n",
            "Train Epoch: 866 [100/100 (100%)]\tLoss: 1.621694\n",
            "Train Epoch: 867 [100/100 (100%)]\tLoss: 1.621695\n",
            "Train Epoch: 868 [100/100 (100%)]\tLoss: 1.621695\n",
            "Train Epoch: 869 [100/100 (100%)]\tLoss: 1.621694\n",
            "Train Epoch: 870 [100/100 (100%)]\tLoss: 1.621693\n",
            "Train Epoch: 871 [100/100 (100%)]\tLoss: 1.621692\n",
            "Train Epoch: 872 [100/100 (100%)]\tLoss: 1.621691\n",
            "Train Epoch: 873 [100/100 (100%)]\tLoss: 1.621691\n",
            "Train Epoch: 874 [100/100 (100%)]\tLoss: 1.621690\n",
            "Train Epoch: 875 [100/100 (100%)]\tLoss: 1.621689\n",
            "Train Epoch: 876 [100/100 (100%)]\tLoss: 1.621688\n",
            "Train Epoch: 877 [100/100 (100%)]\tLoss: 1.621688\n",
            "Train Epoch: 878 [100/100 (100%)]\tLoss: 1.621686\n",
            "Train Epoch: 879 [100/100 (100%)]\tLoss: 1.621685\n",
            "Train Epoch: 880 [100/100 (100%)]\tLoss: 1.621683\n",
            "Train Epoch: 881 [100/100 (100%)]\tLoss: 1.621681\n",
            "Train Epoch: 882 [100/100 (100%)]\tLoss: 1.621680\n",
            "Train Epoch: 883 [100/100 (100%)]\tLoss: 1.621679\n",
            "Train Epoch: 884 [100/100 (100%)]\tLoss: 1.621679\n",
            "Train Epoch: 885 [100/100 (100%)]\tLoss: 1.621680\n",
            "Train Epoch: 886 [100/100 (100%)]\tLoss: 1.621680\n",
            "Train Epoch: 887 [100/100 (100%)]\tLoss: 1.621679\n",
            "Train Epoch: 888 [100/100 (100%)]\tLoss: 1.621679\n",
            "Train Epoch: 889 [100/100 (100%)]\tLoss: 1.621678\n",
            "Train Epoch: 890 [100/100 (100%)]\tLoss: 1.621678\n",
            "Train Epoch: 891 [100/100 (100%)]\tLoss: 1.621678\n",
            "Train Epoch: 892 [100/100 (100%)]\tLoss: 1.621678\n",
            "Train Epoch: 893 [100/100 (100%)]\tLoss: 1.621678\n",
            "Train Epoch: 894 [100/100 (100%)]\tLoss: 1.621678\n",
            "Train Epoch: 895 [100/100 (100%)]\tLoss: 1.621678\n",
            "Train Epoch: 896 [100/100 (100%)]\tLoss: 1.621678\n",
            "Train Epoch: 897 [100/100 (100%)]\tLoss: 1.621677\n",
            "Train Epoch: 898 [100/100 (100%)]\tLoss: 1.621677\n",
            "Train Epoch: 899 [100/100 (100%)]\tLoss: 1.621676\n",
            "Train Epoch: 900 [100/100 (100%)]\tLoss: 1.621676\n",
            "Train Epoch: 901 [100/100 (100%)]\tLoss: 1.621676\n",
            "Train Epoch: 902 [100/100 (100%)]\tLoss: 1.621675\n",
            "Train Epoch: 903 [100/100 (100%)]\tLoss: 1.621675\n",
            "Train Epoch: 904 [100/100 (100%)]\tLoss: 1.621674\n",
            "Train Epoch: 905 [100/100 (100%)]\tLoss: 1.621673\n",
            "Train Epoch: 906 [100/100 (100%)]\tLoss: 1.621673\n",
            "Train Epoch: 907 [100/100 (100%)]\tLoss: 1.621671\n",
            "Train Epoch: 908 [100/100 (100%)]\tLoss: 1.621670\n",
            "Train Epoch: 909 [100/100 (100%)]\tLoss: 1.621669\n",
            "Train Epoch: 910 [100/100 (100%)]\tLoss: 1.621667\n",
            "Train Epoch: 911 [100/100 (100%)]\tLoss: 1.621666\n",
            "Train Epoch: 912 [100/100 (100%)]\tLoss: 1.621666\n",
            "Train Epoch: 913 [100/100 (100%)]\tLoss: 1.621666\n",
            "Train Epoch: 914 [100/100 (100%)]\tLoss: 1.621665\n",
            "Train Epoch: 915 [100/100 (100%)]\tLoss: 1.621665\n",
            "Train Epoch: 916 [100/100 (100%)]\tLoss: 1.621664\n",
            "Train Epoch: 917 [100/100 (100%)]\tLoss: 1.621664\n",
            "Train Epoch: 918 [100/100 (100%)]\tLoss: 1.621664\n",
            "Train Epoch: 919 [100/100 (100%)]\tLoss: 1.621663\n",
            "Train Epoch: 920 [100/100 (100%)]\tLoss: 1.621663\n",
            "Train Epoch: 921 [100/100 (100%)]\tLoss: 1.621663\n",
            "Train Epoch: 922 [100/100 (100%)]\tLoss: 1.621661\n",
            "Train Epoch: 923 [100/100 (100%)]\tLoss: 1.621660\n",
            "Train Epoch: 924 [100/100 (100%)]\tLoss: 1.621660\n",
            "Train Epoch: 925 [100/100 (100%)]\tLoss: 1.621659\n",
            "Train Epoch: 926 [100/100 (100%)]\tLoss: 1.621658\n",
            "Train Epoch: 927 [100/100 (100%)]\tLoss: 1.621657\n",
            "Train Epoch: 928 [100/100 (100%)]\tLoss: 1.621655\n",
            "Train Epoch: 929 [100/100 (100%)]\tLoss: 1.621654\n",
            "Train Epoch: 930 [100/100 (100%)]\tLoss: 1.621652\n",
            "Train Epoch: 931 [100/100 (100%)]\tLoss: 1.621651\n",
            "Train Epoch: 932 [100/100 (100%)]\tLoss: 1.621650\n",
            "Train Epoch: 933 [100/100 (100%)]\tLoss: 1.621650\n",
            "Train Epoch: 934 [100/100 (100%)]\tLoss: 1.621650\n",
            "Train Epoch: 935 [100/100 (100%)]\tLoss: 1.621649\n",
            "Train Epoch: 936 [100/100 (100%)]\tLoss: 1.621648\n",
            "Train Epoch: 937 [100/100 (100%)]\tLoss: 1.621647\n",
            "Train Epoch: 938 [100/100 (100%)]\tLoss: 1.621647\n",
            "Train Epoch: 939 [100/100 (100%)]\tLoss: 1.621646\n",
            "Train Epoch: 940 [100/100 (100%)]\tLoss: 1.621645\n",
            "Train Epoch: 941 [100/100 (100%)]\tLoss: 1.621644\n",
            "Train Epoch: 942 [100/100 (100%)]\tLoss: 1.621644\n",
            "Train Epoch: 943 [100/100 (100%)]\tLoss: 1.621644\n",
            "Train Epoch: 944 [100/100 (100%)]\tLoss: 1.621644\n",
            "Train Epoch: 945 [100/100 (100%)]\tLoss: 1.621644\n",
            "Train Epoch: 946 [100/100 (100%)]\tLoss: 1.621644\n",
            "Train Epoch: 947 [100/100 (100%)]\tLoss: 1.621644\n",
            "Train Epoch: 948 [100/100 (100%)]\tLoss: 1.621643\n",
            "Train Epoch: 949 [100/100 (100%)]\tLoss: 1.621643\n",
            "Train Epoch: 950 [100/100 (100%)]\tLoss: 1.621643\n",
            "Train Epoch: 951 [100/100 (100%)]\tLoss: 1.621643\n",
            "Train Epoch: 952 [100/100 (100%)]\tLoss: 1.621643\n",
            "Train Epoch: 953 [100/100 (100%)]\tLoss: 1.621643\n",
            "Train Epoch: 954 [100/100 (100%)]\tLoss: 1.621642\n",
            "Train Epoch: 955 [100/100 (100%)]\tLoss: 1.621642\n",
            "Train Epoch: 956 [100/100 (100%)]\tLoss: 1.621642\n",
            "Train Epoch: 957 [100/100 (100%)]\tLoss: 1.621642\n",
            "Train Epoch: 958 [100/100 (100%)]\tLoss: 1.621642\n",
            "Train Epoch: 959 [100/100 (100%)]\tLoss: 1.621641\n",
            "Train Epoch: 960 [100/100 (100%)]\tLoss: 1.621641\n",
            "Train Epoch: 961 [100/100 (100%)]\tLoss: 1.621640\n",
            "Train Epoch: 962 [100/100 (100%)]\tLoss: 1.621640\n",
            "Train Epoch: 963 [100/100 (100%)]\tLoss: 1.621640\n",
            "Train Epoch: 964 [100/100 (100%)]\tLoss: 1.621639\n",
            "Train Epoch: 965 [100/100 (100%)]\tLoss: 1.621639\n",
            "Train Epoch: 966 [100/100 (100%)]\tLoss: 1.621638\n",
            "Train Epoch: 967 [100/100 (100%)]\tLoss: 1.621637\n",
            "Train Epoch: 968 [100/100 (100%)]\tLoss: 1.621636\n",
            "Train Epoch: 969 [100/100 (100%)]\tLoss: 1.621634\n",
            "Train Epoch: 970 [100/100 (100%)]\tLoss: 1.621632\n",
            "Train Epoch: 971 [100/100 (100%)]\tLoss: 1.621629\n",
            "Train Epoch: 972 [100/100 (100%)]\tLoss: 1.621626\n",
            "Train Epoch: 973 [100/100 (100%)]\tLoss: 1.621621\n",
            "Train Epoch: 974 [100/100 (100%)]\tLoss: 1.621617\n",
            "Train Epoch: 975 [100/100 (100%)]\tLoss: 1.621614\n",
            "Train Epoch: 976 [100/100 (100%)]\tLoss: 1.621614\n",
            "Train Epoch: 977 [100/100 (100%)]\tLoss: 1.621614\n",
            "Train Epoch: 978 [100/100 (100%)]\tLoss: 1.621614\n",
            "Train Epoch: 979 [100/100 (100%)]\tLoss: 1.621613\n",
            "Train Epoch: 980 [100/100 (100%)]\tLoss: 1.621613\n",
            "Train Epoch: 981 [100/100 (100%)]\tLoss: 1.621613\n",
            "Train Epoch: 982 [100/100 (100%)]\tLoss: 1.621612\n",
            "Train Epoch: 983 [100/100 (100%)]\tLoss: 1.621612\n",
            "Train Epoch: 984 [100/100 (100%)]\tLoss: 1.621612\n",
            "Train Epoch: 985 [100/100 (100%)]\tLoss: 1.621612\n",
            "Train Epoch: 986 [100/100 (100%)]\tLoss: 1.621612\n",
            "Train Epoch: 987 [100/100 (100%)]\tLoss: 1.621611\n",
            "Train Epoch: 988 [100/100 (100%)]\tLoss: 1.621611\n",
            "Train Epoch: 989 [100/100 (100%)]\tLoss: 1.621611\n",
            "Train Epoch: 990 [100/100 (100%)]\tLoss: 1.621611\n",
            "Train Epoch: 991 [100/100 (100%)]\tLoss: 1.621611\n",
            "Train Epoch: 992 [100/100 (100%)]\tLoss: 1.621610\n",
            "Train Epoch: 993 [100/100 (100%)]\tLoss: 1.621610\n",
            "Train Epoch: 994 [100/100 (100%)]\tLoss: 1.621610\n",
            "Train Epoch: 995 [100/100 (100%)]\tLoss: 1.621610\n",
            "Train Epoch: 996 [100/100 (100%)]\tLoss: 1.621610\n",
            "Train Epoch: 997 [100/100 (100%)]\tLoss: 1.621610\n",
            "Train Epoch: 998 [100/100 (100%)]\tLoss: 1.621610\n",
            "Train Epoch: 999 [100/100 (100%)]\tLoss: 1.621610\n",
            "Train Epoch: 1000 [100/100 (100%)]\tLoss: 1.621610\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viCWrlEQBYj5",
        "colab_type": "text"
      },
      "source": [
        "Now we will use **fine-tuning**, which is doing the same, but training the new projection layer with a normal learning rate and the reused part with a very low learning rate.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YBOid2RBoOz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex2optimizer2 = torch.optim.Adam([\n",
        "    {\"params\": ex2model2.encoder.parameters(), \"lr\": 1e-4},\n",
        "    {\"params\": ex2model2.linear1.parameters(), \"lr\": 1e-3}],\n",
        "    lr = 1e-4, \n",
        "    weight_decay = 1e-5\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feWG-2SuEFFC",
        "colab_type": "text"
      },
      "source": [
        "Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dWCdbmJEJPe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "49d22b07-106c-4f38-bc43-f54f4020b574"
      },
      "source": [
        "classifier2 = train_net2(ex2model2, train_loader2, ex2optimizer2, hparams['num_epochs3'], ex2criterion)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [100/100 (100%)]\tLoss: 2.346251\n",
            "Train Epoch: 2 [100/100 (100%)]\tLoss: 2.336316\n",
            "Train Epoch: 3 [100/100 (100%)]\tLoss: 2.325684\n",
            "Train Epoch: 4 [100/100 (100%)]\tLoss: 2.314358\n",
            "Train Epoch: 5 [100/100 (100%)]\tLoss: 2.302344\n",
            "Train Epoch: 6 [100/100 (100%)]\tLoss: 2.289626\n",
            "Train Epoch: 7 [100/100 (100%)]\tLoss: 2.276194\n",
            "Train Epoch: 8 [100/100 (100%)]\tLoss: 2.262045\n",
            "Train Epoch: 9 [100/100 (100%)]\tLoss: 2.247193\n",
            "Train Epoch: 10 [100/100 (100%)]\tLoss: 2.231645\n",
            "Train Epoch: 11 [100/100 (100%)]\tLoss: 2.215457\n",
            "Train Epoch: 12 [100/100 (100%)]\tLoss: 2.198697\n",
            "Train Epoch: 13 [100/100 (100%)]\tLoss: 2.181482\n",
            "Train Epoch: 14 [100/100 (100%)]\tLoss: 2.163892\n",
            "Train Epoch: 15 [100/100 (100%)]\tLoss: 2.146031\n",
            "Train Epoch: 16 [100/100 (100%)]\tLoss: 2.128007\n",
            "Train Epoch: 17 [100/100 (100%)]\tLoss: 2.109905\n",
            "Train Epoch: 18 [100/100 (100%)]\tLoss: 2.091765\n",
            "Train Epoch: 19 [100/100 (100%)]\tLoss: 2.073599\n",
            "Train Epoch: 20 [100/100 (100%)]\tLoss: 2.055387\n",
            "Train Epoch: 21 [100/100 (100%)]\tLoss: 2.037091\n",
            "Train Epoch: 22 [100/100 (100%)]\tLoss: 2.018636\n",
            "Train Epoch: 23 [100/100 (100%)]\tLoss: 1.999954\n",
            "Train Epoch: 24 [100/100 (100%)]\tLoss: 1.981054\n",
            "Train Epoch: 25 [100/100 (100%)]\tLoss: 1.962041\n",
            "Train Epoch: 26 [100/100 (100%)]\tLoss: 1.943150\n",
            "Train Epoch: 27 [100/100 (100%)]\tLoss: 1.924770\n",
            "Train Epoch: 28 [100/100 (100%)]\tLoss: 1.907366\n",
            "Train Epoch: 29 [100/100 (100%)]\tLoss: 1.891339\n",
            "Train Epoch: 30 [100/100 (100%)]\tLoss: 1.876740\n",
            "Train Epoch: 31 [100/100 (100%)]\tLoss: 1.863147\n",
            "Train Epoch: 32 [100/100 (100%)]\tLoss: 1.850096\n",
            "Train Epoch: 33 [100/100 (100%)]\tLoss: 1.837389\n",
            "Train Epoch: 34 [100/100 (100%)]\tLoss: 1.825070\n",
            "Train Epoch: 35 [100/100 (100%)]\tLoss: 1.813314\n",
            "Train Epoch: 36 [100/100 (100%)]\tLoss: 1.802302\n",
            "Train Epoch: 37 [100/100 (100%)]\tLoss: 1.792155\n",
            "Train Epoch: 38 [100/100 (100%)]\tLoss: 1.782887\n",
            "Train Epoch: 39 [100/100 (100%)]\tLoss: 1.774384\n",
            "Train Epoch: 40 [100/100 (100%)]\tLoss: 1.766407\n",
            "Train Epoch: 41 [100/100 (100%)]\tLoss: 1.758698\n",
            "Train Epoch: 42 [100/100 (100%)]\tLoss: 1.751020\n",
            "Train Epoch: 43 [100/100 (100%)]\tLoss: 1.743212\n",
            "Train Epoch: 44 [100/100 (100%)]\tLoss: 1.735177\n",
            "Train Epoch: 45 [100/100 (100%)]\tLoss: 1.726864\n",
            "Train Epoch: 46 [100/100 (100%)]\tLoss: 1.718233\n",
            "Train Epoch: 47 [100/100 (100%)]\tLoss: 1.709243\n",
            "Train Epoch: 48 [100/100 (100%)]\tLoss: 1.699966\n",
            "Train Epoch: 49 [100/100 (100%)]\tLoss: 1.690755\n",
            "Train Epoch: 50 [100/100 (100%)]\tLoss: 1.682321\n",
            "Train Epoch: 51 [100/100 (100%)]\tLoss: 1.675461\n",
            "Train Epoch: 52 [100/100 (100%)]\tLoss: 1.670508\n",
            "Train Epoch: 53 [100/100 (100%)]\tLoss: 1.667045\n",
            "Train Epoch: 54 [100/100 (100%)]\tLoss: 1.664261\n",
            "Train Epoch: 55 [100/100 (100%)]\tLoss: 1.661572\n",
            "Train Epoch: 56 [100/100 (100%)]\tLoss: 1.658816\n",
            "Train Epoch: 57 [100/100 (100%)]\tLoss: 1.656067\n",
            "Train Epoch: 58 [100/100 (100%)]\tLoss: 1.653425\n",
            "Train Epoch: 59 [100/100 (100%)]\tLoss: 1.650942\n",
            "Train Epoch: 60 [100/100 (100%)]\tLoss: 1.648623\n",
            "Train Epoch: 61 [100/100 (100%)]\tLoss: 1.646472\n",
            "Train Epoch: 62 [100/100 (100%)]\tLoss: 1.644517\n",
            "Train Epoch: 63 [100/100 (100%)]\tLoss: 1.642736\n",
            "Train Epoch: 64 [100/100 (100%)]\tLoss: 1.640992\n",
            "Train Epoch: 65 [100/100 (100%)]\tLoss: 1.639051\n",
            "Train Epoch: 66 [100/100 (100%)]\tLoss: 1.636703\n",
            "Train Epoch: 67 [100/100 (100%)]\tLoss: 1.633990\n",
            "Train Epoch: 68 [100/100 (100%)]\tLoss: 1.631329\n",
            "Train Epoch: 69 [100/100 (100%)]\tLoss: 1.629132\n",
            "Train Epoch: 70 [100/100 (100%)]\tLoss: 1.627366\n",
            "Train Epoch: 71 [100/100 (100%)]\tLoss: 1.625781\n",
            "Train Epoch: 72 [100/100 (100%)]\tLoss: 1.624274\n",
            "Train Epoch: 73 [100/100 (100%)]\tLoss: 1.622931\n",
            "Train Epoch: 74 [100/100 (100%)]\tLoss: 1.621849\n",
            "Train Epoch: 75 [100/100 (100%)]\tLoss: 1.620975\n",
            "Train Epoch: 76 [100/100 (100%)]\tLoss: 1.620157\n",
            "Train Epoch: 77 [100/100 (100%)]\tLoss: 1.619230\n",
            "Train Epoch: 78 [100/100 (100%)]\tLoss: 1.618018\n",
            "Train Epoch: 79 [100/100 (100%)]\tLoss: 1.616301\n",
            "Train Epoch: 80 [100/100 (100%)]\tLoss: 1.613893\n",
            "Train Epoch: 81 [100/100 (100%)]\tLoss: 1.610837\n",
            "Train Epoch: 82 [100/100 (100%)]\tLoss: 1.607059\n",
            "Train Epoch: 83 [100/100 (100%)]\tLoss: 1.601545\n",
            "Train Epoch: 84 [100/100 (100%)]\tLoss: 1.594275\n",
            "Train Epoch: 85 [100/100 (100%)]\tLoss: 1.588942\n",
            "Train Epoch: 86 [100/100 (100%)]\tLoss: 1.583585\n",
            "Train Epoch: 87 [100/100 (100%)]\tLoss: 1.574803\n",
            "Train Epoch: 88 [100/100 (100%)]\tLoss: 1.565455\n",
            "Train Epoch: 89 [100/100 (100%)]\tLoss: 1.558765\n",
            "Train Epoch: 90 [100/100 (100%)]\tLoss: 1.554458\n",
            "Train Epoch: 91 [100/100 (100%)]\tLoss: 1.551289\n",
            "Train Epoch: 92 [100/100 (100%)]\tLoss: 1.548853\n",
            "Train Epoch: 93 [100/100 (100%)]\tLoss: 1.547082\n",
            "Train Epoch: 94 [100/100 (100%)]\tLoss: 1.545829\n",
            "Train Epoch: 95 [100/100 (100%)]\tLoss: 1.544903\n",
            "Train Epoch: 96 [100/100 (100%)]\tLoss: 1.544150\n",
            "Train Epoch: 97 [100/100 (100%)]\tLoss: 1.543484\n",
            "Train Epoch: 98 [100/100 (100%)]\tLoss: 1.542878\n",
            "Train Epoch: 99 [100/100 (100%)]\tLoss: 1.542335\n",
            "Train Epoch: 100 [100/100 (100%)]\tLoss: 1.541861\n",
            "Train Epoch: 101 [100/100 (100%)]\tLoss: 1.541459\n",
            "Train Epoch: 102 [100/100 (100%)]\tLoss: 1.541124\n",
            "Train Epoch: 103 [100/100 (100%)]\tLoss: 1.540846\n",
            "Train Epoch: 104 [100/100 (100%)]\tLoss: 1.540613\n",
            "Train Epoch: 105 [100/100 (100%)]\tLoss: 1.540415\n",
            "Train Epoch: 106 [100/100 (100%)]\tLoss: 1.540246\n",
            "Train Epoch: 107 [100/100 (100%)]\tLoss: 1.540097\n",
            "Train Epoch: 108 [100/100 (100%)]\tLoss: 1.539964\n",
            "Train Epoch: 109 [100/100 (100%)]\tLoss: 1.539843\n",
            "Train Epoch: 110 [100/100 (100%)]\tLoss: 1.539732\n",
            "Train Epoch: 111 [100/100 (100%)]\tLoss: 1.539630\n",
            "Train Epoch: 112 [100/100 (100%)]\tLoss: 1.539535\n",
            "Train Epoch: 113 [100/100 (100%)]\tLoss: 1.539445\n",
            "Train Epoch: 114 [100/100 (100%)]\tLoss: 1.539362\n",
            "Train Epoch: 115 [100/100 (100%)]\tLoss: 1.539284\n",
            "Train Epoch: 116 [100/100 (100%)]\tLoss: 1.539211\n",
            "Train Epoch: 117 [100/100 (100%)]\tLoss: 1.539144\n",
            "Train Epoch: 118 [100/100 (100%)]\tLoss: 1.539081\n",
            "Train Epoch: 119 [100/100 (100%)]\tLoss: 1.539023\n",
            "Train Epoch: 120 [100/100 (100%)]\tLoss: 1.538969\n",
            "Train Epoch: 121 [100/100 (100%)]\tLoss: 1.538918\n",
            "Train Epoch: 122 [100/100 (100%)]\tLoss: 1.538870\n",
            "Train Epoch: 123 [100/100 (100%)]\tLoss: 1.538825\n",
            "Train Epoch: 124 [100/100 (100%)]\tLoss: 1.538781\n",
            "Train Epoch: 125 [100/100 (100%)]\tLoss: 1.538739\n",
            "Train Epoch: 126 [100/100 (100%)]\tLoss: 1.538698\n",
            "Train Epoch: 127 [100/100 (100%)]\tLoss: 1.538658\n",
            "Train Epoch: 128 [100/100 (100%)]\tLoss: 1.538620\n",
            "Train Epoch: 129 [100/100 (100%)]\tLoss: 1.538582\n",
            "Train Epoch: 130 [100/100 (100%)]\tLoss: 1.538547\n",
            "Train Epoch: 131 [100/100 (100%)]\tLoss: 1.538513\n",
            "Train Epoch: 132 [100/100 (100%)]\tLoss: 1.538481\n",
            "Train Epoch: 133 [100/100 (100%)]\tLoss: 1.538450\n",
            "Train Epoch: 134 [100/100 (100%)]\tLoss: 1.538421\n",
            "Train Epoch: 135 [100/100 (100%)]\tLoss: 1.538393\n",
            "Train Epoch: 136 [100/100 (100%)]\tLoss: 1.538367\n",
            "Train Epoch: 137 [100/100 (100%)]\tLoss: 1.538341\n",
            "Train Epoch: 138 [100/100 (100%)]\tLoss: 1.538317\n",
            "Train Epoch: 139 [100/100 (100%)]\tLoss: 1.538294\n",
            "Train Epoch: 140 [100/100 (100%)]\tLoss: 1.538271\n",
            "Train Epoch: 141 [100/100 (100%)]\tLoss: 1.538249\n",
            "Train Epoch: 142 [100/100 (100%)]\tLoss: 1.538227\n",
            "Train Epoch: 143 [100/100 (100%)]\tLoss: 1.538206\n",
            "Train Epoch: 144 [100/100 (100%)]\tLoss: 1.538186\n",
            "Train Epoch: 145 [100/100 (100%)]\tLoss: 1.538166\n",
            "Train Epoch: 146 [100/100 (100%)]\tLoss: 1.538146\n",
            "Train Epoch: 147 [100/100 (100%)]\tLoss: 1.538128\n",
            "Train Epoch: 148 [100/100 (100%)]\tLoss: 1.538110\n",
            "Train Epoch: 149 [100/100 (100%)]\tLoss: 1.538092\n",
            "Train Epoch: 150 [100/100 (100%)]\tLoss: 1.538074\n",
            "Train Epoch: 151 [100/100 (100%)]\tLoss: 1.538056\n",
            "Train Epoch: 152 [100/100 (100%)]\tLoss: 1.538039\n",
            "Train Epoch: 153 [100/100 (100%)]\tLoss: 1.538021\n",
            "Train Epoch: 154 [100/100 (100%)]\tLoss: 1.538004\n",
            "Train Epoch: 155 [100/100 (100%)]\tLoss: 1.537987\n",
            "Train Epoch: 156 [100/100 (100%)]\tLoss: 1.537972\n",
            "Train Epoch: 157 [100/100 (100%)]\tLoss: 1.537957\n",
            "Train Epoch: 158 [100/100 (100%)]\tLoss: 1.537941\n",
            "Train Epoch: 159 [100/100 (100%)]\tLoss: 1.537925\n",
            "Train Epoch: 160 [100/100 (100%)]\tLoss: 1.537910\n",
            "Train Epoch: 161 [100/100 (100%)]\tLoss: 1.537894\n",
            "Train Epoch: 162 [100/100 (100%)]\tLoss: 1.537878\n",
            "Train Epoch: 163 [100/100 (100%)]\tLoss: 1.537862\n",
            "Train Epoch: 164 [100/100 (100%)]\tLoss: 1.537847\n",
            "Train Epoch: 165 [100/100 (100%)]\tLoss: 1.537831\n",
            "Train Epoch: 166 [100/100 (100%)]\tLoss: 1.537816\n",
            "Train Epoch: 167 [100/100 (100%)]\tLoss: 1.537801\n",
            "Train Epoch: 168 [100/100 (100%)]\tLoss: 1.537785\n",
            "Train Epoch: 169 [100/100 (100%)]\tLoss: 1.537770\n",
            "Train Epoch: 170 [100/100 (100%)]\tLoss: 1.537755\n",
            "Train Epoch: 171 [100/100 (100%)]\tLoss: 1.537738\n",
            "Train Epoch: 172 [100/100 (100%)]\tLoss: 1.537723\n",
            "Train Epoch: 173 [100/100 (100%)]\tLoss: 1.537708\n",
            "Train Epoch: 174 [100/100 (100%)]\tLoss: 1.537692\n",
            "Train Epoch: 175 [100/100 (100%)]\tLoss: 1.537677\n",
            "Train Epoch: 176 [100/100 (100%)]\tLoss: 1.537661\n",
            "Train Epoch: 177 [100/100 (100%)]\tLoss: 1.537646\n",
            "Train Epoch: 178 [100/100 (100%)]\tLoss: 1.537630\n",
            "Train Epoch: 179 [100/100 (100%)]\tLoss: 1.537615\n",
            "Train Epoch: 180 [100/100 (100%)]\tLoss: 1.537599\n",
            "Train Epoch: 181 [100/100 (100%)]\tLoss: 1.537584\n",
            "Train Epoch: 182 [100/100 (100%)]\tLoss: 1.537570\n",
            "Train Epoch: 183 [100/100 (100%)]\tLoss: 1.537555\n",
            "Train Epoch: 184 [100/100 (100%)]\tLoss: 1.537540\n",
            "Train Epoch: 185 [100/100 (100%)]\tLoss: 1.537526\n",
            "Train Epoch: 186 [100/100 (100%)]\tLoss: 1.537511\n",
            "Train Epoch: 187 [100/100 (100%)]\tLoss: 1.537497\n",
            "Train Epoch: 188 [100/100 (100%)]\tLoss: 1.537483\n",
            "Train Epoch: 189 [100/100 (100%)]\tLoss: 1.537469\n",
            "Train Epoch: 190 [100/100 (100%)]\tLoss: 1.537454\n",
            "Train Epoch: 191 [100/100 (100%)]\tLoss: 1.537440\n",
            "Train Epoch: 192 [100/100 (100%)]\tLoss: 1.537425\n",
            "Train Epoch: 193 [100/100 (100%)]\tLoss: 1.537408\n",
            "Train Epoch: 194 [100/100 (100%)]\tLoss: 1.537389\n",
            "Train Epoch: 195 [100/100 (100%)]\tLoss: 1.537367\n",
            "Train Epoch: 196 [100/100 (100%)]\tLoss: 1.537337\n",
            "Train Epoch: 197 [100/100 (100%)]\tLoss: 1.537289\n",
            "Train Epoch: 198 [100/100 (100%)]\tLoss: 1.537202\n",
            "Train Epoch: 199 [100/100 (100%)]\tLoss: 1.537014\n",
            "Train Epoch: 200 [100/100 (100%)]\tLoss: 1.536557\n",
            "Train Epoch: 201 [100/100 (100%)]\tLoss: 1.535382\n",
            "Train Epoch: 202 [100/100 (100%)]\tLoss: 1.532454\n",
            "Train Epoch: 203 [100/100 (100%)]\tLoss: 1.526438\n",
            "Train Epoch: 204 [100/100 (100%)]\tLoss: 1.515321\n",
            "Train Epoch: 205 [100/100 (100%)]\tLoss: 1.496558\n",
            "Train Epoch: 206 [100/100 (100%)]\tLoss: 1.485620\n",
            "Train Epoch: 207 [100/100 (100%)]\tLoss: 1.489392\n",
            "Train Epoch: 208 [100/100 (100%)]\tLoss: 1.482470\n",
            "Train Epoch: 209 [100/100 (100%)]\tLoss: 1.474011\n",
            "Train Epoch: 210 [100/100 (100%)]\tLoss: 1.471042\n",
            "Train Epoch: 211 [100/100 (100%)]\tLoss: 1.470384\n",
            "Train Epoch: 212 [100/100 (100%)]\tLoss: 1.469540\n",
            "Train Epoch: 213 [100/100 (100%)]\tLoss: 1.467870\n",
            "Train Epoch: 214 [100/100 (100%)]\tLoss: 1.465916\n",
            "Train Epoch: 215 [100/100 (100%)]\tLoss: 1.464363\n",
            "Train Epoch: 216 [100/100 (100%)]\tLoss: 1.463370\n",
            "Train Epoch: 217 [100/100 (100%)]\tLoss: 1.462780\n",
            "Train Epoch: 218 [100/100 (100%)]\tLoss: 1.462426\n",
            "Train Epoch: 219 [100/100 (100%)]\tLoss: 1.462205\n",
            "Train Epoch: 220 [100/100 (100%)]\tLoss: 1.462058\n",
            "Train Epoch: 221 [100/100 (100%)]\tLoss: 1.461954\n",
            "Train Epoch: 222 [100/100 (100%)]\tLoss: 1.461878\n",
            "Train Epoch: 223 [100/100 (100%)]\tLoss: 1.461817\n",
            "Train Epoch: 224 [100/100 (100%)]\tLoss: 1.461767\n",
            "Train Epoch: 225 [100/100 (100%)]\tLoss: 1.461725\n",
            "Train Epoch: 226 [100/100 (100%)]\tLoss: 1.461689\n",
            "Train Epoch: 227 [100/100 (100%)]\tLoss: 1.461657\n",
            "Train Epoch: 228 [100/100 (100%)]\tLoss: 1.461629\n",
            "Train Epoch: 229 [100/100 (100%)]\tLoss: 1.461605\n",
            "Train Epoch: 230 [100/100 (100%)]\tLoss: 1.461583\n",
            "Train Epoch: 231 [100/100 (100%)]\tLoss: 1.461564\n",
            "Train Epoch: 232 [100/100 (100%)]\tLoss: 1.461547\n",
            "Train Epoch: 233 [100/100 (100%)]\tLoss: 1.461532\n",
            "Train Epoch: 234 [100/100 (100%)]\tLoss: 1.461518\n",
            "Train Epoch: 235 [100/100 (100%)]\tLoss: 1.461506\n",
            "Train Epoch: 236 [100/100 (100%)]\tLoss: 1.461495\n",
            "Train Epoch: 237 [100/100 (100%)]\tLoss: 1.461485\n",
            "Train Epoch: 238 [100/100 (100%)]\tLoss: 1.461476\n",
            "Train Epoch: 239 [100/100 (100%)]\tLoss: 1.461467\n",
            "Train Epoch: 240 [100/100 (100%)]\tLoss: 1.461459\n",
            "Train Epoch: 241 [100/100 (100%)]\tLoss: 1.461452\n",
            "Train Epoch: 242 [100/100 (100%)]\tLoss: 1.461445\n",
            "Train Epoch: 243 [100/100 (100%)]\tLoss: 1.461438\n",
            "Train Epoch: 244 [100/100 (100%)]\tLoss: 1.461432\n",
            "Train Epoch: 245 [100/100 (100%)]\tLoss: 1.461426\n",
            "Train Epoch: 246 [100/100 (100%)]\tLoss: 1.461420\n",
            "Train Epoch: 247 [100/100 (100%)]\tLoss: 1.461414\n",
            "Train Epoch: 248 [100/100 (100%)]\tLoss: 1.461409\n",
            "Train Epoch: 249 [100/100 (100%)]\tLoss: 1.461405\n",
            "Train Epoch: 250 [100/100 (100%)]\tLoss: 1.461400\n",
            "Train Epoch: 251 [100/100 (100%)]\tLoss: 1.461395\n",
            "Train Epoch: 252 [100/100 (100%)]\tLoss: 1.461391\n",
            "Train Epoch: 253 [100/100 (100%)]\tLoss: 1.461387\n",
            "Train Epoch: 254 [100/100 (100%)]\tLoss: 1.461383\n",
            "Train Epoch: 255 [100/100 (100%)]\tLoss: 1.461380\n",
            "Train Epoch: 256 [100/100 (100%)]\tLoss: 1.461376\n",
            "Train Epoch: 257 [100/100 (100%)]\tLoss: 1.461373\n",
            "Train Epoch: 258 [100/100 (100%)]\tLoss: 1.461371\n",
            "Train Epoch: 259 [100/100 (100%)]\tLoss: 1.461368\n",
            "Train Epoch: 260 [100/100 (100%)]\tLoss: 1.461365\n",
            "Train Epoch: 261 [100/100 (100%)]\tLoss: 1.461363\n",
            "Train Epoch: 262 [100/100 (100%)]\tLoss: 1.461360\n",
            "Train Epoch: 263 [100/100 (100%)]\tLoss: 1.461358\n",
            "Train Epoch: 264 [100/100 (100%)]\tLoss: 1.461356\n",
            "Train Epoch: 265 [100/100 (100%)]\tLoss: 1.461354\n",
            "Train Epoch: 266 [100/100 (100%)]\tLoss: 1.461353\n",
            "Train Epoch: 267 [100/100 (100%)]\tLoss: 1.461351\n",
            "Train Epoch: 268 [100/100 (100%)]\tLoss: 1.461349\n",
            "Train Epoch: 269 [100/100 (100%)]\tLoss: 1.461348\n",
            "Train Epoch: 270 [100/100 (100%)]\tLoss: 1.461346\n",
            "Train Epoch: 271 [100/100 (100%)]\tLoss: 1.461345\n",
            "Train Epoch: 272 [100/100 (100%)]\tLoss: 1.461343\n",
            "Train Epoch: 273 [100/100 (100%)]\tLoss: 1.461342\n",
            "Train Epoch: 274 [100/100 (100%)]\tLoss: 1.461341\n",
            "Train Epoch: 275 [100/100 (100%)]\tLoss: 1.461339\n",
            "Train Epoch: 276 [100/100 (100%)]\tLoss: 1.461338\n",
            "Train Epoch: 277 [100/100 (100%)]\tLoss: 1.461337\n",
            "Train Epoch: 278 [100/100 (100%)]\tLoss: 1.461335\n",
            "Train Epoch: 279 [100/100 (100%)]\tLoss: 1.461334\n",
            "Train Epoch: 280 [100/100 (100%)]\tLoss: 1.461333\n",
            "Train Epoch: 281 [100/100 (100%)]\tLoss: 1.461332\n",
            "Train Epoch: 282 [100/100 (100%)]\tLoss: 1.461331\n",
            "Train Epoch: 283 [100/100 (100%)]\tLoss: 1.461329\n",
            "Train Epoch: 284 [100/100 (100%)]\tLoss: 1.461328\n",
            "Train Epoch: 285 [100/100 (100%)]\tLoss: 1.461327\n",
            "Train Epoch: 286 [100/100 (100%)]\tLoss: 1.461326\n",
            "Train Epoch: 287 [100/100 (100%)]\tLoss: 1.461324\n",
            "Train Epoch: 288 [100/100 (100%)]\tLoss: 1.461323\n",
            "Train Epoch: 289 [100/100 (100%)]\tLoss: 1.461322\n",
            "Train Epoch: 290 [100/100 (100%)]\tLoss: 1.461320\n",
            "Train Epoch: 291 [100/100 (100%)]\tLoss: 1.461319\n",
            "Train Epoch: 292 [100/100 (100%)]\tLoss: 1.461317\n",
            "Train Epoch: 293 [100/100 (100%)]\tLoss: 1.461316\n",
            "Train Epoch: 294 [100/100 (100%)]\tLoss: 1.461314\n",
            "Train Epoch: 295 [100/100 (100%)]\tLoss: 1.461313\n",
            "Train Epoch: 296 [100/100 (100%)]\tLoss: 1.461311\n",
            "Train Epoch: 297 [100/100 (100%)]\tLoss: 1.461310\n",
            "Train Epoch: 298 [100/100 (100%)]\tLoss: 1.461308\n",
            "Train Epoch: 299 [100/100 (100%)]\tLoss: 1.461306\n",
            "Train Epoch: 300 [100/100 (100%)]\tLoss: 1.461304\n",
            "Train Epoch: 301 [100/100 (100%)]\tLoss: 1.461303\n",
            "Train Epoch: 302 [100/100 (100%)]\tLoss: 1.461301\n",
            "Train Epoch: 303 [100/100 (100%)]\tLoss: 1.461299\n",
            "Train Epoch: 304 [100/100 (100%)]\tLoss: 1.461297\n",
            "Train Epoch: 305 [100/100 (100%)]\tLoss: 1.461295\n",
            "Train Epoch: 306 [100/100 (100%)]\tLoss: 1.461293\n",
            "Train Epoch: 307 [100/100 (100%)]\tLoss: 1.461291\n",
            "Train Epoch: 308 [100/100 (100%)]\tLoss: 1.461289\n",
            "Train Epoch: 309 [100/100 (100%)]\tLoss: 1.461287\n",
            "Train Epoch: 310 [100/100 (100%)]\tLoss: 1.461285\n",
            "Train Epoch: 311 [100/100 (100%)]\tLoss: 1.461283\n",
            "Train Epoch: 312 [100/100 (100%)]\tLoss: 1.461281\n",
            "Train Epoch: 313 [100/100 (100%)]\tLoss: 1.461279\n",
            "Train Epoch: 314 [100/100 (100%)]\tLoss: 1.461276\n",
            "Train Epoch: 315 [100/100 (100%)]\tLoss: 1.461274\n",
            "Train Epoch: 316 [100/100 (100%)]\tLoss: 1.461272\n",
            "Train Epoch: 317 [100/100 (100%)]\tLoss: 1.461270\n",
            "Train Epoch: 318 [100/100 (100%)]\tLoss: 1.461268\n",
            "Train Epoch: 319 [100/100 (100%)]\tLoss: 1.461265\n",
            "Train Epoch: 320 [100/100 (100%)]\tLoss: 1.461263\n",
            "Train Epoch: 321 [100/100 (100%)]\tLoss: 1.461261\n",
            "Train Epoch: 322 [100/100 (100%)]\tLoss: 1.461259\n",
            "Train Epoch: 323 [100/100 (100%)]\tLoss: 1.461257\n",
            "Train Epoch: 324 [100/100 (100%)]\tLoss: 1.461255\n",
            "Train Epoch: 325 [100/100 (100%)]\tLoss: 1.461252\n",
            "Train Epoch: 326 [100/100 (100%)]\tLoss: 1.461250\n",
            "Train Epoch: 327 [100/100 (100%)]\tLoss: 1.461249\n",
            "Train Epoch: 328 [100/100 (100%)]\tLoss: 1.461247\n",
            "Train Epoch: 329 [100/100 (100%)]\tLoss: 1.461244\n",
            "Train Epoch: 330 [100/100 (100%)]\tLoss: 1.461242\n",
            "Train Epoch: 331 [100/100 (100%)]\tLoss: 1.461241\n",
            "Train Epoch: 332 [100/100 (100%)]\tLoss: 1.461239\n",
            "Train Epoch: 333 [100/100 (100%)]\tLoss: 1.461237\n",
            "Train Epoch: 334 [100/100 (100%)]\tLoss: 1.461236\n",
            "Train Epoch: 335 [100/100 (100%)]\tLoss: 1.461234\n",
            "Train Epoch: 336 [100/100 (100%)]\tLoss: 1.461233\n",
            "Train Epoch: 337 [100/100 (100%)]\tLoss: 1.461231\n",
            "Train Epoch: 338 [100/100 (100%)]\tLoss: 1.461230\n",
            "Train Epoch: 339 [100/100 (100%)]\tLoss: 1.461228\n",
            "Train Epoch: 340 [100/100 (100%)]\tLoss: 1.461227\n",
            "Train Epoch: 341 [100/100 (100%)]\tLoss: 1.461226\n",
            "Train Epoch: 342 [100/100 (100%)]\tLoss: 1.461225\n",
            "Train Epoch: 343 [100/100 (100%)]\tLoss: 1.461223\n",
            "Train Epoch: 344 [100/100 (100%)]\tLoss: 1.461222\n",
            "Train Epoch: 345 [100/100 (100%)]\tLoss: 1.461221\n",
            "Train Epoch: 346 [100/100 (100%)]\tLoss: 1.461220\n",
            "Train Epoch: 347 [100/100 (100%)]\tLoss: 1.461219\n",
            "Train Epoch: 348 [100/100 (100%)]\tLoss: 1.461218\n",
            "Train Epoch: 349 [100/100 (100%)]\tLoss: 1.461217\n",
            "Train Epoch: 350 [100/100 (100%)]\tLoss: 1.461216\n",
            "Train Epoch: 351 [100/100 (100%)]\tLoss: 1.461215\n",
            "Train Epoch: 352 [100/100 (100%)]\tLoss: 1.461214\n",
            "Train Epoch: 353 [100/100 (100%)]\tLoss: 1.461213\n",
            "Train Epoch: 354 [100/100 (100%)]\tLoss: 1.461213\n",
            "Train Epoch: 355 [100/100 (100%)]\tLoss: 1.461212\n",
            "Train Epoch: 356 [100/100 (100%)]\tLoss: 1.461211\n",
            "Train Epoch: 357 [100/100 (100%)]\tLoss: 1.461210\n",
            "Train Epoch: 358 [100/100 (100%)]\tLoss: 1.461210\n",
            "Train Epoch: 359 [100/100 (100%)]\tLoss: 1.461209\n",
            "Train Epoch: 360 [100/100 (100%)]\tLoss: 1.461208\n",
            "Train Epoch: 361 [100/100 (100%)]\tLoss: 1.461208\n",
            "Train Epoch: 362 [100/100 (100%)]\tLoss: 1.461207\n",
            "Train Epoch: 363 [100/100 (100%)]\tLoss: 1.461207\n",
            "Train Epoch: 364 [100/100 (100%)]\tLoss: 1.461206\n",
            "Train Epoch: 365 [100/100 (100%)]\tLoss: 1.461205\n",
            "Train Epoch: 366 [100/100 (100%)]\tLoss: 1.461205\n",
            "Train Epoch: 367 [100/100 (100%)]\tLoss: 1.461205\n",
            "Train Epoch: 368 [100/100 (100%)]\tLoss: 1.461204\n",
            "Train Epoch: 369 [100/100 (100%)]\tLoss: 1.461204\n",
            "Train Epoch: 370 [100/100 (100%)]\tLoss: 1.461203\n",
            "Train Epoch: 371 [100/100 (100%)]\tLoss: 1.461203\n",
            "Train Epoch: 372 [100/100 (100%)]\tLoss: 1.461202\n",
            "Train Epoch: 373 [100/100 (100%)]\tLoss: 1.461202\n",
            "Train Epoch: 374 [100/100 (100%)]\tLoss: 1.461201\n",
            "Train Epoch: 375 [100/100 (100%)]\tLoss: 1.461201\n",
            "Train Epoch: 376 [100/100 (100%)]\tLoss: 1.461201\n",
            "Train Epoch: 377 [100/100 (100%)]\tLoss: 1.461200\n",
            "Train Epoch: 378 [100/100 (100%)]\tLoss: 1.461200\n",
            "Train Epoch: 379 [100/100 (100%)]\tLoss: 1.461199\n",
            "Train Epoch: 380 [100/100 (100%)]\tLoss: 1.461199\n",
            "Train Epoch: 381 [100/100 (100%)]\tLoss: 1.461199\n",
            "Train Epoch: 382 [100/100 (100%)]\tLoss: 1.461198\n",
            "Train Epoch: 383 [100/100 (100%)]\tLoss: 1.461198\n",
            "Train Epoch: 384 [100/100 (100%)]\tLoss: 1.461198\n",
            "Train Epoch: 385 [100/100 (100%)]\tLoss: 1.461197\n",
            "Train Epoch: 386 [100/100 (100%)]\tLoss: 1.461197\n",
            "Train Epoch: 387 [100/100 (100%)]\tLoss: 1.461197\n",
            "Train Epoch: 388 [100/100 (100%)]\tLoss: 1.461197\n",
            "Train Epoch: 389 [100/100 (100%)]\tLoss: 1.461196\n",
            "Train Epoch: 390 [100/100 (100%)]\tLoss: 1.461196\n",
            "Train Epoch: 391 [100/100 (100%)]\tLoss: 1.461195\n",
            "Train Epoch: 392 [100/100 (100%)]\tLoss: 1.461195\n",
            "Train Epoch: 393 [100/100 (100%)]\tLoss: 1.461195\n",
            "Train Epoch: 394 [100/100 (100%)]\tLoss: 1.461195\n",
            "Train Epoch: 395 [100/100 (100%)]\tLoss: 1.461195\n",
            "Train Epoch: 396 [100/100 (100%)]\tLoss: 1.461195\n",
            "Train Epoch: 397 [100/100 (100%)]\tLoss: 1.461194\n",
            "Train Epoch: 398 [100/100 (100%)]\tLoss: 1.461194\n",
            "Train Epoch: 399 [100/100 (100%)]\tLoss: 1.461194\n",
            "Train Epoch: 400 [100/100 (100%)]\tLoss: 1.461194\n",
            "Train Epoch: 401 [100/100 (100%)]\tLoss: 1.461194\n",
            "Train Epoch: 402 [100/100 (100%)]\tLoss: 1.461193\n",
            "Train Epoch: 403 [100/100 (100%)]\tLoss: 1.461193\n",
            "Train Epoch: 404 [100/100 (100%)]\tLoss: 1.461193\n",
            "Train Epoch: 405 [100/100 (100%)]\tLoss: 1.461193\n",
            "Train Epoch: 406 [100/100 (100%)]\tLoss: 1.461192\n",
            "Train Epoch: 407 [100/100 (100%)]\tLoss: 1.461192\n",
            "Train Epoch: 408 [100/100 (100%)]\tLoss: 1.461192\n",
            "Train Epoch: 409 [100/100 (100%)]\tLoss: 1.461192\n",
            "Train Epoch: 410 [100/100 (100%)]\tLoss: 1.461192\n",
            "Train Epoch: 411 [100/100 (100%)]\tLoss: 1.461191\n",
            "Train Epoch: 412 [100/100 (100%)]\tLoss: 1.461191\n",
            "Train Epoch: 413 [100/100 (100%)]\tLoss: 1.461191\n",
            "Train Epoch: 414 [100/100 (100%)]\tLoss: 1.461191\n",
            "Train Epoch: 415 [100/100 (100%)]\tLoss: 1.461191\n",
            "Train Epoch: 416 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 417 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 418 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 419 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 420 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 421 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 422 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 423 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 424 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 425 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 426 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 427 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 428 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 429 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 430 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 431 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 432 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 433 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 434 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 435 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 436 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 437 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 438 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 439 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 440 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 441 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 442 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 443 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 444 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 445 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 446 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 447 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 448 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 449 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 450 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 451 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 452 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 453 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 454 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 455 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 456 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 457 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 458 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 459 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 460 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 461 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 462 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 463 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 464 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 465 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 466 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 467 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 468 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 469 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 470 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 471 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 472 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 473 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 474 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 475 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 476 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 477 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 478 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 479 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 480 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 481 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 482 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 483 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 484 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 485 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 486 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 487 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 488 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 489 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 490 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 491 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 492 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 493 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 494 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 495 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 496 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 497 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 498 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 499 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 500 [100/100 (100%)]\tLoss: 1.461182\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4j6B5Z-ERbb",
        "colab_type": "text"
      },
      "source": [
        "Now we will start **from scratch**, which mean that we will train the model on the 100 samples without reusing the decoder weights at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wy1SBBdUFo6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ex2optimizer3 = torch.optim.Adam(ex2model3.parameters(), lr = hparams['learning_rate'],\n",
        "                             weight_decay = 1e-5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TedX9cGIGCnC",
        "colab_type": "text"
      },
      "source": [
        "Let's train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qW7x11XRGDzV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c2b44a3d-6049-40fc-cb0a-674534f39d4a"
      },
      "source": [
        "classifier3 = train_net2(ex2model3, train_loader2, ex2optimizer3, hparams['num_epochs4'], ex2criterion)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 1 [100/100 (100%)]\tLoss: 2.302834\n",
            "Train Epoch: 2 [100/100 (100%)]\tLoss: 2.270785\n",
            "Train Epoch: 3 [100/100 (100%)]\tLoss: 2.233215\n",
            "Train Epoch: 4 [100/100 (100%)]\tLoss: 2.198313\n",
            "Train Epoch: 5 [100/100 (100%)]\tLoss: 2.154681\n",
            "Train Epoch: 6 [100/100 (100%)]\tLoss: 2.113549\n",
            "Train Epoch: 7 [100/100 (100%)]\tLoss: 2.071005\n",
            "Train Epoch: 8 [100/100 (100%)]\tLoss: 2.035726\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:14: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 9 [100/100 (100%)]\tLoss: 2.012658\n",
            "Train Epoch: 10 [100/100 (100%)]\tLoss: 1.988173\n",
            "Train Epoch: 11 [100/100 (100%)]\tLoss: 1.958632\n",
            "Train Epoch: 12 [100/100 (100%)]\tLoss: 1.847847\n",
            "Train Epoch: 13 [100/100 (100%)]\tLoss: 1.992564\n",
            "Train Epoch: 14 [100/100 (100%)]\tLoss: 1.820458\n",
            "Train Epoch: 15 [100/100 (100%)]\tLoss: 1.807931\n",
            "Train Epoch: 16 [100/100 (100%)]\tLoss: 1.819330\n",
            "Train Epoch: 17 [100/100 (100%)]\tLoss: 1.774567\n",
            "Train Epoch: 18 [100/100 (100%)]\tLoss: 1.768462\n",
            "Train Epoch: 19 [100/100 (100%)]\tLoss: 1.727270\n",
            "Train Epoch: 20 [100/100 (100%)]\tLoss: 1.705996\n",
            "Train Epoch: 21 [100/100 (100%)]\tLoss: 1.693706\n",
            "Train Epoch: 22 [100/100 (100%)]\tLoss: 1.680076\n",
            "Train Epoch: 23 [100/100 (100%)]\tLoss: 1.678339\n",
            "Train Epoch: 24 [100/100 (100%)]\tLoss: 1.670923\n",
            "Train Epoch: 25 [100/100 (100%)]\tLoss: 1.656426\n",
            "Train Epoch: 26 [100/100 (100%)]\tLoss: 1.649030\n",
            "Train Epoch: 27 [100/100 (100%)]\tLoss: 1.646889\n",
            "Train Epoch: 28 [100/100 (100%)]\tLoss: 1.646050\n",
            "Train Epoch: 29 [100/100 (100%)]\tLoss: 1.644923\n",
            "Train Epoch: 30 [100/100 (100%)]\tLoss: 1.641872\n",
            "Train Epoch: 31 [100/100 (100%)]\tLoss: 1.637662\n",
            "Train Epoch: 32 [100/100 (100%)]\tLoss: 1.634209\n",
            "Train Epoch: 33 [100/100 (100%)]\tLoss: 1.632189\n",
            "Train Epoch: 34 [100/100 (100%)]\tLoss: 1.631292\n",
            "Train Epoch: 35 [100/100 (100%)]\tLoss: 1.630892\n",
            "Train Epoch: 36 [100/100 (100%)]\tLoss: 1.630539\n",
            "Train Epoch: 37 [100/100 (100%)]\tLoss: 1.630083\n",
            "Train Epoch: 38 [100/100 (100%)]\tLoss: 1.629546\n",
            "Train Epoch: 39 [100/100 (100%)]\tLoss: 1.628991\n",
            "Train Epoch: 40 [100/100 (100%)]\tLoss: 1.628457\n",
            "Train Epoch: 41 [100/100 (100%)]\tLoss: 1.627973\n",
            "Train Epoch: 42 [100/100 (100%)]\tLoss: 1.627560\n",
            "Train Epoch: 43 [100/100 (100%)]\tLoss: 1.627239\n",
            "Train Epoch: 44 [100/100 (100%)]\tLoss: 1.627007\n",
            "Train Epoch: 45 [100/100 (100%)]\tLoss: 1.626824\n",
            "Train Epoch: 46 [100/100 (100%)]\tLoss: 1.626641\n",
            "Train Epoch: 47 [100/100 (100%)]\tLoss: 1.626433\n",
            "Train Epoch: 48 [100/100 (100%)]\tLoss: 1.626200\n",
            "Train Epoch: 49 [100/100 (100%)]\tLoss: 1.625968\n",
            "Train Epoch: 50 [100/100 (100%)]\tLoss: 1.625782\n",
            "Train Epoch: 51 [100/100 (100%)]\tLoss: 1.625683\n",
            "Train Epoch: 52 [100/100 (100%)]\tLoss: 1.625666\n",
            "Train Epoch: 53 [100/100 (100%)]\tLoss: 1.625682\n",
            "Train Epoch: 54 [100/100 (100%)]\tLoss: 1.625682\n",
            "Train Epoch: 55 [100/100 (100%)]\tLoss: 1.625645\n",
            "Train Epoch: 56 [100/100 (100%)]\tLoss: 1.625575\n",
            "Train Epoch: 57 [100/100 (100%)]\tLoss: 1.625492\n",
            "Train Epoch: 58 [100/100 (100%)]\tLoss: 1.625417\n",
            "Train Epoch: 59 [100/100 (100%)]\tLoss: 1.625356\n",
            "Train Epoch: 60 [100/100 (100%)]\tLoss: 1.625301\n",
            "Train Epoch: 61 [100/100 (100%)]\tLoss: 1.625244\n",
            "Train Epoch: 62 [100/100 (100%)]\tLoss: 1.625187\n",
            "Train Epoch: 63 [100/100 (100%)]\tLoss: 1.625140\n",
            "Train Epoch: 64 [100/100 (100%)]\tLoss: 1.625112\n",
            "Train Epoch: 65 [100/100 (100%)]\tLoss: 1.625099\n",
            "Train Epoch: 66 [100/100 (100%)]\tLoss: 1.625088\n",
            "Train Epoch: 67 [100/100 (100%)]\tLoss: 1.625065\n",
            "Train Epoch: 68 [100/100 (100%)]\tLoss: 1.625024\n",
            "Train Epoch: 69 [100/100 (100%)]\tLoss: 1.624973\n",
            "Train Epoch: 70 [100/100 (100%)]\tLoss: 1.624921\n",
            "Train Epoch: 71 [100/100 (100%)]\tLoss: 1.624876\n",
            "Train Epoch: 72 [100/100 (100%)]\tLoss: 1.624838\n",
            "Train Epoch: 73 [100/100 (100%)]\tLoss: 1.624801\n",
            "Train Epoch: 74 [100/100 (100%)]\tLoss: 1.624759\n",
            "Train Epoch: 75 [100/100 (100%)]\tLoss: 1.624715\n",
            "Train Epoch: 76 [100/100 (100%)]\tLoss: 1.624673\n",
            "Train Epoch: 77 [100/100 (100%)]\tLoss: 1.624638\n",
            "Train Epoch: 78 [100/100 (100%)]\tLoss: 1.624608\n",
            "Train Epoch: 79 [100/100 (100%)]\tLoss: 1.624578\n",
            "Train Epoch: 80 [100/100 (100%)]\tLoss: 1.624544\n",
            "Train Epoch: 81 [100/100 (100%)]\tLoss: 1.624506\n",
            "Train Epoch: 82 [100/100 (100%)]\tLoss: 1.624467\n",
            "Train Epoch: 83 [100/100 (100%)]\tLoss: 1.624429\n",
            "Train Epoch: 84 [100/100 (100%)]\tLoss: 1.624392\n",
            "Train Epoch: 85 [100/100 (100%)]\tLoss: 1.624353\n",
            "Train Epoch: 86 [100/100 (100%)]\tLoss: 1.624312\n",
            "Train Epoch: 87 [100/100 (100%)]\tLoss: 1.624271\n",
            "Train Epoch: 88 [100/100 (100%)]\tLoss: 1.624233\n",
            "Train Epoch: 89 [100/100 (100%)]\tLoss: 1.624197\n",
            "Train Epoch: 90 [100/100 (100%)]\tLoss: 1.624162\n",
            "Train Epoch: 91 [100/100 (100%)]\tLoss: 1.624127\n",
            "Train Epoch: 92 [100/100 (100%)]\tLoss: 1.624093\n",
            "Train Epoch: 93 [100/100 (100%)]\tLoss: 1.624062\n",
            "Train Epoch: 94 [100/100 (100%)]\tLoss: 1.624037\n",
            "Train Epoch: 95 [100/100 (100%)]\tLoss: 1.624018\n",
            "Train Epoch: 96 [100/100 (100%)]\tLoss: 1.624003\n",
            "Train Epoch: 97 [100/100 (100%)]\tLoss: 1.623990\n",
            "Train Epoch: 98 [100/100 (100%)]\tLoss: 1.623980\n",
            "Train Epoch: 99 [100/100 (100%)]\tLoss: 1.623970\n",
            "Train Epoch: 100 [100/100 (100%)]\tLoss: 1.623961\n",
            "Train Epoch: 101 [100/100 (100%)]\tLoss: 1.623952\n",
            "Train Epoch: 102 [100/100 (100%)]\tLoss: 1.623943\n",
            "Train Epoch: 103 [100/100 (100%)]\tLoss: 1.623934\n",
            "Train Epoch: 104 [100/100 (100%)]\tLoss: 1.623925\n",
            "Train Epoch: 105 [100/100 (100%)]\tLoss: 1.623917\n",
            "Train Epoch: 106 [100/100 (100%)]\tLoss: 1.623908\n",
            "Train Epoch: 107 [100/100 (100%)]\tLoss: 1.623899\n",
            "Train Epoch: 108 [100/100 (100%)]\tLoss: 1.623890\n",
            "Train Epoch: 109 [100/100 (100%)]\tLoss: 1.623881\n",
            "Train Epoch: 110 [100/100 (100%)]\tLoss: 1.623873\n",
            "Train Epoch: 111 [100/100 (100%)]\tLoss: 1.623864\n",
            "Train Epoch: 112 [100/100 (100%)]\tLoss: 1.623854\n",
            "Train Epoch: 113 [100/100 (100%)]\tLoss: 1.623844\n",
            "Train Epoch: 114 [100/100 (100%)]\tLoss: 1.623834\n",
            "Train Epoch: 115 [100/100 (100%)]\tLoss: 1.623824\n",
            "Train Epoch: 116 [100/100 (100%)]\tLoss: 1.623815\n",
            "Train Epoch: 117 [100/100 (100%)]\tLoss: 1.623805\n",
            "Train Epoch: 118 [100/100 (100%)]\tLoss: 1.623795\n",
            "Train Epoch: 119 [100/100 (100%)]\tLoss: 1.623786\n",
            "Train Epoch: 120 [100/100 (100%)]\tLoss: 1.623776\n",
            "Train Epoch: 121 [100/100 (100%)]\tLoss: 1.623766\n",
            "Train Epoch: 122 [100/100 (100%)]\tLoss: 1.623756\n",
            "Train Epoch: 123 [100/100 (100%)]\tLoss: 1.623746\n",
            "Train Epoch: 124 [100/100 (100%)]\tLoss: 1.623736\n",
            "Train Epoch: 125 [100/100 (100%)]\tLoss: 1.623726\n",
            "Train Epoch: 126 [100/100 (100%)]\tLoss: 1.623715\n",
            "Train Epoch: 127 [100/100 (100%)]\tLoss: 1.623705\n",
            "Train Epoch: 128 [100/100 (100%)]\tLoss: 1.623695\n",
            "Train Epoch: 129 [100/100 (100%)]\tLoss: 1.623684\n",
            "Train Epoch: 130 [100/100 (100%)]\tLoss: 1.623673\n",
            "Train Epoch: 131 [100/100 (100%)]\tLoss: 1.623661\n",
            "Train Epoch: 132 [100/100 (100%)]\tLoss: 1.623649\n",
            "Train Epoch: 133 [100/100 (100%)]\tLoss: 1.623637\n",
            "Train Epoch: 134 [100/100 (100%)]\tLoss: 1.623623\n",
            "Train Epoch: 135 [100/100 (100%)]\tLoss: 1.623610\n",
            "Train Epoch: 136 [100/100 (100%)]\tLoss: 1.623595\n",
            "Train Epoch: 137 [100/100 (100%)]\tLoss: 1.623579\n",
            "Train Epoch: 138 [100/100 (100%)]\tLoss: 1.623561\n",
            "Train Epoch: 139 [100/100 (100%)]\tLoss: 1.623541\n",
            "Train Epoch: 140 [100/100 (100%)]\tLoss: 1.623518\n",
            "Train Epoch: 141 [100/100 (100%)]\tLoss: 1.623491\n",
            "Train Epoch: 142 [100/100 (100%)]\tLoss: 1.623458\n",
            "Train Epoch: 143 [100/100 (100%)]\tLoss: 1.623415\n",
            "Train Epoch: 144 [100/100 (100%)]\tLoss: 1.623358\n",
            "Train Epoch: 145 [100/100 (100%)]\tLoss: 1.623273\n",
            "Train Epoch: 146 [100/100 (100%)]\tLoss: 1.623135\n",
            "Train Epoch: 147 [100/100 (100%)]\tLoss: 1.622878\n",
            "Train Epoch: 148 [100/100 (100%)]\tLoss: 1.622308\n",
            "Train Epoch: 149 [100/100 (100%)]\tLoss: 1.620793\n",
            "Train Epoch: 150 [100/100 (100%)]\tLoss: 1.617639\n",
            "Train Epoch: 151 [100/100 (100%)]\tLoss: 1.616370\n",
            "Train Epoch: 152 [100/100 (100%)]\tLoss: 1.616318\n",
            "Train Epoch: 153 [100/100 (100%)]\tLoss: 1.615517\n",
            "Train Epoch: 154 [100/100 (100%)]\tLoss: 1.615016\n",
            "Train Epoch: 155 [100/100 (100%)]\tLoss: 1.615320\n",
            "Train Epoch: 156 [100/100 (100%)]\tLoss: 1.615594\n",
            "Train Epoch: 157 [100/100 (100%)]\tLoss: 1.615678\n",
            "Train Epoch: 158 [100/100 (100%)]\tLoss: 1.615631\n",
            "Train Epoch: 159 [100/100 (100%)]\tLoss: 1.615486\n",
            "Train Epoch: 160 [100/100 (100%)]\tLoss: 1.615275\n",
            "Train Epoch: 161 [100/100 (100%)]\tLoss: 1.615027\n",
            "Train Epoch: 162 [100/100 (100%)]\tLoss: 1.614809\n",
            "Train Epoch: 163 [100/100 (100%)]\tLoss: 1.614783\n",
            "Train Epoch: 164 [100/100 (100%)]\tLoss: 1.614939\n",
            "Train Epoch: 165 [100/100 (100%)]\tLoss: 1.614949\n",
            "Train Epoch: 166 [100/100 (100%)]\tLoss: 1.614733\n",
            "Train Epoch: 167 [100/100 (100%)]\tLoss: 1.614526\n",
            "Train Epoch: 168 [100/100 (100%)]\tLoss: 1.614444\n",
            "Train Epoch: 169 [100/100 (100%)]\tLoss: 1.614388\n",
            "Train Epoch: 170 [100/100 (100%)]\tLoss: 1.614303\n",
            "Train Epoch: 171 [100/100 (100%)]\tLoss: 1.614243\n",
            "Train Epoch: 172 [100/100 (100%)]\tLoss: 1.614226\n",
            "Train Epoch: 173 [100/100 (100%)]\tLoss: 1.614212\n",
            "Train Epoch: 174 [100/100 (100%)]\tLoss: 1.614187\n",
            "Train Epoch: 175 [100/100 (100%)]\tLoss: 1.614158\n",
            "Train Epoch: 176 [100/100 (100%)]\tLoss: 1.614117\n",
            "Train Epoch: 177 [100/100 (100%)]\tLoss: 1.614073\n",
            "Train Epoch: 178 [100/100 (100%)]\tLoss: 1.614039\n",
            "Train Epoch: 179 [100/100 (100%)]\tLoss: 1.614004\n",
            "Train Epoch: 180 [100/100 (100%)]\tLoss: 1.613923\n",
            "Train Epoch: 181 [100/100 (100%)]\tLoss: 1.612963\n",
            "Train Epoch: 182 [100/100 (100%)]\tLoss: 1.594902\n",
            "Train Epoch: 183 [100/100 (100%)]\tLoss: 1.585074\n",
            "Train Epoch: 184 [100/100 (100%)]\tLoss: 1.584989\n",
            "Train Epoch: 185 [100/100 (100%)]\tLoss: 1.579340\n",
            "Train Epoch: 186 [100/100 (100%)]\tLoss: 1.581318\n",
            "Train Epoch: 187 [100/100 (100%)]\tLoss: 1.571713\n",
            "Train Epoch: 188 [100/100 (100%)]\tLoss: 1.553024\n",
            "Train Epoch: 189 [100/100 (100%)]\tLoss: 1.567947\n",
            "Train Epoch: 190 [100/100 (100%)]\tLoss: 1.549997\n",
            "Train Epoch: 191 [100/100 (100%)]\tLoss: 1.558485\n",
            "Train Epoch: 192 [100/100 (100%)]\tLoss: 1.556062\n",
            "Train Epoch: 193 [100/100 (100%)]\tLoss: 1.547647\n",
            "Train Epoch: 194 [100/100 (100%)]\tLoss: 1.543564\n",
            "Train Epoch: 195 [100/100 (100%)]\tLoss: 1.547125\n",
            "Train Epoch: 196 [100/100 (100%)]\tLoss: 1.546434\n",
            "Train Epoch: 197 [100/100 (100%)]\tLoss: 1.541112\n",
            "Train Epoch: 198 [100/100 (100%)]\tLoss: 1.540513\n",
            "Train Epoch: 199 [100/100 (100%)]\tLoss: 1.541727\n",
            "Train Epoch: 200 [100/100 (100%)]\tLoss: 1.541800\n",
            "Train Epoch: 201 [100/100 (100%)]\tLoss: 1.540475\n",
            "Train Epoch: 202 [100/100 (100%)]\tLoss: 1.539247\n",
            "Train Epoch: 203 [100/100 (100%)]\tLoss: 1.538935\n",
            "Train Epoch: 204 [100/100 (100%)]\tLoss: 1.539161\n",
            "Train Epoch: 205 [100/100 (100%)]\tLoss: 1.539447\n",
            "Train Epoch: 206 [100/100 (100%)]\tLoss: 1.539523\n",
            "Train Epoch: 207 [100/100 (100%)]\tLoss: 1.539292\n",
            "Train Epoch: 208 [100/100 (100%)]\tLoss: 1.538892\n",
            "Train Epoch: 209 [100/100 (100%)]\tLoss: 1.538538\n",
            "Train Epoch: 210 [100/100 (100%)]\tLoss: 1.538355\n",
            "Train Epoch: 211 [100/100 (100%)]\tLoss: 1.538323\n",
            "Train Epoch: 212 [100/100 (100%)]\tLoss: 1.538334\n",
            "Train Epoch: 213 [100/100 (100%)]\tLoss: 1.538316\n",
            "Train Epoch: 214 [100/100 (100%)]\tLoss: 1.538266\n",
            "Train Epoch: 215 [100/100 (100%)]\tLoss: 1.538200\n",
            "Train Epoch: 216 [100/100 (100%)]\tLoss: 1.538126\n",
            "Train Epoch: 217 [100/100 (100%)]\tLoss: 1.538047\n",
            "Train Epoch: 218 [100/100 (100%)]\tLoss: 1.537968\n",
            "Train Epoch: 219 [100/100 (100%)]\tLoss: 1.537896\n",
            "Train Epoch: 220 [100/100 (100%)]\tLoss: 1.537838\n",
            "Train Epoch: 221 [100/100 (100%)]\tLoss: 1.537798\n",
            "Train Epoch: 222 [100/100 (100%)]\tLoss: 1.537776\n",
            "Train Epoch: 223 [100/100 (100%)]\tLoss: 1.537767\n",
            "Train Epoch: 224 [100/100 (100%)]\tLoss: 1.537760\n",
            "Train Epoch: 225 [100/100 (100%)]\tLoss: 1.537745\n",
            "Train Epoch: 226 [100/100 (100%)]\tLoss: 1.537717\n",
            "Train Epoch: 227 [100/100 (100%)]\tLoss: 1.537681\n",
            "Train Epoch: 228 [100/100 (100%)]\tLoss: 1.537640\n",
            "Train Epoch: 229 [100/100 (100%)]\tLoss: 1.537602\n",
            "Train Epoch: 230 [100/100 (100%)]\tLoss: 1.537573\n",
            "Train Epoch: 231 [100/100 (100%)]\tLoss: 1.537556\n",
            "Train Epoch: 232 [100/100 (100%)]\tLoss: 1.537548\n",
            "Train Epoch: 233 [100/100 (100%)]\tLoss: 1.537544\n",
            "Train Epoch: 234 [100/100 (100%)]\tLoss: 1.537540\n",
            "Train Epoch: 235 [100/100 (100%)]\tLoss: 1.537534\n",
            "Train Epoch: 236 [100/100 (100%)]\tLoss: 1.537524\n",
            "Train Epoch: 237 [100/100 (100%)]\tLoss: 1.537513\n",
            "Train Epoch: 238 [100/100 (100%)]\tLoss: 1.537500\n",
            "Train Epoch: 239 [100/100 (100%)]\tLoss: 1.537489\n",
            "Train Epoch: 240 [100/100 (100%)]\tLoss: 1.537478\n",
            "Train Epoch: 241 [100/100 (100%)]\tLoss: 1.537470\n",
            "Train Epoch: 242 [100/100 (100%)]\tLoss: 1.537464\n",
            "Train Epoch: 243 [100/100 (100%)]\tLoss: 1.537459\n",
            "Train Epoch: 244 [100/100 (100%)]\tLoss: 1.537454\n",
            "Train Epoch: 245 [100/100 (100%)]\tLoss: 1.537448\n",
            "Train Epoch: 246 [100/100 (100%)]\tLoss: 1.537440\n",
            "Train Epoch: 247 [100/100 (100%)]\tLoss: 1.537431\n",
            "Train Epoch: 248 [100/100 (100%)]\tLoss: 1.537423\n",
            "Train Epoch: 249 [100/100 (100%)]\tLoss: 1.537417\n",
            "Train Epoch: 250 [100/100 (100%)]\tLoss: 1.537411\n",
            "Train Epoch: 251 [100/100 (100%)]\tLoss: 1.537407\n",
            "Train Epoch: 252 [100/100 (100%)]\tLoss: 1.537403\n",
            "Train Epoch: 253 [100/100 (100%)]\tLoss: 1.537398\n",
            "Train Epoch: 254 [100/100 (100%)]\tLoss: 1.537393\n",
            "Train Epoch: 255 [100/100 (100%)]\tLoss: 1.537388\n",
            "Train Epoch: 256 [100/100 (100%)]\tLoss: 1.537381\n",
            "Train Epoch: 257 [100/100 (100%)]\tLoss: 1.537374\n",
            "Train Epoch: 258 [100/100 (100%)]\tLoss: 1.537367\n",
            "Train Epoch: 259 [100/100 (100%)]\tLoss: 1.537361\n",
            "Train Epoch: 260 [100/100 (100%)]\tLoss: 1.537354\n",
            "Train Epoch: 261 [100/100 (100%)]\tLoss: 1.537349\n",
            "Train Epoch: 262 [100/100 (100%)]\tLoss: 1.537343\n",
            "Train Epoch: 263 [100/100 (100%)]\tLoss: 1.537337\n",
            "Train Epoch: 264 [100/100 (100%)]\tLoss: 1.537331\n",
            "Train Epoch: 265 [100/100 (100%)]\tLoss: 1.537325\n",
            "Train Epoch: 266 [100/100 (100%)]\tLoss: 1.537319\n",
            "Train Epoch: 267 [100/100 (100%)]\tLoss: 1.537313\n",
            "Train Epoch: 268 [100/100 (100%)]\tLoss: 1.537307\n",
            "Train Epoch: 269 [100/100 (100%)]\tLoss: 1.537301\n",
            "Train Epoch: 270 [100/100 (100%)]\tLoss: 1.537296\n",
            "Train Epoch: 271 [100/100 (100%)]\tLoss: 1.537292\n",
            "Train Epoch: 272 [100/100 (100%)]\tLoss: 1.537288\n",
            "Train Epoch: 273 [100/100 (100%)]\tLoss: 1.537284\n",
            "Train Epoch: 274 [100/100 (100%)]\tLoss: 1.537281\n",
            "Train Epoch: 275 [100/100 (100%)]\tLoss: 1.537278\n",
            "Train Epoch: 276 [100/100 (100%)]\tLoss: 1.537275\n",
            "Train Epoch: 277 [100/100 (100%)]\tLoss: 1.537273\n",
            "Train Epoch: 278 [100/100 (100%)]\tLoss: 1.537271\n",
            "Train Epoch: 279 [100/100 (100%)]\tLoss: 1.537269\n",
            "Train Epoch: 280 [100/100 (100%)]\tLoss: 1.537267\n",
            "Train Epoch: 281 [100/100 (100%)]\tLoss: 1.537266\n",
            "Train Epoch: 282 [100/100 (100%)]\tLoss: 1.537264\n",
            "Train Epoch: 283 [100/100 (100%)]\tLoss: 1.537263\n",
            "Train Epoch: 284 [100/100 (100%)]\tLoss: 1.537260\n",
            "Train Epoch: 285 [100/100 (100%)]\tLoss: 1.537258\n",
            "Train Epoch: 286 [100/100 (100%)]\tLoss: 1.537253\n",
            "Train Epoch: 287 [100/100 (100%)]\tLoss: 1.537244\n",
            "Train Epoch: 288 [100/100 (100%)]\tLoss: 1.537207\n",
            "Train Epoch: 289 [100/100 (100%)]\tLoss: 1.536700\n",
            "Train Epoch: 290 [100/100 (100%)]\tLoss: 1.504346\n",
            "Train Epoch: 291 [100/100 (100%)]\tLoss: 1.462312\n",
            "Train Epoch: 292 [100/100 (100%)]\tLoss: 1.507701\n",
            "Train Epoch: 293 [100/100 (100%)]\tLoss: 1.461743\n",
            "Train Epoch: 294 [100/100 (100%)]\tLoss: 1.462867\n",
            "Train Epoch: 295 [100/100 (100%)]\tLoss: 1.470389\n",
            "Train Epoch: 296 [100/100 (100%)]\tLoss: 1.474648\n",
            "Train Epoch: 297 [100/100 (100%)]\tLoss: 1.472148\n",
            "Train Epoch: 298 [100/100 (100%)]\tLoss: 1.465530\n",
            "Train Epoch: 299 [100/100 (100%)]\tLoss: 1.462144\n",
            "Train Epoch: 300 [100/100 (100%)]\tLoss: 1.461692\n",
            "Train Epoch: 301 [100/100 (100%)]\tLoss: 1.461610\n",
            "Train Epoch: 302 [100/100 (100%)]\tLoss: 1.461585\n",
            "Train Epoch: 303 [100/100 (100%)]\tLoss: 1.461591\n",
            "Train Epoch: 304 [100/100 (100%)]\tLoss: 1.461633\n",
            "Train Epoch: 305 [100/100 (100%)]\tLoss: 1.461715\n",
            "Train Epoch: 306 [100/100 (100%)]\tLoss: 1.461829\n",
            "Train Epoch: 307 [100/100 (100%)]\tLoss: 1.461952\n",
            "Train Epoch: 308 [100/100 (100%)]\tLoss: 1.462043\n",
            "Train Epoch: 309 [100/100 (100%)]\tLoss: 1.462066\n",
            "Train Epoch: 310 [100/100 (100%)]\tLoss: 1.462014\n",
            "Train Epoch: 311 [100/100 (100%)]\tLoss: 1.461907\n",
            "Train Epoch: 312 [100/100 (100%)]\tLoss: 1.461783\n",
            "Train Epoch: 313 [100/100 (100%)]\tLoss: 1.461667\n",
            "Train Epoch: 314 [100/100 (100%)]\tLoss: 1.461572\n",
            "Train Epoch: 315 [100/100 (100%)]\tLoss: 1.461500\n",
            "Train Epoch: 316 [100/100 (100%)]\tLoss: 1.461447\n",
            "Train Epoch: 317 [100/100 (100%)]\tLoss: 1.461408\n",
            "Train Epoch: 318 [100/100 (100%)]\tLoss: 1.461380\n",
            "Train Epoch: 319 [100/100 (100%)]\tLoss: 1.461359\n",
            "Train Epoch: 320 [100/100 (100%)]\tLoss: 1.461343\n",
            "Train Epoch: 321 [100/100 (100%)]\tLoss: 1.461330\n",
            "Train Epoch: 322 [100/100 (100%)]\tLoss: 1.461320\n",
            "Train Epoch: 323 [100/100 (100%)]\tLoss: 1.461311\n",
            "Train Epoch: 324 [100/100 (100%)]\tLoss: 1.461303\n",
            "Train Epoch: 325 [100/100 (100%)]\tLoss: 1.461296\n",
            "Train Epoch: 326 [100/100 (100%)]\tLoss: 1.461290\n",
            "Train Epoch: 327 [100/100 (100%)]\tLoss: 1.461284\n",
            "Train Epoch: 328 [100/100 (100%)]\tLoss: 1.461280\n",
            "Train Epoch: 329 [100/100 (100%)]\tLoss: 1.461275\n",
            "Train Epoch: 330 [100/100 (100%)]\tLoss: 1.461270\n",
            "Train Epoch: 331 [100/100 (100%)]\tLoss: 1.461266\n",
            "Train Epoch: 332 [100/100 (100%)]\tLoss: 1.461262\n",
            "Train Epoch: 333 [100/100 (100%)]\tLoss: 1.461258\n",
            "Train Epoch: 334 [100/100 (100%)]\tLoss: 1.461255\n",
            "Train Epoch: 335 [100/100 (100%)]\tLoss: 1.461251\n",
            "Train Epoch: 336 [100/100 (100%)]\tLoss: 1.461249\n",
            "Train Epoch: 337 [100/100 (100%)]\tLoss: 1.461245\n",
            "Train Epoch: 338 [100/100 (100%)]\tLoss: 1.461243\n",
            "Train Epoch: 339 [100/100 (100%)]\tLoss: 1.461240\n",
            "Train Epoch: 340 [100/100 (100%)]\tLoss: 1.461238\n",
            "Train Epoch: 341 [100/100 (100%)]\tLoss: 1.461236\n",
            "Train Epoch: 342 [100/100 (100%)]\tLoss: 1.461234\n",
            "Train Epoch: 343 [100/100 (100%)]\tLoss: 1.461232\n",
            "Train Epoch: 344 [100/100 (100%)]\tLoss: 1.461230\n",
            "Train Epoch: 345 [100/100 (100%)]\tLoss: 1.461228\n",
            "Train Epoch: 346 [100/100 (100%)]\tLoss: 1.461227\n",
            "Train Epoch: 347 [100/100 (100%)]\tLoss: 1.461225\n",
            "Train Epoch: 348 [100/100 (100%)]\tLoss: 1.461224\n",
            "Train Epoch: 349 [100/100 (100%)]\tLoss: 1.461223\n",
            "Train Epoch: 350 [100/100 (100%)]\tLoss: 1.461221\n",
            "Train Epoch: 351 [100/100 (100%)]\tLoss: 1.461220\n",
            "Train Epoch: 352 [100/100 (100%)]\tLoss: 1.461219\n",
            "Train Epoch: 353 [100/100 (100%)]\tLoss: 1.461218\n",
            "Train Epoch: 354 [100/100 (100%)]\tLoss: 1.461216\n",
            "Train Epoch: 355 [100/100 (100%)]\tLoss: 1.461215\n",
            "Train Epoch: 356 [100/100 (100%)]\tLoss: 1.461214\n",
            "Train Epoch: 357 [100/100 (100%)]\tLoss: 1.461213\n",
            "Train Epoch: 358 [100/100 (100%)]\tLoss: 1.461213\n",
            "Train Epoch: 359 [100/100 (100%)]\tLoss: 1.461212\n",
            "Train Epoch: 360 [100/100 (100%)]\tLoss: 1.461211\n",
            "Train Epoch: 361 [100/100 (100%)]\tLoss: 1.461210\n",
            "Train Epoch: 362 [100/100 (100%)]\tLoss: 1.461209\n",
            "Train Epoch: 363 [100/100 (100%)]\tLoss: 1.461209\n",
            "Train Epoch: 364 [100/100 (100%)]\tLoss: 1.461208\n",
            "Train Epoch: 365 [100/100 (100%)]\tLoss: 1.461207\n",
            "Train Epoch: 366 [100/100 (100%)]\tLoss: 1.461207\n",
            "Train Epoch: 367 [100/100 (100%)]\tLoss: 1.461206\n",
            "Train Epoch: 368 [100/100 (100%)]\tLoss: 1.461205\n",
            "Train Epoch: 369 [100/100 (100%)]\tLoss: 1.461205\n",
            "Train Epoch: 370 [100/100 (100%)]\tLoss: 1.461204\n",
            "Train Epoch: 371 [100/100 (100%)]\tLoss: 1.461203\n",
            "Train Epoch: 372 [100/100 (100%)]\tLoss: 1.461203\n",
            "Train Epoch: 373 [100/100 (100%)]\tLoss: 1.461202\n",
            "Train Epoch: 374 [100/100 (100%)]\tLoss: 1.461202\n",
            "Train Epoch: 375 [100/100 (100%)]\tLoss: 1.461201\n",
            "Train Epoch: 376 [100/100 (100%)]\tLoss: 1.461201\n",
            "Train Epoch: 377 [100/100 (100%)]\tLoss: 1.461201\n",
            "Train Epoch: 378 [100/100 (100%)]\tLoss: 1.461200\n",
            "Train Epoch: 379 [100/100 (100%)]\tLoss: 1.461200\n",
            "Train Epoch: 380 [100/100 (100%)]\tLoss: 1.461199\n",
            "Train Epoch: 381 [100/100 (100%)]\tLoss: 1.461199\n",
            "Train Epoch: 382 [100/100 (100%)]\tLoss: 1.461198\n",
            "Train Epoch: 383 [100/100 (100%)]\tLoss: 1.461197\n",
            "Train Epoch: 384 [100/100 (100%)]\tLoss: 1.461197\n",
            "Train Epoch: 385 [100/100 (100%)]\tLoss: 1.461197\n",
            "Train Epoch: 386 [100/100 (100%)]\tLoss: 1.461196\n",
            "Train Epoch: 387 [100/100 (100%)]\tLoss: 1.461196\n",
            "Train Epoch: 388 [100/100 (100%)]\tLoss: 1.461196\n",
            "Train Epoch: 389 [100/100 (100%)]\tLoss: 1.461195\n",
            "Train Epoch: 390 [100/100 (100%)]\tLoss: 1.461195\n",
            "Train Epoch: 391 [100/100 (100%)]\tLoss: 1.461195\n",
            "Train Epoch: 392 [100/100 (100%)]\tLoss: 1.461194\n",
            "Train Epoch: 393 [100/100 (100%)]\tLoss: 1.461194\n",
            "Train Epoch: 394 [100/100 (100%)]\tLoss: 1.461194\n",
            "Train Epoch: 395 [100/100 (100%)]\tLoss: 1.461193\n",
            "Train Epoch: 396 [100/100 (100%)]\tLoss: 1.461193\n",
            "Train Epoch: 397 [100/100 (100%)]\tLoss: 1.461193\n",
            "Train Epoch: 398 [100/100 (100%)]\tLoss: 1.461192\n",
            "Train Epoch: 399 [100/100 (100%)]\tLoss: 1.461192\n",
            "Train Epoch: 400 [100/100 (100%)]\tLoss: 1.461192\n",
            "Train Epoch: 401 [100/100 (100%)]\tLoss: 1.461191\n",
            "Train Epoch: 402 [100/100 (100%)]\tLoss: 1.461191\n",
            "Train Epoch: 403 [100/100 (100%)]\tLoss: 1.461191\n",
            "Train Epoch: 404 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 405 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 406 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 407 [100/100 (100%)]\tLoss: 1.461190\n",
            "Train Epoch: 408 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 409 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 410 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 411 [100/100 (100%)]\tLoss: 1.461189\n",
            "Train Epoch: 412 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 413 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 414 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 415 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 416 [100/100 (100%)]\tLoss: 1.461188\n",
            "Train Epoch: 417 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 418 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 419 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 420 [100/100 (100%)]\tLoss: 1.461187\n",
            "Train Epoch: 421 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 422 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 423 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 424 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 425 [100/100 (100%)]\tLoss: 1.461186\n",
            "Train Epoch: 426 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 427 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 428 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 429 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 430 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 431 [100/100 (100%)]\tLoss: 1.461185\n",
            "Train Epoch: 432 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 433 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 434 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 435 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 436 [100/100 (100%)]\tLoss: 1.461184\n",
            "Train Epoch: 437 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 438 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 439 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 440 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 441 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 442 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 443 [100/100 (100%)]\tLoss: 1.461183\n",
            "Train Epoch: 444 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 445 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 446 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 447 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 448 [100/100 (100%)]\tLoss: 1.461182\n",
            "Train Epoch: 449 [100/100 (100%)]\tLoss: 1.461181\n",
            "Train Epoch: 450 [100/100 (100%)]\tLoss: 1.461181\n",
            "Train Epoch: 451 [100/100 (100%)]\tLoss: 1.461181\n",
            "Train Epoch: 452 [100/100 (100%)]\tLoss: 1.461181\n",
            "Train Epoch: 453 [100/100 (100%)]\tLoss: 1.461181\n",
            "Train Epoch: 454 [100/100 (100%)]\tLoss: 1.461181\n",
            "Train Epoch: 455 [100/100 (100%)]\tLoss: 1.461180\n",
            "Train Epoch: 456 [100/100 (100%)]\tLoss: 1.461180\n",
            "Train Epoch: 457 [100/100 (100%)]\tLoss: 1.461180\n",
            "Train Epoch: 458 [100/100 (100%)]\tLoss: 1.461180\n",
            "Train Epoch: 459 [100/100 (100%)]\tLoss: 1.461180\n",
            "Train Epoch: 460 [100/100 (100%)]\tLoss: 1.461180\n",
            "Train Epoch: 461 [100/100 (100%)]\tLoss: 1.461180\n",
            "Train Epoch: 462 [100/100 (100%)]\tLoss: 1.461179\n",
            "Train Epoch: 463 [100/100 (100%)]\tLoss: 1.461179\n",
            "Train Epoch: 464 [100/100 (100%)]\tLoss: 1.461179\n",
            "Train Epoch: 465 [100/100 (100%)]\tLoss: 1.461179\n",
            "Train Epoch: 466 [100/100 (100%)]\tLoss: 1.461179\n",
            "Train Epoch: 467 [100/100 (100%)]\tLoss: 1.461179\n",
            "Train Epoch: 468 [100/100 (100%)]\tLoss: 1.461179\n",
            "Train Epoch: 469 [100/100 (100%)]\tLoss: 1.461179\n",
            "Train Epoch: 470 [100/100 (100%)]\tLoss: 1.461178\n",
            "Train Epoch: 471 [100/100 (100%)]\tLoss: 1.461178\n",
            "Train Epoch: 472 [100/100 (100%)]\tLoss: 1.461178\n",
            "Train Epoch: 473 [100/100 (100%)]\tLoss: 1.461178\n",
            "Train Epoch: 474 [100/100 (100%)]\tLoss: 1.461178\n",
            "Train Epoch: 475 [100/100 (100%)]\tLoss: 1.461178\n",
            "Train Epoch: 476 [100/100 (100%)]\tLoss: 1.461178\n",
            "Train Epoch: 477 [100/100 (100%)]\tLoss: 1.461178\n",
            "Train Epoch: 478 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 479 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 480 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 481 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 482 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 483 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 484 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 485 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 486 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 487 [100/100 (100%)]\tLoss: 1.461177\n",
            "Train Epoch: 488 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 489 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 490 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 491 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 492 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 493 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 494 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 495 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 496 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 497 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 498 [100/100 (100%)]\tLoss: 1.461176\n",
            "Train Epoch: 499 [100/100 (100%)]\tLoss: 1.461175\n",
            "Train Epoch: 500 [100/100 (100%)]\tLoss: 1.461175\n",
            "Train Epoch: 501 [100/100 (100%)]\tLoss: 1.461175\n",
            "Train Epoch: 502 [100/100 (100%)]\tLoss: 1.461175\n",
            "Train Epoch: 503 [100/100 (100%)]\tLoss: 1.461175\n",
            "Train Epoch: 504 [100/100 (100%)]\tLoss: 1.461175\n",
            "Train Epoch: 505 [100/100 (100%)]\tLoss: 1.461175\n",
            "Train Epoch: 506 [100/100 (100%)]\tLoss: 1.461175\n",
            "Train Epoch: 507 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 508 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 509 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 510 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 511 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 512 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 513 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 514 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 515 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 516 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 517 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 518 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 519 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 520 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 521 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 522 [100/100 (100%)]\tLoss: 1.461174\n",
            "Train Epoch: 523 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 524 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 525 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 526 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 527 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 528 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 529 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 530 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 531 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 532 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 533 [100/100 (100%)]\tLoss: 1.461173\n",
            "Train Epoch: 534 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 535 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 536 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 537 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 538 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 539 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 540 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 541 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 542 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 543 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 544 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 545 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 546 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 547 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 548 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 549 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 550 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 551 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 552 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 553 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 554 [100/100 (100%)]\tLoss: 1.461172\n",
            "Train Epoch: 555 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 556 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 557 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 558 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 559 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 560 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 561 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 562 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 563 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 564 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 565 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 566 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 567 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 568 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 569 [100/100 (100%)]\tLoss: 1.461171\n",
            "Train Epoch: 570 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 571 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 572 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 573 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 574 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 575 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 576 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 577 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 578 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 579 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 580 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 581 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 582 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 583 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 584 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 585 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 586 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 587 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 588 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 589 [100/100 (100%)]\tLoss: 1.461170\n",
            "Train Epoch: 590 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 591 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 592 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 593 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 594 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 595 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 596 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 597 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 598 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 599 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 600 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 601 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 602 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 603 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 604 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 605 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 606 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 607 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 608 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 609 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 610 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 611 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 612 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 613 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 614 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 615 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 616 [100/100 (100%)]\tLoss: 1.461169\n",
            "Train Epoch: 617 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 618 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 619 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 620 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 621 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 622 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 623 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 624 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 625 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 626 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 627 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 628 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 629 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 630 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 631 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 632 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 633 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 634 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 635 [100/100 (100%)]\tLoss: 1.461168\n",
            "Train Epoch: 636 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 637 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 638 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 639 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 640 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 641 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 642 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 643 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 644 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 645 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 646 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 647 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 648 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 649 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 650 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 651 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 652 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 653 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 654 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 655 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 656 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 657 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 658 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 659 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 660 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 661 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 662 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 663 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 664 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 665 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 666 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 667 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 668 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 669 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 670 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 671 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 672 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 673 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 674 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 675 [100/100 (100%)]\tLoss: 1.461167\n",
            "Train Epoch: 676 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 677 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 678 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 679 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 680 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 681 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 682 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 683 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 684 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 685 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 686 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 687 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 688 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 689 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 690 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 691 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 692 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 693 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 694 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 695 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 696 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 697 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 698 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 699 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 700 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 701 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 702 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 703 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 704 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 705 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 706 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 707 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 708 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 709 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 710 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 711 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 712 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 713 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 714 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 715 [100/100 (100%)]\tLoss: 1.461166\n",
            "Train Epoch: 716 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 717 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 718 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 719 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 720 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 721 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 722 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 723 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 724 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 725 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 726 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 727 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 728 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 729 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 730 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 731 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 732 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 733 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 734 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 735 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 736 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 737 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 738 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 739 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 740 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 741 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 742 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 743 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 744 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 745 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 746 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 747 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 748 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 749 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 750 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 751 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 752 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 753 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 754 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 755 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 756 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 757 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 758 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 759 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 760 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 761 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 762 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 763 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 764 [100/100 (100%)]\tLoss: 1.461165\n",
            "Train Epoch: 765 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 766 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 767 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 768 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 769 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 770 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 771 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 772 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 773 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 774 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 775 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 776 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 777 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 778 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 779 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 780 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 781 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 782 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 783 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 784 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 785 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 786 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 787 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 788 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 789 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 790 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 791 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 792 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 793 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 794 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 795 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 796 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 797 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 798 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 799 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 800 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 801 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 802 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 803 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 804 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 805 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 806 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 807 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 808 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 809 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 810 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 811 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 812 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 813 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 814 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 815 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 816 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 817 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 818 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 819 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 820 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 821 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 822 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 823 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 824 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 825 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 826 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 827 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 828 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 829 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 830 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 831 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 832 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 833 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 834 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 835 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 836 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 837 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 838 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 839 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 840 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 841 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 842 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 843 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 844 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 845 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 846 [100/100 (100%)]\tLoss: 1.461164\n",
            "Train Epoch: 847 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 848 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 849 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 850 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 851 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 852 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 853 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 854 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 855 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 856 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 857 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 858 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 859 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 860 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 861 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 862 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 863 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 864 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 865 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 866 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 867 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 868 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 869 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 870 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 871 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 872 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 873 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 874 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 875 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 876 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 877 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 878 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 879 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 880 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 881 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 882 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 883 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 884 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 885 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 886 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 887 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 888 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 889 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 890 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 891 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 892 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 893 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 894 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 895 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 896 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 897 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 898 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 899 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 900 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 901 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 902 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 903 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 904 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 905 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 906 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 907 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 908 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 909 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 910 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 911 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 912 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 913 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 914 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 915 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 916 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 917 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 918 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 919 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 920 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 921 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 922 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 923 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 924 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 925 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 926 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 927 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 928 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 929 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 930 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 931 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 932 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 933 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 934 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 935 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 936 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 937 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 938 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 939 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 940 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 941 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 942 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 943 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 944 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 945 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 946 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 947 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 948 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 949 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 950 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 951 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 952 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 953 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 954 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 955 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 956 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 957 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 958 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 959 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 960 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 961 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 962 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 963 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 964 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 965 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 966 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 967 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 968 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 969 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 970 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 971 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 972 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 973 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 974 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 975 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 976 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 977 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 978 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 979 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 980 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 981 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 982 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 983 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 984 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 985 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 986 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 987 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 988 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 989 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 990 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 991 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 992 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 993 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 994 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 995 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 996 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 997 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 998 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 999 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1000 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1001 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1002 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1003 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1004 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1005 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1006 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1007 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1008 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1009 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1010 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1011 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1012 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1013 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1014 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1015 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1016 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1017 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1018 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1019 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1020 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1021 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1022 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1023 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1024 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1025 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1026 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1027 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1028 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1029 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1030 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1031 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1032 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1033 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1034 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1035 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1036 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1037 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1038 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1039 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1040 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1041 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1042 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1043 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1044 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1045 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1046 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1047 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1048 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1049 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1050 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1051 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1052 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1053 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1054 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1055 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1056 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1057 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1058 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1059 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1060 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1061 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1062 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1063 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1064 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1065 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1066 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1067 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1068 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1069 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1070 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1071 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1072 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1073 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1074 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1075 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1076 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1077 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1078 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1079 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1080 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1081 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1082 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1083 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1084 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1085 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1086 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1087 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1088 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1089 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1090 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1091 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1092 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1093 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1094 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1095 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1096 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1097 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1098 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1099 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1100 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1101 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1102 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1103 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1104 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1105 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1106 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1107 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1108 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1109 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1110 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1111 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1112 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1113 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1114 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1115 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1116 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1117 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1118 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1119 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1120 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1121 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1122 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1123 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1124 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1125 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1126 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1127 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1128 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1129 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1130 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1131 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1132 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1133 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1134 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1135 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1136 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1137 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1138 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1139 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1140 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1141 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1142 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1143 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1144 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1145 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1146 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1147 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1148 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1149 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1150 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1151 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1152 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1153 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1154 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1155 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1156 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1157 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1158 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1159 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1160 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1161 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1162 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1163 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1164 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1165 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1166 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1167 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1168 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1169 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1170 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1171 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1172 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1173 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1174 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1175 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1176 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1177 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1178 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1179 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1180 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1181 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1182 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1183 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1184 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1185 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1186 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1187 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1188 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1189 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1190 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1191 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1192 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1193 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1194 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1195 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1196 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1197 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1198 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1199 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1200 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1201 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1202 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1203 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1204 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1205 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1206 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1207 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1208 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1209 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1210 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1211 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1212 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1213 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1214 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1215 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1216 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1217 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1218 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1219 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1220 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1221 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1222 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1223 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1224 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1225 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1226 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1227 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1228 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1229 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1230 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1231 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1232 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1233 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1234 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1235 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1236 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1237 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1238 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1239 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1240 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1241 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1242 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1243 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1244 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1245 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1246 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1247 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1248 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1249 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1250 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1251 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1252 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1253 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1254 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1255 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1256 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1257 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1258 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1259 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1260 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1261 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1262 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1263 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1264 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1265 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1266 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1267 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1268 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1269 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1270 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1271 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1272 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1273 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1274 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1275 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1276 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1277 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1278 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1279 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1280 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1281 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1282 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1283 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1284 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1285 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1286 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1287 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1288 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1289 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1290 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1291 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1292 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1293 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1294 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1295 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1296 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1297 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1298 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1299 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1300 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1301 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1302 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1303 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1304 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1305 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1306 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1307 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1308 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1309 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1310 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1311 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1312 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1313 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1314 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1315 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1316 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1317 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1318 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1319 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1320 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1321 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1322 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1323 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1324 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1325 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1326 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1327 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1328 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1329 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1330 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1331 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1332 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1333 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1334 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1335 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1336 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1337 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1338 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1339 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1340 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1341 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1342 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1343 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1344 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1345 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1346 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1347 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1348 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1349 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1350 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1351 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1352 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1353 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1354 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1355 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1356 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1357 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1358 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1359 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1360 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1361 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1362 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1363 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1364 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1365 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1366 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1367 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1368 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1369 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1370 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1371 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1372 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1373 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1374 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1375 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1376 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1377 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1378 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1379 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1380 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1381 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1382 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1383 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1384 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1385 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1386 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1387 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1388 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1389 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1390 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1391 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1392 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1393 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1394 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1395 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1396 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1397 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1398 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1399 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1400 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1401 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1402 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1403 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1404 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1405 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1406 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1407 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1408 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1409 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1410 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1411 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1412 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1413 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1414 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1415 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1416 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1417 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1418 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1419 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1420 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1421 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1422 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1423 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1424 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1425 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1426 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1427 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1428 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1429 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1430 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1431 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1432 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1433 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1434 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1435 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1436 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1437 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1438 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1439 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1440 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1441 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1442 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1443 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1444 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1445 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1446 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1447 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1448 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1449 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1450 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1451 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1452 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1453 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1454 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1455 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1456 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1457 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1458 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1459 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1460 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1461 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1462 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1463 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1464 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1465 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1466 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1467 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1468 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1469 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1470 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1471 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1472 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1473 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1474 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1475 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1476 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1477 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1478 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1479 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1480 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1481 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1482 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1483 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1484 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1485 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1486 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1487 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1488 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1489 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1490 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1491 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1492 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1493 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1494 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1495 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1496 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1497 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1498 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1499 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1500 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1501 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1502 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1503 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1504 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1505 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1506 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1507 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1508 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1509 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1510 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1511 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1512 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1513 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1514 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1515 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1516 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1517 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1518 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1519 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1520 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1521 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1522 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1523 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1524 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1525 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1526 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1527 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1528 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1529 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1530 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1531 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1532 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1533 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1534 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1535 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1536 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1537 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1538 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1539 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1540 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1541 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1542 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1543 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1544 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1545 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1546 [100/100 (100%)]\tLoss: 1.461161\n",
            "Train Epoch: 1547 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1548 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1549 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1550 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1551 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1552 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1553 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1554 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1555 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1556 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1557 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1558 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1559 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1560 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1561 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1562 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1563 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1564 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1565 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1566 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1567 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1568 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1569 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1570 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1571 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1572 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1573 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1574 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1575 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1576 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1577 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1578 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1579 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1580 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1581 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1582 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1583 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1584 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1585 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1586 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1587 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1588 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1589 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1590 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1591 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1592 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1593 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1594 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1595 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1596 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1597 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1598 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1599 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1600 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1601 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1602 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1603 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1604 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1605 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1606 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1607 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1608 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1609 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1610 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1611 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1612 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1613 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1614 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1615 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1616 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1617 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1618 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1619 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1620 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1621 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1622 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1623 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1624 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1625 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1626 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1627 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1628 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1629 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1630 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1631 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1632 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1633 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1634 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1635 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1636 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1637 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1638 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1639 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1640 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1641 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1642 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1643 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1644 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1645 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1646 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1647 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1648 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1649 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1650 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1651 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1652 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1653 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1654 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1655 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1656 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1657 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1658 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1659 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1660 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1661 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1662 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1663 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1664 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1665 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1666 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1667 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1668 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1669 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1670 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1671 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1672 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1673 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1674 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1675 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1676 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1677 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1678 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1679 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1680 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1681 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1682 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1683 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1684 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1685 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1686 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1687 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1688 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1689 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1690 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1691 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1692 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1693 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1694 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1695 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1696 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1697 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1698 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1699 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1700 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1701 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1702 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1703 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1704 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1705 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1706 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1707 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1708 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1709 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1710 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1711 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1712 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1713 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1714 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1715 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1716 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1717 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1718 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1719 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1720 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1721 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1722 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1723 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1724 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1725 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1726 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1727 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1728 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1729 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1730 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1731 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1732 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1733 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1734 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1735 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1736 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1737 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1738 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1739 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1740 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1741 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1742 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1743 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1744 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1745 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1746 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1747 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1748 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1749 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1750 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1751 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1752 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1753 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1754 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1755 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1756 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1757 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1758 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1759 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1760 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1761 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1762 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1763 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1764 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1765 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1766 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1767 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1768 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1769 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1770 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1771 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1772 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1773 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1774 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1775 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1776 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1777 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1778 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1779 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1780 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1781 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1782 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1783 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1784 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1785 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1786 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1787 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1788 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1789 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1790 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1791 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1792 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1793 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1794 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1795 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1796 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1797 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1798 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1799 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1800 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1801 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1802 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1803 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1804 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1805 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1806 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1807 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1808 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1809 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1810 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1811 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1812 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1813 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1814 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1815 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1816 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1817 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1818 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1819 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1820 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1821 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1822 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1823 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1824 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1825 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1826 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1827 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1828 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1829 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1830 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1831 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1832 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1833 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1834 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1835 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1836 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1837 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1838 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1839 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1840 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1841 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1842 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1843 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1844 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1845 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1846 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1847 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1848 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1849 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1850 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1851 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1852 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1853 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1854 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1855 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1856 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1857 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1858 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1859 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1860 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1861 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1862 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1863 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1864 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1865 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1866 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1867 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1868 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1869 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1870 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1871 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1872 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1873 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1874 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1875 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1876 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1877 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1878 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1879 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1880 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1881 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1882 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1883 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1884 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1885 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1886 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1887 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1888 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1889 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1890 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1891 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1892 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1893 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1894 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1895 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1896 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1897 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1898 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1899 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1900 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1901 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1902 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1903 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1904 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1905 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1906 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1907 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1908 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1909 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1910 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1911 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1912 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1913 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1914 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1915 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1916 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1917 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1918 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1919 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1920 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1921 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1922 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1923 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1924 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1925 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1926 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1927 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1928 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1929 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1930 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1931 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1932 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1933 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1934 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1935 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1936 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1937 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1938 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1939 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1940 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1941 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1942 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1943 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1944 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1945 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1946 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1947 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1948 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1949 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1950 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1951 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1952 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1953 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1954 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1955 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1956 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1957 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1958 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1959 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1960 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1961 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1962 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1963 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1964 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1965 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1966 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1967 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1968 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1969 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1970 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1971 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1972 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1973 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1974 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1975 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1976 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1977 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1978 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1979 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1980 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1981 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1982 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1983 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1984 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1985 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1986 [100/100 (100%)]\tLoss: 1.461163\n",
            "Train Epoch: 1987 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1988 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1989 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1990 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1991 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1992 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1993 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1994 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1995 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1996 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1997 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1998 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 1999 [100/100 (100%)]\tLoss: 1.461162\n",
            "Train Epoch: 2000 [100/100 (100%)]\tLoss: 1.461162\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GcAB3asNGX63",
        "colab_type": "text"
      },
      "source": [
        "Now let's **test** it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJ7qr1PmGbdf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_predictions(predicted_batch, label_batch):\n",
        "  pred = predicted_batch.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
        "  acum = pred.eq(label_batch.view_as(pred)).sum().item()\n",
        "  return acum\n",
        "\n",
        "def test_model2(network, test_loader, criterion):\n",
        "  network.eval()\n",
        "  test_loss_avg = 0\n",
        "  num_batches = 0\n",
        "  acc = 0\n",
        "  for data, target in test_loader:\n",
        "      with torch.no_grad():\n",
        "          data = data.to(device)\n",
        "          target = target.to(device)\n",
        "\n",
        "          output = network.encoder(data)\n",
        "          output = output.view(output.size(0), -1)\n",
        "          output = network.linear1(output)\n",
        "          output = network.softmax(output)\n",
        "\n",
        "          acc += correct_predictions(output, target)\n",
        "\n",
        "          num_batches += 1  \n",
        "  test_acc = 100. * acc / len(test_loader.dataset)\n",
        "  print('Test Acc: %f' % (test_acc))\n",
        "  return test_acc"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjgFXAFNJR3g",
        "colab_type": "text"
      },
      "source": [
        "Test the 3 different models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlhajaDIIkSy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "1f02108d-1dd2-4ac7-dffb-10de4bf4096c"
      },
      "source": [
        "test_classifier1 = test_model2(ex2model1, test_loader, ex2criterion)\n",
        "test_classifier2 = test_model2(ex2model2, test_loader, ex2criterion)\n",
        "test_classifier3 = test_model2(ex2model3, test_loader, ex2criterion)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Test Acc: 66.870000\n",
            "Test Acc: 83.510000\n",
            "Test Acc: 78.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJ6shf9KJUJ8",
        "colab_type": "text"
      },
      "source": [
        "Let's sumarize the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T6BR261QJXNE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f1952601-0b86-45c5-9a1a-1e4946f5e400"
      },
      "source": [
        "print(tabulate([\n",
        "                ['Pre-training', test_classifier1], \n",
        "                ['Fine-tunning', test_classifier2], \n",
        "                ['From scratch', test_classifier3]\n",
        "                ], \n",
        "               headers=['Classifier', 'Acc']\n",
        "               )\n",
        ")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier      Acc\n",
            "------------  -----\n",
            "Pre-training  66.87\n",
            "Fine-tunning  83.51\n",
            "From scratch  78\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}