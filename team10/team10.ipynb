{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Quesiquesi.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jey73qP2B7OX","colab_type":"text"},"source":["## Exercise 1 Autoencoder\n","\n","# What the exercise asked for:\n","Train a convolutional autoencoder on MNIST, study the influence of the bottleneck size and generate some images.\n","\n","#The steps to be followed:\n","1. Load MNIST train and test sets. Split the original training data into 95% training and 5% validation data.\n","2. Implement a convolutional autoencoder (with separate Encoder and Decoder modules).\n","3. Train the convolutional autoencoder, with different bottleneck sizes. Plot the train and validation loss curves of all autoencoders in the same figure.\n","4. Compute the avg. image reconstruction error (MSE) of the trained models on the MNIST validation and test sets. Show the results in a table, including #params of each model.\n","5. Select one of the autoencoders and feed it 5 random MNIST images from the test set. Show them along with their reconstructions.\n","6. Generate 5 new images by injecting random values as input to the decoder. Show them."]},{"cell_type":"code","metadata":{"id":"oXVK4gP5B3Vq","colab_type":"code","colab":{}},"source":["import torch\n","from torchvision import datasets, transforms\n","import torch.optim as optim\n","import numpy as np\n","from torch.utils.data.sampler import SubsetRandomSampler\n","\n","import torch.nn as nn\n","import torch.nn.functional as F"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"38qpXQsyCDc7","colab_type":"code","colab":{}},"source":["hparams = {\n","    'batch_size':20, #estava a 64\n","    'num_epochs':5,\n","    'test_batch_size':64,\n","    'hidden_size':128,\n","    'num_classes':10,\n","    'num_inputs':784, # això és el numero de píxels per imatge (28x28)\n","    'learning_rate':1e-3,\n","    'log_interval':100,\n","}\n","\n","hparams['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h-fyQqcvCGuY","colab_type":"code","colab":{}},"source":["mnist_trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.Compose([\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize((0.1307,), (0.3081,))\n","                                ]))\n","mnist_testset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.Compose([\n","                                    transforms.ToTensor(),\n","                                    transforms.Normalize((0.1307,), (0.3081,))\n","                                ]))\n","\n","validation_split = 0.05\n","random_seed= 30\n","dataset_size = len(mnist_trainset)\n","indices = list(range(dataset_size))\n","split = int(np.floor(validation_split * dataset_size))\n","\n","# Shuffle dataset\n","np.random.seed(random_seed)\n","np.random.shuffle(indices)\n","\n","# Get samples indices\n","train_indices, val_indices = indices[split:], indices[:split]\n","\n","# Creating PT data samplers\n","train_sampler = SubsetRandomSampler(train_indices)\n","valid_sampler = SubsetRandomSampler(val_indices)\n","\n","train_loader = torch.utils.data.DataLoader(mnist_trainset,\n","                                           batch_size=hparams['batch_size'], \n","                                           sampler=train_sampler)\n","validation_loader = torch.utils.data.DataLoader(mnist_trainset,\n","                                                batch_size=hparams['batch_size'],\n","                                                sampler=valid_sampler)\n","test_loader = torch.utils.data.DataLoader(mnist_testset,\n","                                          batch_size=hparams['batch_size'],\n","                                          shuffle=False)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"d33anaaXCYOS","colab_type":"code","colab":{}},"source":["class ConvEncoder(nn.Module):\n","  def __init__(self,bottleneck):\n","    super().__init__()\n","    '''\n","    self.encoder = nn.Sequential( # entrada: imatge 28x28\n","        nn.Conv2d(1,32,4, stride=2, padding=1) # capa conv: 32 filtres 4x4 amb stride=2 i padding=1, la sortida és 32x14x14\n","        nn.ReLU()\n","        nn.Conv2d(32,64,4,stride=2, padding=1) # capa conv: 64 filtres 4x4 amb stride=2 i padding=1, la sortida és 64x7x7\n","        nn.ReLU()\n","        nn.Conv2d(64,128,5,stride=2, padding=1) # capa conv: 128 filtres 5x5 amb stride=2 i padding=1, la sortida és 128x3x3\n","        nn.ReLU()\n","    )\n","    '''\n","    self.encoder = nn.Sequential( # entrada: imatge 28x28\n","        nn.Conv2d(1,32,2, stride=2), # capa conv: 32 filtres 2x2 amb stride=2 i padding=0, la sortida és 32x14x14\n","        nn.ReLU(),\n","        nn.Conv2d(32,64,2,stride=2), # capa conv: 64 filtres 2x2 amb stride=2 i padding=0, la sortida és 64x7x7\n","        nn.ReLU(),\n","        nn.Conv2d(64,128,3,stride=2), # capa conv: 128 filtres 3x3 amb stride=2 i padding=0, la sortida és 128x3x3\n","        nn.ReLU()\n","    )\n","    self.mlp = nn.Sequential(\n","        nn.Linear(1152,bottleneck), #128x3x3\n","        nn.ReLU()\n","    )\n","\n","  def forward(self, x):\n","    x = self.encoder(x)\n","    x= x.view(x.shape[0], -1)\n","    x = self.mlp(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JvmDOtzTDOTs","colab_type":"code","colab":{}},"source":["class ConvDecoder(nn.Module):\n","  def __init__(self,bottleneck):\n","    super().__init__()\n","    self.mlp = nn.Sequential(\n","        nn.Linear(bottleneck,1152),\n","        nn.ReLU()\n","    )\n","    self.decoder = nn.Sequential(\n","        nn.ConvTranspose2d(128,64,3, stride=2),\n","        nn.ReLU(),\n","        nn.ConvTranspose2d(64,32,2, stride=2),\n","        nn.ReLU(),\n","        nn.ConvTranspose2d(32,1,2, stride=2),\n","        nn.ReLU()\n","    )\n","    \n","  def forward(self, x):\n","    x = self.mlp(x)\n","    x = x.view(x.shape[0], 128, 3, 3)\n","    x = self.decoder(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R6hIBDsGDQ_-","colab_type":"code","colab":{}},"source":["class ConvAutoencoder(nn.Module):\n","  def __init__(self, bottleneck):\n","    super().__init__()\n","    self.encoder = ConvEncoder(bottleneck)\n","    self.decoder = ConvDecoder(bottleneck)\n","\n","  def forward(self, x):\n","    x = self.encoder(x)\n","    x = self.decoder(x)\n","    return x"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"hUGP2ak1IZm-","colab_type":"code","colab":{}},"source":["from torch.autograd import Variable\n","\n","def train_epoch(train_loader, network, optimizer, criterion, hparams):\n","  # Activate the train=True flag inside the model\n","  #network.train()\n","  device = hparams['device']\n","  for epoch in range(hparams['num_epochs']):\n","    avg_loss=0.0\n","    num=0\n","    for data in train_loader: #lo de enumerate fa que peti\n","      img, _ = data\n","      img= img.to(device)\n","      output = network(img)\n","      loss = criterion(output, img)\n","      optimizer.zero_grad()\n","      loss.backward()\n","      optimizer.step()\n","      avg_loss += loss.item()\n","      num+=1\n","      if num % hparams['log_interval'] == 0:\n","        print('epoch [{}/{}], [{}/{} ({:.0f}%)]\\tloss:{:.4f}'.format(epoch+1, hparams['num_epochs'], num * len(data), len(train_loader.dataset),\n","              100. * num / len(train_loader), loss.item()))\n","    avg_loss = avg_loss/len(train_loader)\n","    print('Epoch: {} \\t Average Training Loss: {:.6f}'.format(epoch+1, avg_loss))\n","  return avg_loss\n","\n","\n","def test_epoch(test_loader, network, criterion, hparams):\n","    #network.eval()\n","    device = hparams['device']\n","    test_loss = 0\n","    acc = 0\n","    with torch.no_grad():\n","        for data in test_loader:\n","            img, _ = data\n","            img= img.to(device)\n","            output = network(img)\n","            test_loss += criterion(output, img, reduction='sum').item() # sum up batch loss\n","            # compute number of correct predictions in the batch\n","            #acc += correct_predictions(output, data)\n","    # Average acc across all correct predictions batches now\n","    test_loss /= len(test_loader.dataset)\n","    #test_acc = 100. * acc / len(test_loader.dataset)\n","    print('\\nTest set: Average loss: {:.4f} \\n'.format(test_loss))\n","    return test_loss"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Taj0OIBdIfOz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"6b188161-1391-4a24-aeac-e5cdd89c5330"},"source":["# Init lists to save the evolution of the training & test losses/accuracy.\n","train_losses = []\n","test_losses = []\n","test_accs = []\n","bottleneck=100\n","network = ConvAutoencoder(bottleneck)\n","network.to(hparams['device'])\n","optimizer = optim.Adam(network.parameters(),\n","                       lr=hparams['learning_rate'])\n","criterion = F.mse_loss\n","\n","for epoch in range(1, hparams['num_epochs'] + 1):\n","  train_losses.append(train_epoch(train_loader, network, optimizer, criterion, hparams))\n","  test_loss=test_epoch(validation_loader, network, criterion, hparams)\n","  test_losses.append(test_loss)\n","  #test_accs.append(test_accuracy)\n","\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["epoch [1/5], [200/60000 (4%)]\tloss:1.1174\n","epoch [1/5], [400/60000 (7%)]\tloss:0.9855\n","epoch [1/5], [600/60000 (11%)]\tloss:0.9843\n","epoch [1/5], [800/60000 (14%)]\tloss:1.0280\n","epoch [1/5], [1000/60000 (18%)]\tloss:1.2076\n","epoch [1/5], [1200/60000 (21%)]\tloss:0.9986\n","epoch [1/5], [1400/60000 (25%)]\tloss:0.9316\n","epoch [1/5], [1600/60000 (28%)]\tloss:0.9819\n","epoch [1/5], [1800/60000 (32%)]\tloss:0.9364\n","epoch [1/5], [2000/60000 (35%)]\tloss:0.7947\n","epoch [1/5], [2200/60000 (39%)]\tloss:0.9938\n","epoch [1/5], [2400/60000 (42%)]\tloss:0.9591\n","epoch [1/5], [2600/60000 (46%)]\tloss:1.0177\n","epoch [1/5], [2800/60000 (49%)]\tloss:0.9614\n","epoch [1/5], [3000/60000 (53%)]\tloss:0.9428\n","epoch [1/5], [3200/60000 (56%)]\tloss:0.9337\n","epoch [1/5], [3400/60000 (60%)]\tloss:0.9880\n","epoch [1/5], [3600/60000 (63%)]\tloss:0.9787\n","epoch [1/5], [3800/60000 (67%)]\tloss:0.9233\n","epoch [1/5], [4000/60000 (70%)]\tloss:0.9962\n","epoch [1/5], [4200/60000 (74%)]\tloss:1.0539\n","epoch [1/5], [4400/60000 (77%)]\tloss:0.9738\n","epoch [1/5], [4600/60000 (81%)]\tloss:0.9975\n","epoch [1/5], [4800/60000 (84%)]\tloss:1.0008\n","epoch [1/5], [5000/60000 (88%)]\tloss:1.0145\n","epoch [1/5], [5200/60000 (91%)]\tloss:1.0338\n","epoch [1/5], [5400/60000 (95%)]\tloss:1.0377\n","epoch [1/5], [5600/60000 (98%)]\tloss:0.9344\n","Epoch: 1 \t Average Training Loss: 0.999882\n","epoch [2/5], [200/60000 (4%)]\tloss:1.0131\n","epoch [2/5], [400/60000 (7%)]\tloss:0.9483\n","epoch [2/5], [600/60000 (11%)]\tloss:1.1174\n","epoch [2/5], [800/60000 (14%)]\tloss:0.9561\n","epoch [2/5], [1000/60000 (18%)]\tloss:0.9199\n","epoch [2/5], [1200/60000 (21%)]\tloss:1.0315\n","epoch [2/5], [1400/60000 (25%)]\tloss:1.0064\n","epoch [2/5], [1600/60000 (28%)]\tloss:0.9363\n","epoch [2/5], [1800/60000 (32%)]\tloss:1.0480\n","epoch [2/5], [2000/60000 (35%)]\tloss:0.9055\n","epoch [2/5], [2200/60000 (39%)]\tloss:0.9453\n","epoch [2/5], [2400/60000 (42%)]\tloss:1.0593\n","epoch [2/5], [2600/60000 (46%)]\tloss:0.9402\n","epoch [2/5], [2800/60000 (49%)]\tloss:0.8698\n","epoch [2/5], [3000/60000 (53%)]\tloss:0.8853\n","epoch [2/5], [3200/60000 (56%)]\tloss:1.1505\n","epoch [2/5], [3400/60000 (60%)]\tloss:1.0241\n","epoch [2/5], [3600/60000 (63%)]\tloss:0.9236\n","epoch [2/5], [3800/60000 (67%)]\tloss:1.0309\n","epoch [2/5], [4000/60000 (70%)]\tloss:0.9675\n","epoch [2/5], [4200/60000 (74%)]\tloss:0.9809\n","epoch [2/5], [4400/60000 (77%)]\tloss:1.0577\n","epoch [2/5], [4600/60000 (81%)]\tloss:1.1018\n","epoch [2/5], [4800/60000 (84%)]\tloss:0.9403\n","epoch [2/5], [5000/60000 (88%)]\tloss:1.1222\n","epoch [2/5], [5200/60000 (91%)]\tloss:0.9631\n","epoch [2/5], [5400/60000 (95%)]\tloss:1.1155\n","epoch [2/5], [5600/60000 (98%)]\tloss:0.9290\n","Epoch: 2 \t Average Training Loss: 0.999882\n","epoch [3/5], [200/60000 (4%)]\tloss:0.9920\n","epoch [3/5], [400/60000 (7%)]\tloss:0.9275\n","epoch [3/5], [600/60000 (11%)]\tloss:0.9578\n","epoch [3/5], [800/60000 (14%)]\tloss:1.0338\n","epoch [3/5], [1000/60000 (18%)]\tloss:1.2164\n","epoch [3/5], [1200/60000 (21%)]\tloss:0.9066\n","epoch [3/5], [1400/60000 (25%)]\tloss:0.8594\n","epoch [3/5], [1600/60000 (28%)]\tloss:0.9173\n","epoch [3/5], [1800/60000 (32%)]\tloss:1.0728\n","epoch [3/5], [2000/60000 (35%)]\tloss:0.9401\n","epoch [3/5], [2200/60000 (39%)]\tloss:0.8933\n","epoch [3/5], [2400/60000 (42%)]\tloss:0.9267\n","epoch [3/5], [2600/60000 (46%)]\tloss:1.0643\n","epoch [3/5], [2800/60000 (49%)]\tloss:0.9059\n","epoch [3/5], [3000/60000 (53%)]\tloss:1.0991\n","epoch [3/5], [3200/60000 (56%)]\tloss:0.9706\n","epoch [3/5], [3400/60000 (60%)]\tloss:0.9550\n","epoch [3/5], [3600/60000 (63%)]\tloss:1.1176\n","epoch [3/5], [3800/60000 (67%)]\tloss:1.0501\n","epoch [3/5], [4000/60000 (70%)]\tloss:0.9758\n","epoch [3/5], [4200/60000 (74%)]\tloss:0.9963\n","epoch [3/5], [4400/60000 (77%)]\tloss:1.1307\n","epoch [3/5], [4600/60000 (81%)]\tloss:0.9804\n","epoch [3/5], [4800/60000 (84%)]\tloss:1.0795\n","epoch [3/5], [5000/60000 (88%)]\tloss:1.1128\n","epoch [3/5], [5200/60000 (91%)]\tloss:0.8907\n","epoch [3/5], [5400/60000 (95%)]\tloss:1.0631\n","epoch [3/5], [5600/60000 (98%)]\tloss:1.0739\n","Epoch: 3 \t Average Training Loss: 0.999882\n","epoch [4/5], [200/60000 (4%)]\tloss:1.0725\n","epoch [4/5], [400/60000 (7%)]\tloss:0.9477\n","epoch [4/5], [600/60000 (11%)]\tloss:1.1689\n","epoch [4/5], [800/60000 (14%)]\tloss:1.0504\n","epoch [4/5], [1000/60000 (18%)]\tloss:1.0294\n","epoch [4/5], [1200/60000 (21%)]\tloss:1.0921\n","epoch [4/5], [1400/60000 (25%)]\tloss:0.8791\n","epoch [4/5], [1600/60000 (28%)]\tloss:0.9754\n","epoch [4/5], [1800/60000 (32%)]\tloss:1.0124\n","epoch [4/5], [2000/60000 (35%)]\tloss:1.0122\n","epoch [4/5], [2200/60000 (39%)]\tloss:1.0096\n","epoch [4/5], [2400/60000 (42%)]\tloss:0.9427\n","epoch [4/5], [2600/60000 (46%)]\tloss:1.0634\n","epoch [4/5], [2800/60000 (49%)]\tloss:1.0382\n","epoch [4/5], [3000/60000 (53%)]\tloss:0.9560\n","epoch [4/5], [3200/60000 (56%)]\tloss:0.9725\n","epoch [4/5], [3400/60000 (60%)]\tloss:0.8694\n","epoch [4/5], [3600/60000 (63%)]\tloss:0.9771\n","epoch [4/5], [3800/60000 (67%)]\tloss:0.8720\n","epoch [4/5], [4000/60000 (70%)]\tloss:1.0138\n","epoch [4/5], [4200/60000 (74%)]\tloss:0.9467\n","epoch [4/5], [4400/60000 (77%)]\tloss:1.0027\n","epoch [4/5], [4600/60000 (81%)]\tloss:1.0274\n","epoch [4/5], [4800/60000 (84%)]\tloss:1.0579\n","epoch [4/5], [5000/60000 (88%)]\tloss:1.0498\n","epoch [4/5], [5200/60000 (91%)]\tloss:1.0466\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Wy-VMMMNI6J_","colab_type":"code","colab":{}},"source":["plt.figure(figsize=(10, 8))\n","plt.xlabel('Epoch')\n","plt.ylabel('MSE Loss')\n","plt.plot(tr_losses, label='train')\n","plt.plot(te_losses, label='test')\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"EFWomiK5eC4-","colab_type":"code","colab":{}},"source":["#plot the reconstructed images"],"execution_count":0,"outputs":[]}]}