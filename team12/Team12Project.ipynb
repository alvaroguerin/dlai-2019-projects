{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Team12Project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tdo7-TRvj2C",
        "colab_type": "text"
      },
      "source": [
        "Ex 1: Convolutional Autoencoder\n",
        "\n",
        "Idea: train a convolutional autoencoder on MNIST, study the influence of\n",
        "the bottleneck size and generate some images\n",
        "\n",
        "Steps:\n",
        "1. Load MNIST train and test sets. Split the original training data into 95% training and 5%\n",
        "validation data.\n",
        "2. Implement a convolutional autoencoder (with separate Encoder and Decoder modules).\n",
        "3. Train the convolutional autoencoder, with different bottleneck sizes. Plot the train and\n",
        "validation loss curves of all autoencoders in the same figure.\n",
        "4.\n",
        "Compute the avg. image reconstruction error (MSE) of the trained models on the MNIST\n",
        "validation and test sets. Show the results in a table, including #params of each model.\n",
        "5.\n",
        "Select one of the autoencoders and feed it 5 random MNIST images from the test set.\n",
        "Show them along with their reconstructions.\n",
        "6.\n",
        "Generate 5 new images by injecting random values as input to the decoder. Show them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxvPg_ArpMXR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "np.random.seed(123)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4I2MPdAAvs3c",
        "colab_type": "text"
      },
      "source": [
        "Ex 2: Transfer Learning\n",
        "\n",
        "Idea: reuse autoencoder weights for pre-training and fine-tuning of a\n",
        "classifier trained on a small labeled subset of MNIST.\n",
        "\n",
        "Steps:\n",
        "1. Select a subset of 100 images and their associated labels from the MNIST training data.\n",
        "2. Select one of the previously trained autoencoders.\n",
        "3. Create a digit (0-9) classification model reusing the encoder of the autoencoder and adding\n",
        "the needed fully connected (projection) layer.\n",
        "4. Pre-training: use the weights of the autoencoder as initial values for the network weights and\n",
        "train a classification model on the subset of 100 samples.\n",
        "5. Fine-tuning: do the same, but train the new projection layer with a normal learning rate and\n",
        "the reused part with a very low learning rate.\n",
        "6. From scratch: train the model on the 100 samples without reusing the decoder weights at all.\n",
        "7. Show the accuracy of the four models on the MNIST test set in a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AoV5kf4Pvta2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XHZBlmV7vtrf",
        "colab_type": "text"
      },
      "source": [
        "Ex 3\n",
        "\n",
        "Optional Exercise A: Variational Autoencoder\n",
        "\n",
        "Idea: turn the autoencoder from Exercise 1 into a variational autoencoder.\n",
        "\n",
        "Steps:\n",
        "1. Implement an autoencoder like that from Exercise 1, but turning the deterministic\n",
        "bottleneck into a stochastic bottleneck, with an isotropic Gaussian as distribution for the\n",
        "latent variables.\n",
        "2. Train the model optimizing the Evidence Lower Bound (ELBO).\n",
        "3. Generate samples with the decoder and show them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GY8byT7cvuIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYZLCEjvvvUC",
        "colab_type": "text"
      },
      "source": [
        "Ex 4\n",
        "\n",
        "Optional Exercise B: Conditional GAN\n",
        "\n",
        "Idea: train a condition la Generative Adversarial Network (GAN) to generate digit images based on the given number.\n",
        "Note: you can use as starting point the lab about GANs.\n",
        "\n",
        "Steps:\n",
        "1. Implement a generator and discriminator based con deconvolutions and convolutions\n",
        "respectively.\n",
        "2. Train the GAN on MNIST.\n",
        "3. Generate samples with the generator and show them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9pqXKsGvvmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}