{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "team14.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8mBmXEQGbpM",
        "colab_type": "text"
      },
      "source": [
        "# **DLAI PROJECT - 2019**\n",
        "---\n",
        "##**Elisabet Bayó, Guillem Bonafonte, Witold Drozdzowski, Jady Ramanandray**\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS0qIx3o3kNW",
        "colab_type": "text"
      },
      "source": [
        "#**Preparations of the environment**#\n",
        "\n",
        "We import all the necessary libraries to use and train the neural networks and to visualize the data.\n",
        "Just comment the `drive.mount('/content/gdrive')` line if you don't plan to save/load models from your google drive.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t9G3sIlO3yYi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "e538b5cf-54db-438a-de70-4ab464279eb5"
      },
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "import copy\n",
        "from torch.autograd import Variable\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import itertools\n",
        "from numpy import array\n",
        "\n",
        "from prettytable import PrettyTable\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_9R11LbypQE",
        "colab_type": "text"
      },
      "source": [
        "We define hyperparameters to use in the models as well as saving them. We also define to work on GPU when possible"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTa8DpnP3cca",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams = {\n",
        "    'batch_size':100,\n",
        "    'num_epochs':5,\n",
        "    'learning_rate':1e-3,\n",
        "    'log_interval':100,\n",
        "    'train_percentage':0.95,\n",
        "    'autoencoder_criterion':nn.MSELoss(),\n",
        "    'model_path':F\"/content/gdrive/My Drive/Colab Notebooks/\",\n",
        "}\n",
        "hparams['device'] = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwGgVD7nwE8L",
        "colab_type": "text"
      },
      "source": [
        "#**EXERCISE 1 : Convolutional Autoencoder**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Y9gzzj8vQzy",
        "colab_type": "text"
      },
      "source": [
        "##1. Load MNIST train and test sets. Split the original training data into 95% training and 5% validation data.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oh7H4vkTvU-I",
        "colab_type": "code",
        "outputId": "60e42ad4-7076-4bc2-94f2-78376b4c1bd3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "transform = transforms.ToTensor()\n",
        "\n",
        "initial_data = datasets.MNIST(root='data', train=True, download=True, transform=transform)\n",
        "test_data = datasets.MNIST(root='data', train=False, download=True, transform=transform)\n",
        "\n",
        "train_size = int(hparams['train_percentage'] * len(initial_data))\n",
        "validation_size = len(initial_data) - train_size\n",
        "train_dataset, validation_dataset = torch.utils.data.random_split(initial_data, [train_size, validation_size])\n",
        "test_dataset = test_data\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
        "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=hparams['batch_size'], shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset,batch_size=hparams['batch_size'], shuffle=True)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "9920512it [00:02, 3653009.35it/s]                             \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "32768it [00:00, 56858.53it/s]                           \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1654784it [00:04, 334412.72it/s]                             \n",
            "0it [00:00, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "8192it [00:00, 21294.12it/s]            "
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y50JCesovagZ",
        "colab_type": "text"
      },
      "source": [
        "## 2. Implement a convolutional autoencoder (with separate Encoder and Decoder modules).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDWYr21e1zrX",
        "colab_type": "text"
      },
      "source": [
        "### a) Number of parameters per model\n",
        "We define a function to count the number of parameters of a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZX3Wj7nvdxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def model_params(model):\n",
        "    number_of_paremeters=0\n",
        "    for parameter in list(model.parameters()):\n",
        "        neural_network=1\n",
        "        for size_index in list(parameter.size()):\n",
        "            neural_network = neural_network*size_index\n",
        "        number_of_paremeters += neural_network\n",
        "    return number_of_paremeters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHHaIJ9I5ilO",
        "colab_type": "text"
      },
      "source": [
        "### b) encoder\n",
        "\n",
        "We define a convolutional encoder with two **(convolutional - max pooling - ReLU activation function)** layers, a** flattening layer **and a **fully connected layer**. This encoder takes the `bottleneck_size` as a parameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WEuaYeQT5hXS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, bottleneck_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, 20, 3, padding=1)  \n",
        "    self.conv2 = nn.Conv2d(20, 10, 3, padding=1)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linear = nn.Linear(7*7*10,bottleneck_size) \n",
        "    self.relu = nn.ReLU()    \n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    \n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x) \n",
        "    \n",
        "    x = self.flatten(x)\n",
        "    x = self.linear(x)\n",
        "    x = self.relu(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJuN2VjbIwyu",
        "colab_type": "text"
      },
      "source": [
        "### c) decoder\n",
        "We define the decoder network according to the previously defined encoder.\n",
        "We use : a **fully connected layer** from bottleneck to 10*7*7, a **ReLU activation function**, an unflattening function (**view function**), and two transposed convolutional layers. The **first transposed convolution layer is followed by a ReLU activation**, and the **last one by a Sigmoid activation**, in order to scale the outputs between 0 and 1. Again, we have the `bottleneck_size` as a parameter for its configuration."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wxoys0DX54oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, bottleneck_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.linear = nn.Linear(bottleneck_size, 10*7*7)\n",
        "    self.t_conv1 = nn.ConvTranspose2d(10, 20, 2, stride=2)\n",
        "    self.t_conv2 = nn.ConvTranspose2d(20, 1, 2, stride=2)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.relu = nn.ReLU()\n",
        "    \n",
        "  def forward(self,code):\n",
        "    x = code\n",
        "    x = self.linear(x)\n",
        "    x = self.relu(x)\n",
        "    \n",
        "    x = x.view([x.shape[0],10,7,7])\n",
        "    x = self.t_conv1(x)\n",
        "    x = self.relu(x)\n",
        "    \n",
        "    x = self.t_conv2(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jKAdL9Eve6v",
        "colab_type": "text"
      },
      "source": [
        "## 3. Train the convolutional autoencoder, with different bottleneck sizes. Plot the train and validation loss curves of all autoencoders in the same figure."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "41ESRcHh6byH",
        "colab_type": "text"
      },
      "source": [
        "### a) Epochs preparation and other auxiliary functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UjvbUTn4NHq",
        "colab_type": "text"
      },
      "source": [
        "#### i) Definition of a train epoch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zJnndRGv46L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch(train_loader, EncoderModel, DecoderModel, optimizer, criterion, hparams, epoch):\n",
        "  EncoderModel.train()\n",
        "  DecoderModel.train()\n",
        "  \n",
        "  loss = 0.0\n",
        "  losses = []\n",
        "\n",
        "  for batch_idx, (data) in enumerate(train_loader, 1):\n",
        "      data = data[0].to(hparams['device'])\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      outputs = DecoderModel(EncoderModel(data))\n",
        "      \n",
        "      loss = criterion(outputs, data)\n",
        "      \n",
        "      loss.backward()\n",
        "      losses.append(loss.item())\n",
        "      optimizer.step()\n",
        "\n",
        "      if batch_idx % hparams['log_interval'] == 0 or batch_idx >= len(train_loader):\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
        "          \n",
        "\n",
        "  return losses\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "559CzNIH4ywn",
        "colab_type": "text"
      },
      "source": [
        "#### ii) Definition of the MSE loss over an image and then over a batch of images\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXQRqTfvFVs9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MSE_Image(imageA, imageB):\n",
        "\terr = np.sum((imageA.data.numpy() - imageB.data.numpy()) ** 2)\n",
        "\terr /= float((imageA.shape[0] * imageA.shape[1]))\n",
        "\treturn err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l6fWzmxhFWZd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MSE_Batch (original, reconstructed, batch_size):\n",
        "  err = 0\n",
        "  for i in range (0,batch_size):\n",
        "    err += MSE_Image(original[i].detach(),reconstructed[i].detach())\n",
        "    err /= float(batch_size)\n",
        "  return err"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TygNt5915Tem",
        "colab_type": "text"
      },
      "source": [
        "#### iii) Definition of the validation epoch\n",
        "\n",
        "We set the models to **evaluation mode**. The `criterion` is set as a parameter for the validation step. We also have the option to calculate the average `Image_MSE`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QQ_3r3P65jHV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def eval_epoch(eval_loader,  EncoderModel, DecoderModel, criterion, hparams, printing=True, Image_MSE=False):\n",
        "  EncoderModel.eval()\n",
        "  DecoderModel.eval()\n",
        "  \n",
        "  eval_loss = 0\n",
        "  avg_img = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      for data in eval_loader:\n",
        "          data = data[0].to(hparams['device']) \n",
        "\n",
        "          firstOutput = EncoderModel(data)\n",
        "          outputs = DecoderModel(firstOutput)\n",
        "          eval_loss += criterion(outputs, data).item()\n",
        "          \n",
        "          if Image_MSE:\n",
        "            avg_img += MSE_Batch(data,outputs,hparams['batch_size'])\n",
        "\n",
        "      eval_loss /=len(eval_loader)\n",
        "      avg_img /= len (eval_loader)\n",
        "\n",
        "  if printing:\n",
        "    print('Eval set: Average loss: {:.15f}'.format(eval_loss))\n",
        "  \n",
        "  if Image_MSE:\n",
        "    return eval_loss, avg_img\n",
        "  else:  \n",
        "    return eval_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ajdDesiW6Dg9",
        "colab_type": "text"
      },
      "source": [
        "#### iv) We define a function to train a whole autoencoder\n",
        "\n",
        "We use **Adam** as optimizer because it is effective and has benefits from AdaGrad (deals with problems including sparse gradients) and RMSProp (deals with online and noisy problems). <br>\n",
        "We use **MSELoss** as criterion because the reconstructed image from the autoencoder is a real-valued of the input images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_kI2G-E5kWt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_autoencoder(myEncoder, myDecoder,criterion):\n",
        "  train_losses = []\n",
        "  eval_losses = []\n",
        "  test_losses = []\n",
        "\n",
        "  myEncoder.to(hparams['device'])\n",
        "  myDecoder.to(hparams['device'])\n",
        "\n",
        "  optimizer = optim.Adam(list(myEncoder.parameters()) + list(myDecoder.parameters()), lr=hparams['learning_rate'])\n",
        "\n",
        "  for epoch in range(1, hparams['num_epochs'] + 1):\n",
        "    tr_loss = train_epoch(train_loader, myEncoder, myDecoder, optimizer, criterion, hparams, epoch)\n",
        "    te_loss = eval_epoch(validation_loader, myEncoder, myDecoder, criterion, hparams)\n",
        "    train_losses.append(tr_loss)\n",
        "    eval_losses.append(te_loss)\n",
        "    \n",
        "\n",
        "  return myEncoder, myDecoder, train_losses, eval_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJgz5NjX63ZQ",
        "colab_type": "text"
      },
      "source": [
        "#### v) Save models\n",
        "\n",
        "We define a function to save the models in the Drive so we don't have to train them in every execution of the notebook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ASpXDFW62py",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_models():\n",
        "\n",
        "  for bottleneck_size in bottlenecks:\n",
        "    model_save_name = 'myEncoder' + str(bottleneck_size) + '.pt'\n",
        "    path = hparams[\"model_path\"] + model_save_name\n",
        "    torch.save(myEncoders[bottleneck_size].state_dict(), path)\n",
        "    \n",
        "    model_save_name = 'myDecoder' + str(bottleneck_size) + '.pt'\n",
        "    path = hparams[\"model_path\"] + model_save_name\n",
        "    torch.save(myDecoders[bottleneck_size].state_dict(), path)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6VJS4k0K7TE-",
        "colab_type": "text"
      },
      "source": [
        "#### vi) Load models\n",
        "\n",
        "We define a function that loads one model from Drive, and another one that loads a series of models by a list of bottleneck sizes. Both of them generate again the variables `myEncoders` and `myDecoders` in which we save all the trained models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DpkXYzYJSft",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(modelname, modelClass):\n",
        "  \n",
        "  path = hparams['model_path'] + modelname +'.pt'\n",
        "  modelClass.load_state_dict(torch.load(path))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5pxJVsGXW-H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_all_models(bottlenecks):\n",
        "\n",
        "  myEncoders = {}\n",
        "  myDecoders = {}\n",
        "\n",
        " \n",
        "  for each_bottleneck_size in bottlenecks:\n",
        "    myLoadedEncoder = Encoder(each_bottleneck_size)\n",
        "    myLoadedDecoder = Decoder(each_bottleneck_size)\n",
        "\n",
        "    model_load_name = 'myEncoder' + str(each_bottleneck_size)\n",
        "    load_model(model_load_name, myLoadedEncoder)\n",
        "    myEncoders[each_bottleneck_size] = (myLoadedEncoder)\n",
        "    \n",
        "    model_load_name = 'myDecoder' + str(each_bottleneck_size)\n",
        "    load_model(model_load_name, myLoadedDecoder)\n",
        "    myDecoders[each_bottleneck_size] = (myLoadedDecoder)\n",
        "\n",
        "  \n",
        "  return myEncoders,myDecoders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xfaQcCR6f50",
        "colab_type": "text"
      },
      "source": [
        "### b) Training\n",
        "\n",
        "We train the autoendoer with different sizes of the bottleneck (`bottlenecks`)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfMbDbwe_I7d",
        "colab_type": "text"
      },
      "source": [
        "#### i) Train autoencoders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45lurxB6_ki9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bottlenecks = [5,20,50,70]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Zq-u7RmDuGg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_me = False\n",
        "\n",
        "if train_me :\n",
        "\n",
        "  myEncoders = {}\n",
        "  myDecoders = {}\n",
        "  myTrain_losses = []\n",
        "  myEval_losses = []\n",
        "\n",
        "  for bottleneck_size in bottlenecks:\n",
        "    print(\"\\n***************** BOTTLENECK SIZE:\",bottleneck_size,\" ************************\")\n",
        "    myEncoder = Encoder(bottleneck_size)  \n",
        "    myDecoder = Decoder(bottleneck_size)\n",
        "\n",
        "    myTrainResults = train_autoencoder(myEncoder, myDecoder,hparams['autoencoder_criterion'])\n",
        "    myEncoders[bottleneck_size] = (myTrainResults[0])\n",
        "    myDecoders[bottleneck_size] = (myTrainResults[1])\n",
        "    myTrain_losses.append(myTrainResults[2])\n",
        "    myEval_losses.append(myTrainResults[3])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i_Mbi-gN9lLk",
        "colab_type": "text"
      },
      "source": [
        "#### ii) Save models\n",
        "Only to be executed if we want to save them for a future execution of the notebook\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzvBW0Hf96tX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_models()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z0tUV9NT-gDp",
        "colab_type": "text"
      },
      "source": [
        "### c) Plot loss curves"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AersYqgGSLc1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20, 16))\n",
        "plt.subplot(2,1,1)\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('MSELoss')\n",
        "\n",
        "general_marker_size = 15\n",
        "general_linewidth = 1\n",
        "number_of_losspoints_per_epoch = len(train_dataset)/hparams['batch_size']\n",
        "x_labels = [number_of_losspoints_per_epoch * i for i in range(1,hparams['num_epochs']+1)]\n",
        "colors = ['blue','orange','red','yellow']\n",
        "\n",
        "number_of_models = range(0,len(myTrain_losses),1)\n",
        "\n",
        "for model_number,bt in zip(number_of_models,bottlenecks):\n",
        "  plt.plot(list(itertools.chain(*myTrain_losses[model_number])), label='Train loss, bottleneck = '+ str(bt), marker='', linewidth=general_linewidth, color=colors[model_number])\n",
        "  plt.plot(x_labels,myEval_losses[model_number],label='Evaluation loss, bottleneck = ' + str(bt), marker='o', markersize=general_marker_size, linewidth=0, color=colors[model_number])\n",
        "\n",
        "\n",
        "plt.xticks(x_labels, range(1,hparams['num_epochs']+1))\n",
        "\n",
        "plt.legend();\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UR-k3sD_v5cQ",
        "colab_type": "text"
      },
      "source": [
        "## 4. Compute the avg. image reconstruction error (MSE) of the trained models on the MNIST validation and test sets. Show the results in a table, including number of parameters of each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70NeSDpg-ng0",
        "colab_type": "text"
      },
      "source": [
        "#### Load models\n",
        "Only to be executed if we are using saved models and we haven't trained them in this session. Make sure we have executed the code cell where we define `bottlenecks`, in section 3b.i"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cw2_3Fz17vGO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myEncoders,myDecoders = load_all_models(bottlenecks)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_h78NjHv8ax",
        "colab_type": "code",
        "outputId": "034c7d95-c5ba-48c2-9da1-64b89a7d4d04",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        }
      },
      "source": [
        "test_losses_of_a_model = []\n",
        "test_losses = []\n",
        "eval_losses_of_a_model = []\n",
        "eval_losses = []\n",
        "img_MSE_val = []\n",
        "img_MSE_test = []\n",
        "\n",
        "myTable = PrettyTable(['Model #','Size of bottleneck', 'Average validation MSE','Average test MSE', 'Number of parameters'])\n",
        "\n",
        "index=1\n",
        "\n",
        "for bottleneck_size in bottlenecks:\n",
        "   \n",
        "  eval_loss,img_avg = eval_epoch(validation_loader,  myEncoders[bottleneck_size], myDecoders[bottleneck_size], hparams['autoencoder_criterion'], hparams, False, True)\n",
        "  eval_losses_of_a_model.append(eval_loss)\n",
        "  img_MSE_val.append(img_avg)\n",
        "  \n",
        "  test_loss, img_avg = eval_epoch(test_loader,  myEncoders[bottleneck_size], myDecoders[bottleneck_size], hparams['autoencoder_criterion'], hparams, False, True)\n",
        "  test_losses_of_a_model.append(test_loss)\n",
        "  img_MSE_test.append(img_avg)\n",
        "\n",
        "  \n",
        "  eval_losses.append(eval_losses_of_a_model)\n",
        "  test_losses.append(test_losses_of_a_model)\n",
        "\n",
        "  \n",
        "  myTable.add_row([str(index),bottleneck_size,np.mean(img_MSE_val),np.mean(img_MSE_test), model_params(myEncoders[bottleneck_size]) + model_params(myDecoders[bottleneck_size])])\n",
        "  index +=1\n",
        "\n",
        "print(myTable)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+--------------------+------------------------+----------------------+----------------------+\n",
            "| Model # | Size of bottleneck | Average validation MSE |   Average test MSE   | Number of parameters |\n",
            "+---------+--------------------+------------------------+----------------------+----------------------+\n",
            "|    1    |         5          |  0.015558084903593124  | 0.015867808094196977 |         8306         |\n",
            "|    2    |         20         |  0.011683533429937167  | 0.011775754439942955 |        23021         |\n",
            "|    3    |         50         |  0.010045952569865497  | 0.00999381400359565  |        52451         |\n",
            "|    4    |         70         |  0.009192486885869478  | 0.00906986981668036  |        72071         |\n",
            "+---------+--------------------+------------------------+----------------------+----------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DFwJ4l6v9SQ",
        "colab_type": "text"
      },
      "source": [
        "## 5. Select one of the autoencoders and feed it 5 random MNIST images from the test set. Show them along with their reconstructions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SY2hiZy_9cjy",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "We choose the model with **lowest Average MSE error** and we save the bottleneck_size as a new hyperparameter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrk08yCxEngw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams['bottleneck_size'] = 70 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwroYAJOPF12",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def renderImages(images, output):\n",
        "  images = images.numpy()\n",
        "\n",
        "  output = output.view(hparams['batch_size'], 1, 28, 28)\n",
        "  output = output.detach().numpy()\n",
        "\n",
        "  fig, axes = plt.subplots(nrows=2, ncols=5, sharex=True, sharey=True, figsize=(20,4))\n",
        "\n",
        "  for images, row in zip([images, output], axes):\n",
        "      for img, ax in zip(images, row):\n",
        "            ax.imshow(np.squeeze(img), cmap='gray')\n",
        "            ax.get_xaxis().set_visible(False)\n",
        "            ax.get_yaxis().set_visible(False)\n",
        "            ax.label_outer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hs6My_O6wA9M",
        "colab_type": "code",
        "outputId": "9b1236e3-4dec-436d-da5d-e5cbb6ef87fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "SelectedEncoder = myEncoders[hparams['bottleneck_size']]\n",
        "SelectedDecoder = myDecoders[hparams['bottleneck_size']]\n",
        "\n",
        "dataiter = iter(test_loader)\n",
        "images_, labels_ = dataiter.next()\n",
        "\n",
        "images, labels = images_, labels_\n",
        "\n",
        "output = SelectedDecoder(SelectedEncoder(images))\n",
        "\n",
        "renderImages(images,output)\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAABBAAAADrCAYAAADQf2U5AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIAAAAACV/h880g7aLWCkawAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x288 with 10 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ocf2aJhhwBfY",
        "colab_type": "text"
      },
      "source": [
        "## 6. Generate 5 new images by injecting random values as input to the decoder. Show them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ixmqX6pwTpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "a = torch.randn(5,hparams['bottleneck_size'])\n",
        "a.to(hparams['device'])\n",
        "\n",
        "output = SelectedDecoder(a)\n",
        "output = output.detach().numpy()\n",
        "\n",
        "fig2, axes2 = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(20,4))\n",
        "for i in range(5):\n",
        "  im = output[i]\n",
        "  axes2[i].imshow(np.squeeze(im), cmap='gray')\n",
        "  axes2[i].axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBtfqwOUwTBS",
        "colab_type": "text"
      },
      "source": [
        "# **EXERCISE 2 : Transfer Learning**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NAagyOr3wcXU",
        "colab_type": "text"
      },
      "source": [
        "We need new hyper-parameters for this exercise.\n",
        "\n",
        "The `batch_size_classifier` parameter has **to be smaller** than `number_of_images` so that we have **more than one batch per epoch**. <br>\n",
        "The `num_of_epochs_classifier` parameter can be a **great number**, because we have **only 100 images** to train with. <br>\n",
        "The digits are from 0 to 9: `num_classes` is then set to 10. <br>\n",
        "For `classifier_criterion`, we have chosen the cross-entropy loss **nll_loss**, because it minimizes the distance between two probability distributions: predicted and actual labels.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lATnJRSTH-4J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "hparams['number_of_images']=100\n",
        "hparams['batch_size_classifier']=10\n",
        "hparams['num_epochs_classifier']=100\n",
        "hparams['num_classes']=10\n",
        "hparams['classifier_criterion']=F.nll_loss\n",
        "hparams['bottleneck_size']=70"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHwqkjiLIY3i",
        "colab_type": "text"
      },
      "source": [
        "## 1. Select a subset of 100 images and their associated labels from the MNIST training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koinhXjiwh2C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampling = random.choices(initial_data, k=hparams['number_of_images'])\n",
        "classifier_train_loader = torch.utils.data.DataLoader(sampling, batch_size=hparams['batch_size_classifier'], shuffle=True)\n",
        "\n",
        "iter_ = iter(classifier_train_loader)\n",
        "bimg, blabel = next(iter_)\n",
        "bimg_final = np.squeeze(bimg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NnCmn8bwiTn",
        "colab_type": "text"
      },
      "source": [
        "## 2. Select one of the previously trained autoencoders."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6druU3C8Jf8N",
        "colab_type": "text"
      },
      "source": [
        "We load the encoder with the selected bottlenecck size trained from exercise 1. We will copy it as a base for the pre-training and later on, as a base for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7ZnfxEPwrgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "myTrainedEncoder = Encoder(hparams['bottleneck_size'])\n",
        "load_model(\"myEncoder\"+str(hparams['bottleneck_size']),myTrainedEncoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLaNoUENwvn0",
        "colab_type": "text"
      },
      "source": [
        "## 3. Create a digit (0-9) classification model reusing the encoder of the autoencoder and adding the needed fully connected (projection) layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hnmQ-DzHjl6",
        "colab_type": "text"
      },
      "source": [
        "### a) Define the Classification Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YR-zmyUdw0we",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class ClassificationModel(nn.Module):\n",
        "  \n",
        "  def __init__(self, theTrainedEncoder):\n",
        "    super(ClassificationModel, self).__init__()\n",
        "    self.trainedEncoder = theTrainedEncoder\n",
        "    self.classifier = nn.Sequential(\n",
        "        nn.Linear(hparams['bottleneck_size'], 50),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        \n",
        "        nn.Linear(50, hparams['num_classes']),\n",
        "        nn.ReLU(),        \n",
        "        nn.LogSoftmax()\n",
        "        \n",
        "        )\n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.trainedEncoder(x)\n",
        "    x = self.classifier(x)\n",
        "\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfZeNYFZH0iR",
        "colab_type": "text"
      },
      "source": [
        "### b) Define the fuctions needed for training and validation\n",
        "\n",
        "We define new functions: training and validating each epoch, and the general training function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "erUgnRA3w4qK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch_classifier(train_loader, myClassificationModel, optimizer, criterion, hparams, epoch):\n",
        "  \n",
        "  myClassificationModel.train()\n",
        "  \n",
        "  loss = 0.0\n",
        "  losses = []\n",
        "\n",
        "\n",
        "  for data in train_loader:\n",
        "    image = data[0].to(hparams['device'])\n",
        "    label = data[1]\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    firstOutput = myClassificationModel(image)\n",
        "    loss = criterion(firstOutput, label)\n",
        "    loss.backward()\n",
        "    losses.append(loss.item())\n",
        "    optimizer.step()\n",
        "  if epoch % 10 == 0:\n",
        "    print('Train Epoch: {} \\tAverage training loss: {:.6f}'.format(epoch, np.mean(losses)))\n",
        "            \n",
        "  return losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "To-vj4B28UVa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def correct_predictions(output_batch, label_batch):\n",
        "  pred = output_batch.argmax(dim=1, keepdim=True)\n",
        "  acum = pred.eq(label_batch.view_as(pred)).sum().item()\n",
        "  return acum\n",
        "\n",
        "def eval_epoch_classifier(eval_loader,  myClassificationModel, criterion, hparams, printing=True):\n",
        "  myClassificationModel.eval()\n",
        "  \n",
        "  eval_loss = 0\n",
        "  accuracy = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "      for data in eval_loader:\n",
        "          image = data[0].to(hparams['device'])\n",
        "          label = data[1].to(hparams['device'])\n",
        "          firstOutput = myClassificationModel(image)\n",
        "          eval_loss += criterion(firstOutput, label).item()\n",
        "          accuracy += correct_predictions(firstOutput, label)\n",
        "\n",
        "      eval_loss /=len(eval_loader)\n",
        "      accuracy = 100. * accuracy /len(eval_loader.dataset)\n",
        "\n",
        "  if printing:\n",
        "    print('Eval set: Average loss: {:.15f}'.format(eval_loss))\n",
        "    print('Acc set: A: {:.15f}'.format(accuracy))\n",
        "\n",
        "  return eval_loss, accuracy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3uUlrU3Iiq7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_classifier(train_loader, myClassificationModel, criterion, optimizer):\n",
        "  \n",
        "  train_losses = []\n",
        "\n",
        "  myClassificationModel.to(hparams['device'])\n",
        "  \n",
        "  for epoch in range(1, hparams['num_epochs_classifier'] + 1):\n",
        "    tr_loss = train_epoch_classifier(train_loader, myClassificationModel, optimizer, criterion, hparams, epoch)\n",
        "    train_losses.append(tr_loss)\n",
        "\n",
        "  return myClassificationModel, train_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1cxvu-4wzgS",
        "colab_type": "text"
      },
      "source": [
        "## 4. Pre-training: use the weights of the autoencoder as initial values for the network weights and train a classification model on the subset of 100 samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgVoZcxYL3XE",
        "colab_type": "code",
        "outputId": "28912db9-3f7d-4e9c-c339-5d3123cf64c7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "myTrainedEncoder_pre = copy.copy(myTrainedEncoder)\n",
        "\n",
        "myClassificationModel_pre = ClassificationModel(myTrainedEncoder_pre)\n",
        "\n",
        "classifier_optimizer_pre=optim.RMSprop(myClassificationModel_pre.parameters(), hparams['learning_rate'])\n",
        "\n",
        "myTrainResults_pre = train_classifier(classifier_train_loader, myClassificationModel_pre, hparams['classifier_criterion'], classifier_optimizer_pre)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 10 \tAverage training loss: 1.209450\n",
            "Train Epoch: 20 \tAverage training loss: 0.620761\n",
            "Train Epoch: 30 \tAverage training loss: 0.593645\n",
            "Train Epoch: 40 \tAverage training loss: 0.471965\n",
            "Train Epoch: 50 \tAverage training loss: 0.368066\n",
            "Train Epoch: 60 \tAverage training loss: 0.365645\n",
            "Train Epoch: 70 \tAverage training loss: 0.376482\n",
            "Train Epoch: 80 \tAverage training loss: 0.353926\n",
            "Train Epoch: 90 \tAverage training loss: 0.354605\n",
            "Train Epoch: 100 \tAverage training loss: 0.351993\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lv9G8-D0w5TA",
        "colab_type": "text"
      },
      "source": [
        "## 5. Fine-tuning: do the same, but train the new projection layer with a normal learning rate and the reused part with a very low learning rate."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWSYejpgw8bj",
        "colab_type": "code",
        "outputId": "188798ef-84db-416a-b24d-6fa52887daa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "myTrainedEncoder_fine = copy.copy(myTrainedEncoder)\n",
        "\n",
        "myClassificationModel_fine = ClassificationModel(myTrainedEncoder_fine)\n",
        "\n",
        "classifier_optimizer_fine=optim.RMSprop([\n",
        "                {'params': myClassificationModel_fine.trainedEncoder.parameters(), 'lr': 1e-9},\n",
        "                {'params': myClassificationModel_fine.classifier.parameters()}\n",
        "            ], lr=hparams['learning_rate'])\n",
        "\n",
        "myTrainResults_fine = train_classifier(classifier_train_loader, myClassificationModel_fine, hparams['classifier_criterion'], classifier_optimizer_fine)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 10 \tAverage training loss: 0.941793\n",
            "Train Epoch: 20 \tAverage training loss: 0.889251\n",
            "Train Epoch: 30 \tAverage training loss: 0.564042\n",
            "Train Epoch: 40 \tAverage training loss: 0.586865\n",
            "Train Epoch: 50 \tAverage training loss: 0.509139\n",
            "Train Epoch: 60 \tAverage training loss: 0.478508\n",
            "Train Epoch: 70 \tAverage training loss: 0.450762\n",
            "Train Epoch: 80 \tAverage training loss: 0.427694\n",
            "Train Epoch: 90 \tAverage training loss: 0.442815\n",
            "Train Epoch: 100 \tAverage training loss: 0.353524\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLAwADU-w9Af",
        "colab_type": "text"
      },
      "source": [
        "## 6. From scratch: train the model on the 100 samples without reusing the encoder weights at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IODaMSq9w_yC",
        "colab_type": "code",
        "outputId": "2e362d2e-4d0e-4366-c559-4371e900b50f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "myEncoder_scratch = Encoder(hparams['bottleneck_size'])\n",
        "\n",
        "myClassificationModel_scratch = ClassificationModel(myEncoder_scratch)\n",
        "\n",
        "classifier_optimizer_scratch=optim.RMSprop(myClassificationModel_scratch.parameters(), hparams['learning_rate'])\n",
        "\n",
        "myTrainResults_scratch = train_classifier(classifier_train_loader, myClassificationModel_scratch, hparams['classifier_criterion'], classifier_optimizer_scratch)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train Epoch: 10 \tAverage training loss: 1.923797\n",
            "Train Epoch: 20 \tAverage training loss: 1.159494\n",
            "Train Epoch: 30 \tAverage training loss: 0.808275\n",
            "Train Epoch: 40 \tAverage training loss: 0.673155\n",
            "Train Epoch: 50 \tAverage training loss: 0.667959\n",
            "Train Epoch: 60 \tAverage training loss: 0.603095\n",
            "Train Epoch: 70 \tAverage training loss: 0.623187\n",
            "Train Epoch: 80 \tAverage training loss: 0.603898\n",
            "Train Epoch: 90 \tAverage training loss: 0.597093\n",
            "Train Epoch: 100 \tAverage training loss: 0.589967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qN5XHTrQxAbA",
        "colab_type": "text"
      },
      "source": [
        "## 7. Show the accuracy of the four models on the MNIST test set in a table."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqL9v3DXxCNe",
        "colab_type": "code",
        "outputId": "13269349-5d83-4003-b46b-c3378d2bd047",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188
        }
      },
      "source": [
        "classifier_loss_acc_pre = eval_epoch_classifier(test_loader,myClassificationModel_pre,hparams['classifier_criterion'],hparams,False)\n",
        "classifier_loss_acc_fine= eval_epoch_classifier(test_loader,myClassificationModel_fine,hparams['classifier_criterion'],hparams,False)\n",
        "classifier_loss_acc_scratch= eval_epoch_classifier(test_loader,myClassificationModel_scratch,hparams['classifier_criterion'],hparams,False)\n",
        "\n",
        "accuracy_table = PrettyTable(['Classifier type','Accuracy'])\n",
        "\n",
        "accuracy_table.add_row([\"Pretrained\",classifier_loss_acc_pre[1]])\n",
        "accuracy_table.add_row([\"Finetuned\",classifier_loss_acc_fine[1]])\n",
        "accuracy_table.add_row([\"From scratch\",classifier_loss_acc_scratch[1]])\n",
        "\n",
        "print(accuracy_table)\n"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torch/nn/modules/container.py:92: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
            "  input = module(input)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "+-----------------+----------+\n",
            "| Classifier type | Accuracy |\n",
            "+-----------------+----------+\n",
            "|    Pretrained   |  68.53   |\n",
            "|    Finetuned    |  72.53   |\n",
            "|   From scratch  |  66.68   |\n",
            "+-----------------+----------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1pZeqtlzxyp",
        "colab_type": "text"
      },
      "source": [
        "# **Optional Exercise A: Variational Autoencoder**\n",
        "\n",
        "## 1. Implement an autoencoder like that from Exercise 1, but turning the deterministic bottleneck into a stochastic bottleneck, with an isotropic Gaussian as distribution for the latent variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UoqbOWdBGFJ_",
        "colab_type": "text"
      },
      "source": [
        "We define a convolutional variational encoder and decoder with the same architecture as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KChuYVUn8HiT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalEncoder(nn.Module):\n",
        "  def __init__(self, bottleneck_size):\n",
        "    super(VariationalEncoder, self).__init__()\n",
        "    \n",
        "    self.conv1 = nn.Conv2d(1, 20, 3, padding=1)  \n",
        "    self.conv2 = nn.Conv2d(20, 10, 3, padding=1)\n",
        "    self.flatten = nn.Flatten()\n",
        "    self.linearMu = nn.Linear(7*7*10,bottleneck_size)\n",
        "    self.linearLV = nn.Linear(7*7*10,bottleneck_size)\n",
        "    self.relu = nn.ReLU()    \n",
        "    self.pool = nn.MaxPool2d(2, 2)\n",
        "    \n",
        "\n",
        "  def forward(self,x):\n",
        "    x = self.conv1(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.conv2(x)\n",
        "    x = self.relu(x)\n",
        "    x = self.pool(x)\n",
        "\n",
        "    x = self.flatten(x)\n",
        "    return self.linearMu(x), self.linearLV(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uOtIlL5O-umr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class VariationalDecoder(nn.Module):\n",
        "  def __init__(self, bottleneck_size):\n",
        "    super(VariationalDecoder, self).__init__()\n",
        "    self.linear = nn.Linear(bottleneck_size, 10*7*7)\n",
        "    self.t_conv1 = nn.ConvTranspose2d(10, 20, 2, stride=2)\n",
        "    self.t_conv2 = nn.ConvTranspose2d(20, 1, 2, stride=2)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.relu = nn.ReLU()\n",
        "    \n",
        "  def forward(self,code):\n",
        "    x = code\n",
        "    x = self.linear(x)\n",
        "    x = self.relu(x)\n",
        "    \n",
        "    x = x.view([x.shape[0],10,7,7])\n",
        "\n",
        "    x = self.t_conv1(x)\n",
        "    x = self.relu(x)\n",
        "    \n",
        "    x = self.t_conv2(x)\n",
        "    x = self.sigmoid(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ngzl3C-C_HBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def reparametrize (mu, logvar):\n",
        "  std = logvar.mul(0.5).exp_()\n",
        "  eps = Variable(std.data.new(std.size()).normal_())\n",
        "  return eps.mul(std).add_(mu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFGP17QtFLKQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reconstruction_function = nn.MSELoss(size_average=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjHfsV-0Cz0X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def loss_function(recon_x, x, mu, logvar):\n",
        "    BCE = reconstruction_function(recon_x, x) \n",
        "    KLD_element = mu.pow(2).add_(logvar.exp()).mul_(-1).add_(1).add_(logvar)\n",
        "    KLD = torch.sum(KLD_element).mul_(-0.5)\n",
        "    return BCE + KLD"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kk6ym0ClAKUE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_epoch_variational(train_loader, EncoderModel, DecoderModel, optimizer, hparams, epoch):\n",
        "  \n",
        "  EncoderModel.train()\n",
        "  DecoderModel.train()\n",
        "  \n",
        "  loss = 0.0\n",
        "  losses = []\n",
        "\n",
        "  for batch_idx, (data) in enumerate(train_loader, 1):\n",
        "      data = data[0].to(hparams['device'])\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      mu, logvar = EncoderModel(data)\n",
        "      fixedresult = reparametrize(mu, logvar)\n",
        "      outputs = DecoderModel(fixedresult)\n",
        "\n",
        "      loss = loss_function(outputs, data, mu, logvar)\n",
        "\n",
        "      loss.backward()\n",
        "      losses.append(loss.item())\n",
        "      optimizer.step()\n",
        "\n",
        "      if batch_idx % hparams['log_interval'] == 0 or batch_idx >= len(train_loader):\n",
        "          print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, batch_idx * len(data), len(train_loader.dataset), 100. * batch_idx / len(train_loader), loss.item()))\n",
        "          \n",
        "\n",
        "  return losses "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuUy4D7_BdC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_variational_autoencoder(myEncoder, myDecoder):\n",
        "  train_losses = []\n",
        "  myEncoder.to(hparams['device'])\n",
        "  myDecoder.to(hparams['device'])\n",
        "\n",
        "  optimizer = optim.Adam(list(myEncoder.parameters()) + list(myDecoder.parameters()), lr=hparams['learning_rate'])\n",
        "  \n",
        "  for epoch in range(1, hparams['num_epochs'] + 1):\n",
        "    tr_loss = train_epoch_variational(train_loader, myEncoder, myDecoder, optimizer, hparams, epoch)\n",
        "    train_losses.append(tr_loss)\n",
        "    \n",
        "\n",
        "  return myEncoder, myDecoder, train_losses"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUx4QkfNz3T2",
        "colab_type": "text"
      },
      "source": [
        "## 2. Train the model optimizing the Evidence Lower Bound (ELBO)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgc0E2Ex7ZNR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  bottleneck_size_VA = 50\n",
        " \n",
        "  myVEncoder = VariationalEncoder(bottleneck_size_VA) \n",
        "  myVDecoder = VariationalDecoder(bottleneck_size_VA)\n",
        "\n",
        "  myTrainResults = train_variational_autoencoder(myVEncoder, myVDecoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-xMyYAsEHfB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataiter = iter(test_loader)\n",
        "images_, labels_ = dataiter.next()\n",
        "\n",
        "images, labels = images_, labels_\n",
        "\n",
        "mu, logvar = myVEncoder(images)\n",
        "output = myVDecoder(mu)\n",
        "\n",
        "renderImages(images,output)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R7TB4Yhwz51l",
        "colab_type": "text"
      },
      "source": [
        "## 3. Generate samples with the decoder and show them."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lx3ta01DJKfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "random_5_numbers = torch.randn(5,bottleneck_size_VA)\n",
        "random_5_numbers.to(hparams['device'])\n",
        "\n",
        "output = myVDecoder(random_5_numbers)\n",
        "output = output.detach().numpy()\n",
        "\n",
        "fig2, axes2 = plt.subplots(nrows=1, ncols=5, sharex=True, sharey=True, figsize=(20,4))\n",
        "for i in range(5):\n",
        "  im = output[i]\n",
        "  axes2[i].imshow(np.squeeze(im), cmap='gray')\n",
        "  axes2[i].axis('off')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}